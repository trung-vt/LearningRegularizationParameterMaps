{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing torch ...\n",
      "torch imported in 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "begin = time()\n",
    "print(f\"Importing torch ...\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "print(f\"torch imported in {time()-begin:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def grad_h(u):\n",
    "    # Compute the gradient in both x and y directions\n",
    "    grad_u = torch.zeros(u.size() + (2,), device=u.device)\n",
    "    grad_u[..., 0] = torch.diff(u, n=1, dim=0, append=u[-1:, ...])  # Gradient in the x direction\n",
    "    grad_u[..., 1] = torch.diff(u, n=1, dim=1, append=u[:, -1:])    # Gradient in the y direction\n",
    "    return grad_u\n",
    "\n",
    "def div_h(v):\n",
    "    # Compute the divergence from the gradient in both x and y directions\n",
    "    div_v = torch.zeros(v.size()[:-1], device=v.device)\n",
    "    div_v += torch.diff(v[..., 0], n=1, dim=0, prepend=v[0:1, ..., 0])  # Divergence in the x direction\n",
    "    div_v += torch.diff(v[..., 1], n=1, dim=1, prepend=v[:, 0:1, ..., 1])  # Divergence in the y direction\n",
    "    return div_v\n",
    "\n",
    "class CustomAlgorithm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomAlgorithm, self).__init__()\n",
    "        self.sigma = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "        self.tau = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "        self.theta = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while self.sigma * self.tau * 0.5 * (17 + torch.sqrt(torch.tensor(33.0))) > 1:\n",
    "                self.sigma /= 2\n",
    "                self.tau /= 2\n",
    "\n",
    "    def forward(self, u0, p0, num_iters=100):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        u0 : torch.Tensor\n",
    "            Initial guess for the solution u. \n",
    "            Expect 2D tensor of shape [n, n] (for now).\n",
    "        \"\"\"\n",
    "        device = u0.device\n",
    "        u0 = u0.to(device)\n",
    "        p0 = p0.to(device)\n",
    "\n",
    "        v0 = torch.zeros_like(grad_h(u0)).to(device)  # Adjusted to match gradient size\n",
    "        w0 = torch.zeros_like(grad_h(p0)).to(device)  # Adjusted to match gradient size\n",
    "        u_bar0 = u0.clone().detach().to(device) # shape = [10, 10]\n",
    "        p_bar0 = p0.clone().detach().to(device) # shape = [10, 10]\n",
    "\n",
    "        for i in range(num_iters):\n",
    "            if i == 0:\n",
    "                print(f\"grad_h(u_bar0).shape = {grad_h(u_bar0).shape}\")\n",
    "                print(f\"p_bar0.shape = {p_bar0.shape}\")\n",
    "            vn_plus1 = self.P_alpha1(v0 + self.sigma * (grad_h(u_bar0) - p_bar0))\n",
    "            wn_plus1 = self.P_alpha0(w0 + self.sigma * grad_h(p0))\n",
    "            un_plus1 = self.id_tauFh_inverse(u0 + self.tau * div_h(vn_plus1))\n",
    "            pn_plus1 = p0 + self.tau * (vn_plus1 + div_h(wn_plus1))\n",
    "            u_bar0 = 2 * un_plus1 - u0\n",
    "            p_bar0 = 2 * pn_plus1 - p0\n",
    "\n",
    "            u0, p0 = un_plus1, pn_plus1\n",
    "            v0, w0 = vn_plus1, wn_plus1\n",
    "\n",
    "        return u0\n",
    "\n",
    "    def P_alpha1(self, x):\n",
    "        # Placeholder for projection operator P_alpha1\n",
    "        return x\n",
    "\n",
    "    def P_alpha0(self, x):\n",
    "        # Placeholder for projection operator P_alpha0\n",
    "        return x\n",
    "\n",
    "    def id_tauFh_inverse(self, x):\n",
    "        # Placeholder for (id + τ∂Fh)^(-1)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_h(u_bar0).shape = torch.Size([10, 10, 2])\n",
      "p_bar0.shape = torch.Size([10, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m p0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m CustomAlgorithm()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m uN \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(uN)\n",
      "File \u001b[0;32m/mnt/h/GIT/DISSERTATION/LearningRegularizationParameterMaps/venv_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/h/GIT/DISSERTATION/LearningRegularizationParameterMaps/venv_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 44\u001b[0m, in \u001b[0;36mCustomAlgorithm.forward\u001b[0;34m(self, u0, p0, N)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_h(u_bar0).shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrad_h(u_bar0)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_bar0.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp_bar0\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m vn_plus1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP_alpha1(v0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma \u001b[38;5;241m*\u001b[39m (\u001b[43mgrad_h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_bar0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp_bar0\u001b[49m))\n\u001b[1;32m     45\u001b[0m wn_plus1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP_alpha0(w0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma \u001b[38;5;241m*\u001b[39m grad_h(p0))\n\u001b[1;32m     46\u001b[0m un_plus1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_tauFh_inverse(u0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau \u001b[38;5;241m*\u001b[39m div_h(vn_plus1))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "u0 = torch.zeros((10, 10), device=device)\n",
    "p0 = torch.zeros((10, 10), device=device)\n",
    "model = CustomAlgorithm().to(device)\n",
    "\n",
    "uN = model(u0, p0, N=100)\n",
    "print(uN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m     49\u001b[0m     vn_plus1 \u001b[38;5;241m=\u001b[39m P_alpha1(v0 \u001b[38;5;241m+\u001b[39m sigma \u001b[38;5;241m*\u001b[39m (grad_h(u_bar0) \u001b[38;5;241m-\u001b[39m p_bar0))\n\u001b[0;32m---> 50\u001b[0m     wn_plus1 \u001b[38;5;241m=\u001b[39m P_alpha0(w0 \u001b[38;5;241m+\u001b[39m \u001b[43msigma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrad_h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp0\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     51\u001b[0m     un_plus1 \u001b[38;5;241m=\u001b[39m id_tauFh_inverse(u0 \u001b[38;5;241m+\u001b[39m tau \u001b[38;5;241m*\u001b[39m div_h(vn_plus1))\n\u001b[1;32m     52\u001b[0m     pn_plus1 \u001b[38;5;241m=\u001b[39m p0 \u001b[38;5;241m+\u001b[39m tau \u001b[38;5;241m*\u001b[39m (vn_plus1 \u001b[38;5;241m+\u001b[39m div_h(wn_plus1))\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming some necessary functions and projections are defined\n",
    "def P_alpha1(x):\n",
    "    # Placeholder for projection operator P_alpha1\n",
    "    return x\n",
    "\n",
    "def P_alpha0(x):\n",
    "    # Placeholder for projection operator P_alpha0\n",
    "    return x\n",
    "\n",
    "def id_tauFh_inverse(x):\n",
    "    # Placeholder for (id + τ∂Fh)^(-1)\n",
    "    return x\n",
    "\n",
    "def Fh(u):\n",
    "    # Placeholder for the function Fh\n",
    "    return u\n",
    "\n",
    "def TGV2_alpha(u):\n",
    "    # Placeholder for the TGV^2_α function\n",
    "    return u\n",
    "\n",
    "def grad_h(u):\n",
    "    # Placeholder for the gradient operator ∇h\n",
    "    return np.gradient(u)\n",
    "\n",
    "def div_h(v):\n",
    "    # Placeholder for the divergence operator div_h\n",
    "    return np.divergence(v)\n",
    "\n",
    "# Parameters\n",
    "sigma = 1.0\n",
    "tau = 1.0\n",
    "while sigma * tau * 0.5 * (17 + np.sqrt(33)) > 1:\n",
    "    sigma /= 2\n",
    "    tau /= 2\n",
    "\n",
    "# Initial variables\n",
    "u0, p0 = np.zeros((10, 10)), np.zeros((10, 10))\n",
    "v0, w0 = np.zeros((10, 10)), np.zeros((10, 10))\n",
    "u_bar0, p_bar0 = u0.copy(), p0.copy()\n",
    "\n",
    "# Number of iterations\n",
    "num_iters = 100\n",
    "\n",
    "# Iteration loop\n",
    "for i in range(num_iters):\n",
    "    vn_plus1 = P_alpha1(v0 + sigma * (grad_h(u_bar0) - p_bar0))\n",
    "    wn_plus1 = P_alpha0(w0 + sigma * grad_h(p0))\n",
    "    un_plus1 = id_tauFh_inverse(u0 + tau * div_h(vn_plus1))\n",
    "    pn_plus1 = p0 + tau * (vn_plus1 + div_h(wn_plus1))\n",
    "    u_bar0 = 2 * un_plus1 - u0\n",
    "    p_bar0 = 2 * pn_plus1 - p0\n",
    "\n",
    "    # Update for next iteration\n",
    "    u0, p0 = un_plus1, pn_plus1\n",
    "    v0, w0 = vn_plus1, wn_plus1\n",
    "\n",
    "# Result\n",
    "uN = u0\n",
    "\n",
    "# Output the result\n",
    "print(uN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
