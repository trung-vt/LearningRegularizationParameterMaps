{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time_importing_torch = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing torch ...\n",
      "Importing torch took 13.042095422744751 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# NOTE: Importing torch the first time will always take a long time!\n",
    "import time\n",
    "# NOTE: Importing torch the first time will always take a long time!\n",
    "if first_time_importing_torch:\n",
    "    print(f\"Importing torch ...\")\n",
    "    import_torch_start_time = time.time() \n",
    "import torch\n",
    "if first_time_importing_torch:\n",
    "    import_torch_end_time = time.time()\n",
    "    print(f\"Importing torch took {import_torch_end_time - import_torch_start_time} seconds\")\n",
    "    first_time_importing_torch = False\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test different implementations of UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. My implementation (subject to change)\n",
    "\n",
    "Notable points:\n",
    "- linear interpolation with a convolution layer to reduct the number of channels right away\n",
    "\n",
    "\n",
    "Can choose:\n",
    "- number of blocks\n",
    "- initial number of filters\n",
    "- downsampling: max pooling or average pooling\n",
    "\n",
    "Cannot config:\n",
    "- size of each block. Each block is fixed with 2 convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_parts.py as a reference\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels: int, out_channels: int, n_dimensions=3, activation=\"LeakyReLU\"):\n",
    "        super(DoubleConv, self).__init__()\n",
    "\n",
    "        def get_conv(in_channels, out_channels):\n",
    "            # 1-dimensional convolution is not supported\n",
    "            if n_dimensions == 3:\n",
    "                return nn.Conv3d(in_channels, out_channels, kernel_size=(3, 3, 1), padding=(1, 1, 0))\n",
    "            elif n_dimensions == 2:\n",
    "                return nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported number of dimensions: {n_dimensions}\")\n",
    "\n",
    "        def get_activation():\n",
    "            if activation == \"LeakyReLU\":\n",
    "                return nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "            elif activation == \"ReLU\":\n",
    "                return nn.ReLU(inplace=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            get_conv(in_channels, out_channels), get_activation(),\n",
    "            get_conv(out_channels, out_channels), get_activation())\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv_block(x)\n",
    "        \n",
    "\n",
    "class EncodeBlock3d(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels: int, n_dimensions=3,\n",
    "            activation=\"LeakyReLU\",\n",
    "            downsampling_kernel=(2, 2, 1), downsampling_mode=\"max\"):\n",
    "        super(EncodeBlock3d, self).__init__()\n",
    "\n",
    "        len = downsampling_kernel[0] # Assume kernel has shape (len, len, 1)\n",
    "        assert downsampling_kernel == (len, len, 1), f\"Expected a flat square kernel like {(len, len, 1)}, got {downsampling_kernel}\"\n",
    "        stride = (2, 2, 1) # Stride 2x2 to halve each side \n",
    "        padding = ((len-1)//2, (len-1)//2, 0) # Padding (len-1) // 2 to exactly halve each side \n",
    "        if downsampling_mode == \"max\":\n",
    "            self.pool = nn.MaxPool3d(\n",
    "                kernel_size=downsampling_kernel, stride=stride, padding=padding)\n",
    "        elif downsampling_mode == \"avg\":\n",
    "            self.pool = nn.AvgPool3d(\n",
    "                kernel_size=downsampling_kernel, stride=stride, padding=padding)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling method: {downsampling_mode}\")\n",
    "\n",
    "        self.double_conv = DoubleConv(in_channels, in_channels * 2, n_dimensions, activation=activation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.pool(x)\n",
    "        x = self.double_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class DecodeBlock3d(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels: int, n_dimensions=3, \n",
    "            activation=\"LeakyReLU\",\n",
    "            upsampling_kernel=(2, 2, 1), upsampling_mode=\"linear_interpolation\"):\n",
    "        super(DecodeBlock3d, self).__init__()\n",
    "\n",
    "        if upsampling_mode == \"linear_interpolation\":\n",
    "            self.upsampling = nn.Sequential(\n",
    "                nn.Upsample(\n",
    "                    scale_factor=(2, 2, 1), # Assume the shape is (Nx, Ny, 1) where Nx is the image width and Ny is the image height.\n",
    "                    mode='trilinear', align_corners=True), # What difference does it make in the end if align_corners is True or False? Preserving symmetry?\n",
    "                # 1x1x1 convolution to reduce the number of channels while keeping the size the same\n",
    "                nn.Conv3d(\n",
    "                    in_channels, in_channels // 2, \n",
    "                    kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0))\n",
    "            )\n",
    "        elif upsampling_mode == \"transposed_convolution\":  \n",
    "            len = upsampling_kernel[0] # Assume kernel has shape (len, len, 1)\n",
    "            assert upsampling_kernel == (len, len, 1), f\"Expected a flat square kernel like {(len, len, 1)}, got {upsampling_kernel}\"\n",
    "            stride = (2, 2, 1) # Stride 2x2 to double each side \n",
    "            padding = ((len-1)//2, (len-1)//2, 0) # Padding (len-1) // 2 to exactly double each side    \n",
    "            self.upsampling = nn.ConvTranspose3d(\n",
    "                in_channels, in_channels // 2, \n",
    "                kernel_size=upsampling_kernel, stride=stride, padding=padding, \n",
    "                output_padding=padding # TODO: Should this be the same as padding?\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported upsampling method: {upsampling_mode}\")\n",
    "        \n",
    "        self.double_conv = DoubleConv(in_channels, in_channels // 2, n_dimensions, activation=activation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_encoder_output: torch.Tensor):\n",
    "        x = self.upsampling(x)\n",
    "        x = torch.cat([x_encoder_output, x], dim=1)   # skip-connection. No cropping since we ensure that the size is the same.\n",
    "        x = self.double_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class UNet3d(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=1, out_channels=2, init_filters=32, n_blocks=3,\n",
    "            activation=\"LeakyReLU\",\n",
    "            downsampling_kernel=(2, 2, 1), downsampling_mode=\"max\",\n",
    "            upsampling_kernel=(2, 2, 1), upsampling_mode=\"linear_interpolation\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Assume that input is 5D tensor of shape (batch_size, channels, Nx, Ny, Nt)\n",
    "        where Nx is the image width and Ny is the image height.\n",
    "        Assume that batch_size = 1, channels = 1, Nx = Ny (square image), Nt = 1 (static image).\n",
    "        NOTE: The convention used in pytorch documentation is (batch_size, channels, Nt, Ny, Nx).\n",
    "        \"channels\" is equivalent to the number of filters or features.\n",
    "\n",
    "        Our paper (figure 2):\n",
    "            - in_channels = 1\n",
    "            - out_channels = 2\n",
    "            - init_filters = 32\n",
    "            - n_blocks = 3\n",
    "            - pooling: max pooling 2x2\n",
    "            - pool padding = 1\n",
    "                - 1 padding will keep the size of the \"image\" the same after each convolution. The skip-connection will NOT crop the encoder's output.\n",
    "            - upsampling kernel: 2x2 ?\n",
    "            - up_mode: linear interpolation\n",
    "\n",
    "        U-Net paper (2015, Olaf Ronneberger https://arxiv.org/abs/1505.04597):\n",
    "            - in_channels = 1\n",
    "            - out_channels = 2\n",
    "            - init_filters = 64\n",
    "            - n_blocks = 4\n",
    "            - pooling: max pooling 2x2\n",
    "            - pool padding = 0\n",
    "                - 0 padding will reduce the size of the \"image\" by 2 in each dimension after each convolution. The skip-connection will have to crop the encoder's output to match the decoder's input.\n",
    "            - upsampling kernel: 2x2\n",
    "            - up_mode: ? (linear interpolation or transposed convolution)\n",
    "        \"\"\"\n",
    "        super(UNet3d, self).__init__()\n",
    "        \n",
    "        self.c0x0 = DoubleConv( # TODO: Find a better name\n",
    "            in_channels=in_channels, \n",
    "            out_channels=init_filters,\n",
    "            activation=activation\n",
    "        )\n",
    "        self.encoder = nn.ModuleList([\n",
    "            EncodeBlock3d(\n",
    "                in_channels=init_filters * 2**i,\n",
    "                activation=activation,\n",
    "                downsampling_kernel=downsampling_kernel,\n",
    "                downsampling_mode=downsampling_mode\n",
    "            ) for i in range(n_blocks)\n",
    "        ])\n",
    "        self.decoder = nn.ModuleList([\n",
    "            DecodeBlock3d(\n",
    "                in_channels=init_filters * 2**(n_blocks-i),\n",
    "                activation=activation, \n",
    "                upsampling_kernel=upsampling_kernel,\n",
    "                upsampling_mode=upsampling_mode\n",
    "            ) for i in range(n_blocks)\n",
    "        ])\n",
    "        # 1x1x1 convo\n",
    "        self.c1x1 = nn.Conv3d( # TODO: Find a better name\n",
    "            in_channels=init_filters,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Assume that x is 5D tensor of shape (batch_size, channels, Nx, Ny, Nt)\n",
    "        # where Nx is the image width and Ny is the image height.\n",
    "        # Aslo assume that batch_size = 1, channels = 1, Nx = Ny (square image), Nt = 1 (static image).\n",
    "        # NOTE: The convention used in pytorch documentation is (batch_size, channels, Nt, Ny, Nx).\n",
    "        assert len(x.size()) == 5, f\"Expected 5D tensor, got {x.size()}\"\n",
    "        batch_size, channels, Nx, Ny, Nt = x.size()\n",
    "        assert channels == 1, f\"Expected 1 channel, got {channels}\" # TODO: Allow multiple channels (colour images)\n",
    "        assert Nx == Ny, f\"Expected square image, got ({Nx}, {Ny})\" # TODO: Allow non-square images\n",
    "        assert Nt == 1, f\"Expected 1 time step, got {Nt}\" # TODO: Allow multiple time steps (dynamic images, video)\n",
    "        assert batch_size == 1, f\"Expected batch size 1, got {batch_size}\" # TODO: Might train with larger batch size\n",
    "\n",
    "        n_blocks = len(self.encoder)\n",
    "        assert Nx >= 2**n_blocks, f\"Expected width (Nx) of at least {2**n_blocks}, got {Nx}\"\n",
    "        assert Ny >= 2**n_blocks, f\"Expected height (Ny) of at least {2**n_blocks}, got {Ny}\"\n",
    "\n",
    "        x = self.c0x0(x)\n",
    "\n",
    "        encoder_outputs = []\n",
    "        for i, enc_block in enumerate(self.encoder):\n",
    "            encoder_outputs.append(x)\n",
    "            x = enc_block(x)\n",
    "        for i, dec_block in enumerate(self.decoder):\n",
    "            x = dec_block(x, encoder_outputs[-i-1]) # skip-connection inside\n",
    "            \n",
    "        x = self.c1x1(x)\n",
    "\n",
    "        for enc_output in encoder_outputs:\n",
    "            del enc_output\n",
    "        del encoder_outputs\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_and_clear_cuda(expected, actual):\n",
    "    try:\n",
    "        assert expected == actual\n",
    "    except AssertionError:\n",
    "        print(f\"!!! ERROR !!! Expected: {expected}, got {actual}\")\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "def test_unet_3d():  \n",
    "    input_tensor = torch.randn(1, 1, 512, 512, 1)  # batch size of 1, 1 channel, 512x512x1 volume\n",
    "    \n",
    "\n",
    "    # Example usage\n",
    "    model = UNet3d(\n",
    "        init_filters=32,\n",
    "        n_blocks=3,\n",
    "        activation=\"ReLU\",\n",
    "        downsampling_kernel=(2, 2, 1),\n",
    "        downsampling_mode=\"max\",\n",
    "        upsampling_kernel=(2, 2, 1),\n",
    "        upsampling_mode=\"linear_interpolation\",\n",
    "    )\n",
    "    output = model(input_tensor)\n",
    "    print(f\"UNet output shape: {output.shape}\")\n",
    "    assert_and_clear_cuda((1, 2, 512, 512, 1), output.shape)\n",
    "\n",
    "\n",
    "    conv_3d = nn.Conv3d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "    conv_3d_output = conv_3d(input_tensor)\n",
    "    print(f\"Conv3d output shape: {conv_3d_output.shape}\")\n",
    "    assert_and_clear_cuda((1, 64, 512, 512, 1), conv_3d_output.shape)\n",
    "\n",
    "\n",
    "    double_conv_3d = DoubleConv(64, 128)\n",
    "    double_conv_output = double_conv_3d(conv_3d_output)\n",
    "    print(f\"{DoubleConv.__name__} output shape: {double_conv_output.shape}\")\n",
    "    assert_and_clear_cuda((1, 128, 512, 512, 1), double_conv_output.shape)\n",
    "\n",
    "\n",
    "    max_3d = nn.MaxPool3d((3, 3, 1), stride=(2, 2, 1), padding=(1, 1, 0))\n",
    "    max_3d_output_1 = max_3d(input_tensor)\n",
    "    print(f\"MaxPool3d output 1 shape: {max_3d_output_1.shape}\")\n",
    "    assert_and_clear_cuda((1, 1, 256, 256, 1), max_3d_output_1.shape)\n",
    "\n",
    "    max_3d_input = torch.randn(1, 128, 512, 512, 1)\n",
    "    max_3d_output_2 = max_3d(max_3d_input)\n",
    "    print(f\"MaxPool3d output 2 shape: {max_3d_output_2.shape}\")\n",
    "    assert_and_clear_cuda((1, 128, 256, 256, 1), max_3d_output_2.shape)\n",
    "\n",
    "    conv_transpose_3d = nn.ConvTranspose3d(\n",
    "        1024, 512, \n",
    "        kernel_size=(3, 3, 1), \n",
    "        stride=(2, 2, 1), \n",
    "        padding=(1, 1, 0), \n",
    "        output_padding=(1, 1, 0)\n",
    "    )\n",
    "    conv_transpose_3d_input = torch.randn(1, 1024, 32, 32, 1)\n",
    "    conv_transpose_3d_output = conv_transpose_3d(conv_transpose_3d_input)\n",
    "    print(f\"ConvTranspose3d output shape: {conv_transpose_3d_output.shape}\")\n",
    "    assert_and_clear_cuda((1, 512, 64, 64, 1), conv_transpose_3d_output.shape)\n",
    "\n",
    "\n",
    "    up_sample = nn.Upsample(\n",
    "        scale_factor=(2, 2, 1), \n",
    "        mode='trilinear', align_corners=True) # What difference does it make if align_corners is True or False?\n",
    "    up_sample_output = up_sample(input_tensor)\n",
    "    print(f\"Upsample output shape: {up_sample_output.shape}\")\n",
    "    assert_and_clear_cuda((1, 1, 1024, 1024, 1), up_sample_output.shape)\n",
    "                    \n",
    "\n",
    "    # # print(f\"\\n{model}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # # Delete the model and the output tensor\n",
    "    # del model\n",
    "    # del output\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "test_unet_3d()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dynamic Image Denoising?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/koflera/LearningRegularizationParameterMaps/blob/main/networks/unet.py\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    A block of convolutional layers (1D, 2D or 3D)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in,\n",
    "        n_ch_out,\n",
    "        n_convs,\n",
    "        kernel_size=3,\n",
    "        bias=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if dim == 1:\n",
    "            conv_op = nn.Conv1d\n",
    "        if dim == 2:\n",
    "            conv_op = nn.Conv2d\n",
    "        elif dim == 3:\n",
    "            conv_op = nn.Conv3d\n",
    "\n",
    "        padding = int(np.floor(kernel_size / 2))\n",
    "\n",
    "        conv_block_list = []\n",
    "        conv_block_list.extend(\n",
    "            [\n",
    "                conv_op(\n",
    "                    n_ch_in,\n",
    "                    n_ch_out,\n",
    "                    kernel_size,\n",
    "                    padding=padding,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                ),\n",
    "                nn.LeakyReLU(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i in range(n_convs - 1):\n",
    "            conv_block_list.extend(\n",
    "                [\n",
    "                    conv_op(\n",
    "                        n_ch_out,\n",
    "                        n_ch_out,\n",
    "                        kernel_size,\n",
    "                        padding=padding,\n",
    "                        bias=bias,\n",
    "                        padding_mode=padding_mode,\n",
    "                    ),\n",
    "                    nn.LeakyReLU(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in,\n",
    "        n_enc_stages,\n",
    "        n_convs_per_stage,\n",
    "        n_filters,\n",
    "        kernel_size=3,\n",
    "        bias=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        n_ch_list = [n_ch_in]\n",
    "        for ne in range(n_enc_stages):\n",
    "            n_ch_list.append(int(n_filters) * 2**ne)\n",
    "\n",
    "        self.enc_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(\n",
    "                    dim,\n",
    "                    n_ch_list[i],\n",
    "                    n_ch_list[i + 1],\n",
    "                    n_convs_per_stage,\n",
    "                    kernel_size=kernel_size,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                )\n",
    "                for i in range(len(n_ch_list) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if dim == 1:\n",
    "            pool_op = nn.MaxPool1d(2)\n",
    "        elif dim == 2:\n",
    "            pool_op = nn.MaxPool2d(2)\n",
    "        elif dim == 3:\n",
    "            pool_op = nn.MaxPool3d(2)\n",
    "\n",
    "        self.pool = pool_op\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "            x = self.pool(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in,\n",
    "        n_dec_stages,\n",
    "        n_convs_per_stage,\n",
    "        n_filters,\n",
    "        kernel_size=3,\n",
    "        bias=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        n_ch_list = []\n",
    "        for ne in range(n_dec_stages):\n",
    "            n_ch_list.append(int(n_ch_in * (1 / 2) ** ne))\n",
    "\n",
    "        if dim == 1:\n",
    "            conv_op = nn.Conv1d\n",
    "            interp_mode = \"linear\"\n",
    "        elif dim == 2:\n",
    "            conv_op = nn.Conv2d\n",
    "            interp_mode = \"bilinear\"\n",
    "        elif dim == 3:\n",
    "            interp_mode = \"trilinear\"\n",
    "            conv_op = nn.Conv3d\n",
    "\n",
    "        self.interp_mode = interp_mode\n",
    "\n",
    "        padding = int(np.floor(kernel_size / 2))\n",
    "        self.upconvs = nn.ModuleList(\n",
    "            [\n",
    "                conv_op(\n",
    "                    n_ch_list[i],\n",
    "                    n_ch_list[i + 1],\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                )\n",
    "                for i in range(len(n_ch_list) - 1)\n",
    "            ]\n",
    "        )\n",
    "        self.dec_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(\n",
    "                    dim,\n",
    "                    n_ch_list[i],\n",
    "                    n_ch_list[i + 1],\n",
    "                    n_convs_per_stage,\n",
    "                    kernel_size=kernel_size,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                )\n",
    "                for i in range(len(n_ch_list) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.dec_blocks)):\n",
    "            enc_features = encoder_features[i]\n",
    "            enc_features_shape = enc_features.shape\n",
    "            x = nn.functional.interpolate(\n",
    "                x, enc_features_shape[2:], mode=self.interp_mode, align_corners=False\n",
    "            )\n",
    "            x = self.upconvs[i](x)\n",
    "            x = torch.cat([x, enc_features], dim=1)\n",
    "            x = self.dec_blocks[i](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in=2,\n",
    "        n_ch_out=2,\n",
    "        n_enc_stages=3,\n",
    "        n_convs_per_stage=2,\n",
    "        n_filters=16,\n",
    "        kernel_size=3,\n",
    "        res_connection=False,\n",
    "        bias=True,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            dim,\n",
    "            n_ch_in,\n",
    "            n_enc_stages,\n",
    "            n_convs_per_stage,\n",
    "            n_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            dim,\n",
    "            n_filters * (2 ** (n_enc_stages - 1)),\n",
    "            n_enc_stages,\n",
    "            n_convs_per_stage,\n",
    "            n_filters * (n_enc_stages * 2),\n",
    "            kernel_size=kernel_size,\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "\n",
    "        if dim == 1:\n",
    "            conv_op = nn.Conv1d\n",
    "        elif dim == 2:\n",
    "            conv_op = nn.Conv2d\n",
    "        elif dim == 3:\n",
    "            conv_op = nn.Conv3d\n",
    "\n",
    "        self.c1x1 = conv_op(n_filters, n_ch_out, kernel_size=1, padding=0, bias=bias)\n",
    "        if res_connection:\n",
    "            if n_ch_in == n_ch_out:\n",
    "                self.res_connection = lambda x: x\n",
    "            else:\n",
    "                self.res_connection = conv_op(n_ch_in, n_ch_out, 1)\n",
    "        else:\n",
    "            self.res_connection = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_features = self.encoder(x)\n",
    "        dec = self.decoder(enc_features[-1], enc_features[::-1][1:])\n",
    "        out = self.c1x1(dec)\n",
    "        if self.res_connection:\n",
    "            out = out + self.res_connection(x)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
