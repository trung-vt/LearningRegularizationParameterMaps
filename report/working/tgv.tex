
\subsection{Discrete TGV minimisation problem}

Let
\begin{equation}
    \Omega_h = \{(i,j) \mid i,j \in \mathbb{N}, \ 1 \leq i \leq N_1, \ 1 \leq j \leq N_2\}.
\end{equation}
be ...
\newline

An image is a function $\Omega_h \rightarrow \mathbb{R}$. \newline

% A matrix is a discrete function. \newline

Let $u \in \mathbb{R}^{N_1 \times N_2}$ be 
% the image to be reconstructed, 
an image,
$f \in \mathbb{R}^{N_1 \times N_2}$ the observed image. \newline


The spaces of scalar, vector, and symmetric matrix valued functions are defined as (page 13)
\begin{equation}
    \begin{aligned}
    U &= \{u : \Omega_h \rightarrow \mathbb{R}\} \\
    V &= \{u : \Omega_h \rightarrow \mathbb{R}^2\} \\
    W &= \{u : \Omega_h \rightarrow \mathrm{Sym}^2(\mathbb{R}^2)\}
    \end{aligned}
\end{equation}

where $\mathrm{Sym}^2(\mathbb{R}^2)$ is the space of symmetric $2 \times 2$ matrices. \newline

We use the notations $v$ and $w$ where

\begin{equation}
    \begin{aligned}
    $v &\in V$ has components $\left(v\right)^{(1)}$ and $\left(v\right)^{(2)}$ \\
    $w &\in W$ has components $\left(w\right)^{(1,1)}$, $\left(w\right)^{(1,2)}$, $\left(w\right)^{(2,1)}$, $\left(w\right)^{(2,2)} \\
    % & \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \text{where} \left(w\right)^{(1,2)} = \left(w\right)^{(2,1)}$. 
\end{aligned}
\end{equation}

with \left(w\right)^{(1,2)} = \left(w\right)^{(2,1)}$. \newline

% \begin{equation}
%     a, b : \Omega_h \rightarrow \mathbb{R}, \quad \langle a, b \rangle = \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} a_{i,j} b_{i,j}
% \end{equation}

Use the scalar product notation

\begin{equation}
    a, b : \Omega_h \rightarrow \mathbb{R} : \quad \langle a, b \rangle = \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} a_{i,j} b_{i,j}
\end{equation}

the scalar products in the spaces $V$ and $W$ are

\begin{equation}
\begin{aligned}
    % & u, r \in U:\langle u, r\rangle_U=\left\langle u, r\right\rangle \\
    % u, r \in U &: \langle u, r\rangle_U = \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} u_{i,j} r_{i,j} \\
    v, p \in V &: \langle v, p\rangle_V = \left\langle\left(v\right)^{(1)},\left(p\right)^{(1)}\right\rangle+\left\langle\left(v\right)^{(2)},\left(p\right)^{(2)}\right\rangle \\
    w, q \in W &: \langle w, q\rangle_W = \left\langle\left(w\right)^{(1,1)},\left(q\right)^{(1,1)}\right\rangle+\left\langle\left(w\right)^{(2,2)},\left(q\right)^{(2,2)}\right\rangle+2\left\langle\left(w\right)^{(1,2)},\left(q\right)^{(1,2)}\right\rangle
\end{aligned}
\end{equation}

are the scalar products in $U$, $V$, $W$.
\newline


The $x$ and $y$ forward finite difference operators are (page 13)
\begin{equation}
(\partial_x^+ u)_{i,j} =
\begin{cases}
u_{i+1,j} - u_{i,j} & \text{for } 1 \leq i < N_1 \\
0 & \text{for } i = N_1
\end{cases}
\end{equation}

\begin{equation}
(\partial_y^+ u)_{i,j} =
\begin{cases}
u_{i,j+1} - u_{i,j} & \text{for } 1 \leq j < N_2 \\
0 & \text{for } j = N_2
\end{cases}
\end{equation}

and the backward finite difference operators are (page 13)
\begin{equation}
    (\partial_x^- u)_{i,j} = 
    \begin{cases}
    u_{1,j} & \text{if } i = 1 \\
    u_{i,j} - u_{i-1,j} & \text{for } 1 < i < N_1 \\
    -u_{N_1-1,j} & \text{for } i = N_1
    \end{cases}
\end{equation}
    
\begin{equation}
    (\partial_y^- u)_{i,j} = 
    \begin{cases}
    u_{i,1} & \text{if } j = 1 \\
    u_{i,j} - u_{i,j-1} & \text{for } 1 < j < N_2 \\
    -u_{i,N_2-1} & \text{for } j = N_2
    \end{cases}
\end{equation}






The gradient operator $\nabla_h$ and the symmetrised gradient operator $\mathcal{E}_h$ are defined as (pages 13, 14)

\begin{equation}
    \begin{aligned}
    \nabla_h : U \rightarrow V &, \quad \nabla_h u = 
    \begin{pmatrix}
    \partial_x^+ u \\
    \partial_y^+ u
    \end{pmatrix} \\
    \mathcal{E}_h : V \rightarrow W &, \quad \mathcal{E}_h (v) = 
    \begin{pmatrix}
    \partial_x^- (v)^{(1)} &
    \frac{1}{2} \left( \partial_y^- (v)^{(1)} + \partial_x^- (v)^{(2)} \right) \\
    \frac{1}{2} \left( \partial_y^- (v)^{(1)} + \partial_x^- (v)^{(2)} \right) &
    \partial_y^- (v)^{(2)}
    \end{pmatrix}
    \end{aligned}
\end{equation}

and the divergence operators $\mathrm{div}_h$ are


\begin{equation}
\begin{aligned}
\mathrm{div}_h : V \rightarrow U &, \quad \mathrm{div}_h v = \partial_x^- (v)^{(1)} + \partial_y^- (v)^{(2)} \\
% \end{equation}
% \begin{equation}
\mathrm{div}_h : W \rightarrow V &, \quad \mathrm{div}_h w = 
\begin{pmatrix}
\partial_x^+ (w)^{(11)} + \partial_y^+ (w)^{(12)} \\
\partial_x^+ (w)^{(12)} + \partial_y^+ (w)^{(22)}
\end{pmatrix}
\end{aligned}
\end{equation}

The discrete $\infty\text{-norms}$ are
\begin{equation}
    \begin{aligned}
    v \in V &: \quad \|v\|_\infty = \max_{(i,j) \in \Omega_h} \left( \left( (v)_{i,j}^{(1)} \right)^{2} + \left( (v)_{i,j}^{(2)} \right)^{2} \right)^{1/2} \\
    w \in W &: \quad \|w\|_\infty = \max_{(i,j) \in \Omega_h} \left( \left( (w)_{i,j}^{(1,1)} \right)^{2} + \left( (w)_{i,j}^{(2,2)} \right)^{2} + 2 \left( (w)_{i,j}^{(1,2)} \right)^{2} \right)^{1/2}
\end{aligned}
\end{equation}


Finally, the TGV minimisation problem
is defined as (page ...)
\begin{equation}
    % \min_{u \in L^p(\Omega)} F(u) + \mathrm{TGV}^2_\alpha(u)
    \min_{u \in \mathbb{R}^{N_1 \times N_2}} F(u) + \mathrm{TGV}^2_\alpha(u)
    \label{eq:tgv_minimisation_problem}
\end{equation}

where
the data fidelity/discrepancy function is (page ...)
\begin{equation}
    F(u) = \frac{1}{2} || u - f ||^2_2 = 
    \frac{1}{2}
    % \frac{1}{2 N_1 N_2} 
    \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} (u_{i,j} - f_{i,j})^2
    \label{eq:data_fidelity_term}
\end{equation}

and
the regularisation term is (page 14)
\begin{equation}
    \mathrm{TGV}^2_\alpha(u) = \max \left\{ \langle u, \mathrm{div}_h v \rangle_U \ \middle| \ 
    \begin{aligned}
    &(v, w) \in V \times W, \ \mathrm{div}_h w = v, \\
    &\|w\|_\infty \leq \alpha_0, \ \|v\|_\infty \leq \alpha_1 
    \end{aligned}
    \right\}
\end{equation}

In [14, 34] it is demonstrated that the TGV functional can be equivalently written as

\begin{equation}
    \text{TGV}^2_{\alpha}(u) = \min_{w \in BD(\Omega)} \alpha_1 \lvert Du - w \rvert (\Omega) + \alpha_0 \lvert \mathcal{E} w \rvert (\Omega),
\end{equation}
    




\subsection{
% PDHG 
Numerical
algorithm for TGV minimisation problem
}

% This minimisation problem is equivalent to the saddle-point problem (page ...).


\begin{algorithm}[H]
    \caption{PDHG algorithm for image denoising with fixed regularisation parameter-map $\boldsymbol{\Lambda}$ (adapted from \cite{kofler2023learning} using the implementation by \cite{dyn_img_pdhg_code})}
    \begin{algorithmic}[1]
    
    \STATE \textbf{Input:} 
    $L = \| [\mathbf{I}, \nabla]^\text{T} \|_2$, 
    % $L = \sqrt{13}$, 
    % $\tau \lesssim 1/L$, 
    % $\sigma \lesssim 1/L$, 
    % $\theta \lesssim 1$, 
    $\tau = \text{sigmoid}(10) / L$, 
    $\sigma = \text{sigmoid}(10) / L$, 
    $\theta = \text{sigmoid}(10)$, 
    noisy image $\mathbf{x}_0$
    
    \STATE \textbf{Output:} reconstructed image $\hat{\mathbf{x}}$
    % \STATE $\mathbf{y} = \mathbf{x}_0$
    \STATE $\bar{\mathbf{x}}_0 = \mathbf{x}_0$
    % \STATE $\mathbf{p}_0 = \mathbf{0}$
    \STATE $\mathbf{p}_0 = \mathbf{x}_0$
    \STATE $\mathbf{q}_0 = \mathbf{0}$
    \FOR{$k < T$}
        % \STATE $\mathbf{p}_{k+1} = \left(\mathbf{p}_k + \sigma ( \bar{\mathbf{x}}_k - \mathbf{y})\right) / (1 + \sigma)$
        \STATE $\mathbf{p}_{k+1} = \left(\mathbf{p}_k + \sigma ( \bar{\mathbf{x}}_k - \mathbf{x}_0)\right) / (1 + \sigma)$
        \STATE $\mathbf{q}_{k+1} = \text{clip}_{\boldsymbol{\Lambda}} \left(\mathbf{q}_k + \sigma \nabla \bar{\mathbf{x}}_k \right)$
        \STATE $\mathbf{x}_{k+1} = \mathbf{x}_k - \tau \mathbf{p}_{k+1} - \tau \nabla^\text{T} \mathbf{q}_{k+1}$
        \STATE $\bar{\mathbf{x}}_{k+1} = \mathbf{x}_{k+1} + \theta (\mathbf{x}_{k+1} - \mathbf{x}_k)$
    \ENDFOR
    \STATE $\hat{\mathbf{x}} = \mathbf{x}_T$
    \end{algorithmic}
    \end{algorithm}


\begin{algorithm}[H]
    \caption{Solve $\min_{u \in U} F_h(u) + \mathrm{TGV}^2_{\alpha}(u)$}

    \begin{algorithmic}[1]
    \STATE \textbf{Input:} $f \in U$ is the input (noisy) image, and $\alpha_0, \alpha_1 > 0$ are the regularisation parameters.
    \STATE Choose $\sigma > 0$, $\tau > 0$ such that $\sigma \tau \frac{1}{2} (17 + \sqrt{33}) \leq 1$. Here we choose $\sigma = \tau = 0.29$
    \STATE $u^{[0]} = f$
    \STATE $p^{[0]} = \mathbf{0}_V$
    \STATE $v^{[0]} = \mathbf{0}_V$
    \STATE $w^{[0]} = \mathbf{0}_W$
    \STATE $\bar{u}^{[0]} = u^{[0]}$
    \STATE $\bar{p}^{[0]} = p^{[0]}$

    \FOR {$n = 0, 1, 2, \ldots$}
            \STATE $v^{[n+1]} = \mathcal{P}_{\alpha_1} \left( v^{[n]} + \sigma (\nabla_h \bar{u}^{[n]} - \bar{p}^{[n]}) \right)$
            \STATE $w^{[n+1]} = \mathcal{P}_{\alpha_0} \left( w^{[n]} + \sigma \mathcal{E}_h (\bar{p}^{[n]}) \right)$
            \STATE $u^{[n+1]} = \left( u^{[n]} + \tau (\text{div}_h v^{[n+1]} + f) \right) / ( 1 + \tau )$
            \STATE $p^{[n+1]} = p^{[n]} + \tau (v^{[n+1]} + \text{div}_h w^{[n+1]})$
            \STATE $\bar{u}^{[n+1]} = 2u^{[n+1]} - u^{[n]}$
            \STATE $\bar{p}^{[n+1]} = 2p^{[n+1]} - p^{[n]}$
    \ENDFOR
    \STATE Return $u^{[N]}$ for some large $N$.

    \end{algorithmic}
    \label{alg:pdhg_tgv_scalar}
\end{algorithm}

where $\mathbf{0}_V \in V$ and $\mathbf{0}_W \in W$ are the zeros matrices in $V$ and $W$ respectively, and $\mathcal{P}_{\alpha_1}$ and $\mathcal{P}_{\alpha_0}$ are the projection operators defined as

\begin{equation}
    \begin{aligned}
    & \mathcal{P}_{\alpha_1}(v) = \frac{v}{\max \left(\mathbf{1}, \frac{1}{\alpha_1}|v|\right)}, \quad v \in V \\
    & \mathcal{P}_{\alpha_0}(w) = \frac{w}{\max \left(\mathbf{1}, \frac{1}{\alpha_0}|w|\right)}, \quad w \in W
    \end{aligned}
\end{equation}

where $|v|$, $|w|$ $\in U$ are matrices with

\begin{equation}
    \begin{aligned}
   \quad &|v|_{i,j} = \left( \left( (v)_{i,j}^{(1)} \right)^{2} + \left( (v)_{i,j}^{(2)} \right)^{2} \right)^{1/2} \\
    \quad &|w|_{i,j} = \left( \left( (w)_{i,j}^{(11)} \right)^{2} + \left( (w)_{i,j}^{(22)} \right)^{2} + 2 \left( (w)_{i,j}^{(12)} \right)^{2} \right)^{1/2}
    \end{aligned}
    \label{eq:abs_v_w}
\end{equation}

Here $\mathbf{1}$ is a matrix of ones. 
Note that the division is element-wise. (see below)

Denote the matrix underneath as $a$.
The matrix under the division is 2D whereas the matrix above the division is 3D and 4D.
Therefore each element $v_{i,j}^{(k)}$ and $w_{i,j}^{(k,h)}$ of the matrices $v$ and $w$ is divided by the corresponding element $a_{i,j}$ of the matrix under the division.

\newline


This algorithm is used to solve the minimisation problem \cref{eq:tgv_minimisation_problem}.
It is taken from the paper \cite{recovering_piecewise_smooth_multichannel_images}, page ....


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Why Projection is dividing by matrix instead of a scalar}
\subsection{Appendix: Calculate projection $P_{\alpha}$}


Given matrix $a \in V$, we need to find matrix $u \in V$ such that

\begin{equation}
    u = \arg \min_{|| v ||_\infty \leq 1} || v - a ||_V
\end{equation}


where

\begin{equation}
    ||v||_V = \sum_{i,j} \left( \left(v_{i,j}^{(1)}\right)^2 + \left(v_{i,j}^{(2)}\right)^2 \right)^{1/2}
\end{equation}

is the $L_{1, 2}$-norm. 
(Note that here $u \in V$ is not the same as $u \in U$ in the previous section.)

Constraints:

\begin{equation}
    ||v||_\infty = \max_{(i,j) \in \Omega_h} \left( \left(v_{i,j}^{(1)}\right)^2 + \left(v_{i,j}^{(2)}\right)^2 \right)^{1/2} \leq 1  \quad  \quad  \forall i, j
\end{equation}

% or equivalently

% \begin{equation}
%     \left( \left(v_{i,j}^{(1)}\right)^2 + \left(v_{i,j}^{(2)}\right)^2 \right)^{1/2} \leq 1  \quad  \quad  \forall i, j
% \end{equation}


Let $t_{i,j} = \sqrt{(v_{i,j}^{(1)})^2 + (v_{i,j}^{(2)})^2}$.
Then the constraints can be written as

\begin{equation}
    t_{i,j} \leq 1  \quad  \quad  \forall i, j
\end{equation}



We have $n^2$ constraints.
Each constraint is applied to a specific element $v_{i,j}$ and is independent of the others.
Therefore, the solution can be found by solving $n^2$ independent problems:

\begin{equation}
u_{i,j} = \arg \min_{t_{i,j} \leq 1} \| v_{i,j} - a_{i,j} \|  \quad \forall i, j
\end{equation}

Denote $v_{i,j} = x$, $a_{i,j} = y$, $u_{i,j} = z$:

\begin{equation}
    z = \arg \min_{\| x \|_2 \leq 1} \|x - y\| \quad \quad \quad x, y, z \in \mathbb{R}^2
    \label{eq:projection_equation}
\end{equation}

The equation \cref{eq:projection_equation} is a 
projection of $y$ onto the unit circle in $\mathbb{R}^2$.
Therefore, it has the solution


\begin{equation}
    \begin{aligned}
    z & =
    \begin{cases}
        \frac{y}{\|y\|_2} & \text{if } \|y\|_2 > 1 \\
        y & \text{if } \|y\|_2 < 1 \\
    \end{cases} \\
    & = \frac{y}{\max(\mathbf{1}, \|y\|_2)}
    \end{aligned}
\end{equation}



Applying to $u$ and $a$, we have

\begin{equation}
    u_{i,j} = \frac{a_{i,j}}{\max(1, \|a_{i,j}\|_2)}
\end{equation}

or in matrix notation,

\begin{equation}
    u = \frac{a}{\max(\mathbf{1}, \|a\|_\infty)}
\end{equation}

where $\mathbf{1}$ is a matrix of ones.

TODO: $||a||_\infty$ is a scalar, but we need a matriasdasdx?



\subsection{Parameter Maps}

In the scalar version, we are treating every region equally.

In the parameter map version, we will treat different regions differently.
We want to assign appropriate values of $\alpha_0$ and $\alpha_1$ to different regions.

The parameter maps are matrices $\mathbf{\Lambda}_0$ and $\mathbf{\Lambda}_1$ of the same size as the image.

The minimisation problem is now

\begin{equation}
    \min_{u \in U} F_h(u) + \mathrm{TGV}^2_{\mathbf{\Lambda}}(u)
\end{equation}

where $F_h(u)$ is the data fidelity term defined in \cref{eq:data_fidelity_term} and

\begin{equation}
    \mathrm{TGV}^2_{\mathbf{\Lambda}}(u) = \max \left\{ \langle u, \mathrm{div}_h v \rangle_U \ \middle| \
    \begin{aligned}
    &(v, w) \in V \times W, \ \mathrm{div}_h w = v, \\
    &|w| \leq \mathbf{\Lambda}_0, \ |v| \leq \mathbf{\Lambda}_1
    \end{aligned}
    \right\}
\end{equation}

is the regularisation term.
The comparison $\leq$ is element-wise, and $|v|$, $|w|$ $\in U$ are defined in \cref{eq:abs_v_w}.

\subsection{PDHG Algorithm for TGV with Parameter Maps}

\begin{algorithm}[H]
    \caption{Solve $\min_{u \in U} F_h(u) + \mathrm{TGV}^2_{\alpha}(u)$}

    \begin{algorithmic}[1]
    \STATE \textbf{Input:} $f \in U$ is the input (noisy) image, and $\alpha_0, \alpha_1 > 0$ are the regularisation parameters.
    \STATE Choose $\sigma > 0$, $\tau > 0$ such that $\sigma \tau \frac{1}{2} (17 + \sqrt{33}) \leq 1$. Here we choose $\sigma = \tau = 0.29$
    \STATE $u^{[0]} = f$
    \STATE $p^{[0]} = \mathbf{0}_V$
    \STATE $v^{[0]} = \mathbf{0}_V$
    \STATE $w^{[0]} = \mathbf{0}_W$
    \STATE $\bar{u}^{[0]} = u^{[0]}$
    \STATE $\bar{p}^{[0]} = p^{[0]}$

    \FOR {$n = 0, 1, 2, \ldots$}
            \STATE $v^{[n+1]} = \mathcal{P}_{\mathbf{\Lambda}_1} \left( v^{[n]} + \sigma (\nabla_h \bar{u}^{[n]} - \bar{p}^{[n]}) \right)$
            \STATE $w^{[n+1]} = \mathcal{P}_{\mathbf{\Lambda}_0} \left( w^{[n]} + \sigma \mathcal{E}_h (\bar{p}^{[n]}) \right)$
            \STATE $u^{[n+1]} = \left( u^{[n]} + \tau (\text{div}_h v^{[n+1]} + f) \right) / ( 1 + \tau )$
            \STATE $p^{[n+1]} = p^{[n]} + \tau (v^{[n+1]} + \text{div}_h w^{[n+1]})$
            \STATE $\bar{u}^{[n+1]} = 2u^{[n+1]} - u^{[n]}$
            \STATE $\bar{p}^{[n+1]} = 2p^{[n+1]} - p^{[n]}$
    \ENDFOR
    \STATE Return $u^{[N]}$ for some large $N$.

    \end{algorithmic}
    \label{alg:pdhg_tgv_reg_map}
\end{algorithm}

where

\begin{equation}
    \begin{aligned}
    & \mathcal{P}_{\mathbf{\Lambda}_1}(v) = \frac{v}{\max \left(\mathbf{1}, \frac{|v|}{\mathbf{\Lambda}_1}\right)}, \quad v \in V \\
    & \mathcal{P}_{\mathbf{\Lambda}_0}(w) = \frac{w}{\max \left(\mathbf{1}, \frac{|w|}{\mathbf{\Lambda}_0}\right)}, \quad w \in W
    \end{aligned}
\end{equation}

$\frac{|v|}{\mathbf{\Lambda}_1}$ and $\frac{|w|}{\mathbf{\Lambda}_0}$ are element-wise divisions between matrices of the same size.

% Page 19:
% \begin{equation}
%     (\text{id} + \tau \partial F_h)^{-1}(\bar{u}) =
%     \begin{cases}
%     \frac{\bar{u} + \tau f}{1 + \tau} & \text{if } q = 2, \\
%     f + \mathcal{S}_\tau(\bar{u} - f) & \text{if } q = 1.
%     \end{cases}
% \end{equation}
