{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Requirements\n",
    "```python\n",
    "# numpy==1.23.4\n",
    "# matplotlib\n",
    "torch\n",
    "# scikit-image\n",
    "# odl==0.7.0\n",
    "# dival\n",
    "# wget # for downloading the test data\n",
    "pillow # Image utils\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pillow\n",
    "# !pip install torch\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Optional\n",
    "from tqdm import tqdm # progress bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/bin/python\n",
      "Torch version: 2.2.2+cu121\n",
      "Path: /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/scripts/dyn_img_static\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Path: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Import the image\n",
    "\n",
    "Read the image\n",
    "\n",
    "Visualise one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL CODE TO DOWNLOAD THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CODE TO GET AN IMAGE FROM LOCAL FILE SYSTEM\n",
    "# # TODO: CHANGE THIS TO YOUR PATH\n",
    "# # NOTE: Windows uses \\\\ instead of /\n",
    "# def load_images(ids: list):\n",
    "#     data_path = \"../../data/dyn_img_static/tmp/SIDD_Small_sRGB_Only/Data\"\n",
    "#     k = 0\n",
    "#     for folder in os.listdir(data_path):\n",
    "#         img_id = folder[:4]\t# The first 4 characters of folder name is the image id (0001, 0002, ..., 0200)\n",
    "#         if img_id not in ids:\n",
    "#             continue\n",
    "#         k += 1\n",
    "#         print(f'loading image id {img_id}, {k}/{len(ids)}')\n",
    "\n",
    "#         sample_path = os.path.join(data_path, folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO CONVERT AN IMAGE TO GRAYSCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO VISUALISE AN IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Add synthetic noise\n",
    "\n",
    "artificial Gaussian noise\n",
    "\n",
    "Noise can occur in reality.\n",
    "\n",
    "It is difficult to obtain a pair of clean and noisy images of one exact same scene.\n",
    "\n",
    "For training, it is common to add synthetic noise to an image that is considered clean and then try to reconstruct it.\n",
    "\n",
    "There are many types of noise and different ways to add noise. We can add salt-and-pepper noise. (?)We can add more noise in some parts and less in others. We can use a combination of noise-adding strategies to build more robust models.\n",
    "\n",
    "For our purpose, we will focus on Gaussian noise. This is sufficient for most cases. \n",
    "\n",
    "(?) We will add noise with the same probability for each pixel (not using the strategies of focusing on certain regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO ADD NOISE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### (Optional) Rescale the image\n",
    "\n",
    "Make the image a bit smaller\n",
    "\n",
    "Original 5328 x 3000\n",
    "\n",
    "Recaled 0.5 to 2664 x 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO RESCALE THE IMAGE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Calculate PSNR\n",
    "\n",
    "PSNR is a common metrics for noisy image.\n",
    "\n",
    "Compare before and after adding synthetic noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO CALCULATE THE PSNR HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Divide image into patches\n",
    "\n",
    "Cut into 192 x 192 small patches. Each patch is one input (right?)\n",
    "\n",
    "??? Should I try to make the patch size divisible by the image dimension: 222 * 125\n",
    "\n",
    "5328 = 222 * 24\n",
    "3000 = 125 * 24\n",
    "\n",
    "2664 = 222 * 12\n",
    "3000 = 125 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO DIVIDE THE IMAGE INTO PATCHES HERE\n",
    "\n",
    "def extract_patches_3d(x, kernel_size, padding=0, stride=1, dilation=1):\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size, kernel_size)\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding, padding)\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride, stride)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation, dilation)\n",
    "\n",
    "    # print(f\"x.shape: {x.shape}\")\n",
    "    # print(f\"kernel_size: {kernel_size}\")\n",
    "    # print(f\"padding: {padding}\")\n",
    "    # print(f\"stride: {stride}\")\n",
    "    # print(f\"dilation: {dilation}\")\n",
    "    # x.shape: torch.Size([1, 1, 540, 960, 600])\n",
    "    # kernel_size: [192, 192, 32]\n",
    "    # padding: (0, 0, 0)\n",
    "    # stride: [192, 192, 16]\n",
    "    # dilation: (1, 1, 1)\n",
    "\n",
    "    def get_dim_blocks(dim_in, dim_kernel_size, dim_padding = 0, dim_stride = 1, dim_dilation = 1):\n",
    "        dim_out = (dim_in + 2 * dim_padding - dim_dilation * (dim_kernel_size - 1) - 1) // dim_stride + 1\n",
    "        return dim_out\n",
    "\n",
    "    channels = x.shape[-4]\n",
    "    d_dim_in = x.shape[-3]\n",
    "    h_dim_in = x.shape[-2]\n",
    "    w_dim_in = x.shape[-1]\n",
    "    d_dim_out = get_dim_blocks(d_dim_in, kernel_size[0], padding[0], stride[0], dilation[0])\n",
    "    h_dim_out = get_dim_blocks(h_dim_in, kernel_size[1], padding[1], stride[1], dilation[1])\n",
    "    w_dim_out = get_dim_blocks(w_dim_in, kernel_size[2], padding[2], stride[2], dilation[2])\n",
    "    # print(d_dim_in, h_dim_in, w_dim_in, d_dim_out, h_dim_out, w_dim_out)\n",
    "    # print(f\"d_dim_in = {d_dim_in}\")\n",
    "    # print(f\"h_dim_in = {h_dim_in}\")\n",
    "    # print(f\"w_dim_in = {w_dim_in}\")\n",
    "    # print(f\"d_dim_out = {d_dim_out}\")\n",
    "    # print(f\"h_dim_out = {h_dim_out}\")\n",
    "    # print(f\"w_dim_out = {w_dim_out}\")\n",
    "    # d_dim_in = 540\n",
    "    # h_dim_in = 960\n",
    "    # w_dim_in = 600\n",
    "    # d_dim_out = 2\n",
    "    # h_dim_out = 5\n",
    "    # w_dim_out = 36\n",
    "\n",
    "    # (B, C, D, H, W)\n",
    "    x = x.view(-1, \n",
    "               channels, \n",
    "               d_dim_in, \n",
    "               h_dim_in * w_dim_in)                                                     \n",
    "    # (B, C, D, H * W)\n",
    "\n",
    "    x = torch.nn.functional.unfold(x, \n",
    "                                   kernel_size=(kernel_size[0], 1), \n",
    "                                   padding=(padding[0], 0), \n",
    "                                   stride=(stride[0], 1), \n",
    "                                   dilation=(dilation[0], 1))                   \n",
    "    # (B, C * kernel_size[0], d_dim_out * H * W)\n",
    "\n",
    "    # print(f\"x.shape: {x.shape}\")\n",
    "    # x.shape: torch.Size([1, 192, 1152000])\n",
    "    # 1 * 192 * 1,152,000 = 221,184,000\n",
    "    # print(f\"channels = {channels}\")\n",
    "    # print(f\"kernel_size[0] = {kernel_size[0]}\")\n",
    "    # print(f\"d_dim_out = {d_dim_out}\")\n",
    "    # print(f\"h_dim_in = {h_dim_in}\")\n",
    "    # print(f\"w_dim_in = {w_dim_in}\")\n",
    "    # print(f\"channels * kernel_size[0] * d_dim_out * h_dim_out * w_dim_out: {channels * kernel_size[0] * d_dim_out * h_dim_out * w_dim_out}\")\n",
    "    # extracting patches of shape [192, 192, 32]; strides [192, 192, 16]\n",
    "    # x.shape: torch.Size([1, 192, 1152000])\n",
    "    # channels = 1\n",
    "    # kernel_size[0] = 192\n",
    "    # d_dim_out = 2\n",
    "    # h_dim_in = 960\n",
    "    # w_dim_in = 600\n",
    "    # channels * kernel_size[0] * d_dim_out * h_dim_out * w_dim_out: 69120\n",
    "\n",
    "    x = x.view(-1, \n",
    "               channels * kernel_size[0] * d_dim_out, \n",
    "               h_dim_in, \n",
    "               w_dim_in)                                   \n",
    "    # (B, C * kernel_size[0] * d_dim_out, H, W)\n",
    "\n",
    "    x = torch.nn.functional.unfold(x, \n",
    "                                   kernel_size=(kernel_size[1], kernel_size[2]), \n",
    "                                   padding=(padding[1], padding[2]), \n",
    "                                   stride=(stride[1], stride[2]), \n",
    "                                   dilation=(dilation[1], dilation[2]))        \n",
    "    # (B, C * kernel_size[0] * d_dim_out * kernel_size[1] * kernel_size[2], h_dim_out, w_dim_out)\n",
    "\n",
    "    x = x.view(-1, channels, kernel_size[0], d_dim_out, kernel_size[1], kernel_size[2], h_dim_out, w_dim_out)  \n",
    "    # (B, C, kernel_size[0], d_dim_out, kernel_size[1], kernel_size[2], h_dim_out, w_dim_out)  \n",
    "\n",
    "    x = x.permute(0, 1, 3, 6, 7, 2, 4, 5)\n",
    "    # (B, C, d_dim_out, h_dim_out, w_dim_out, kernel_size[0], kernel_size[1], kernel_size[2])\n",
    "\n",
    "    x = x.contiguous().view(-1, channels, kernel_size[0], kernel_size[1], kernel_size[2])\n",
    "    # (B * d_dim_out * h_dim_out * w_dim_out, C, kernel_size[0], kernel_size[1], kernel_size[2])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Stitch the patches together\n",
    "\n",
    "Prep for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO COMBINE THE PATCHES BACK INTO AN IMAGE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Data loading class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicImageStaticDenoisingDataset(Dataset):\n",
    "\t\n",
    "\tdef __init__(\n",
    "\t\tself, \n",
    "\t\tdata_path: str, \n",
    "\t\tids: list,\n",
    "\t\tscale_factor = 0.5, \n",
    "\t\tsigma=0.23,  \n",
    "\t\tpatches_size = None,\n",
    "\t\tstrides= None,\n",
    "\t\textract_data=True,\n",
    "\t\tdevice: str = \"cuda\"\n",
    "\t):\n",
    "\t\tself.device = device\n",
    "\t\tself.scale_factor = scale_factor\n",
    "\n",
    "\t\tids = [str(x).zfill(2) for x in ids]\n",
    "\n",
    "\t\txf_list = []\n",
    "  \n",
    "\t\tk = 0\n",
    "\t\t# for k, img_id in enumerate(ids):\n",
    "\t\tfor folder in os.listdir(data_path):\n",
    "\t\t\timg_id = folder[:4]\t# The first 4 characters of folder name is the image id (0001, 0002, ..., 0200)\n",
    "\t\t\tif img_id not in ids:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tk += 1\n",
    "\t\t\tprint(f'loading image id {img_id}, {k}/{len(ids)}')\n",
    "\t\t\t# sample_path = os.path.join(data_path, f\"MOT17-{img_id}\")\n",
    "\t\t\tsample_path = os.path.join(data_path, folder)\n",
    "\t\t\tif extract_data:\n",
    "\t\t\t\txf = self.create_dyn_img(sample_path)\n",
    "\t\t\t\txf = xf.unsqueeze(0) / 255\n",
    "\t\t\telse:\n",
    "\t\t\t\tscale_factor_str = str(self.scale_factor).replace('.','_')\n",
    "\t\t\t\txf = np.load(os.path.join(sample_path, f\"xf_scale_factor{scale_factor_str}.npy\"))\n",
    "\t\t\t\txf = torch.tensor(xf, dtype=torch.float)\n",
    "\t\t\t\txf = xf.unsqueeze(0)\t# Is it necessary???\n",
    "\t\t\t\txf = xf.unsqueeze(0) / 255\n",
    "\t\t\t\t\n",
    "\t\t\t# print(f\"xf shape: {xf.shape}\")\n",
    "\t\t\t\n",
    "\t\t\tif patches_size is not None:\n",
    "\t\t\t\tprint(f\"extracting patches of shape {patches_size}; strides {strides}\")\n",
    "\t\t\t\txfp = extract_patches_3d(xf.contiguous(), patches_size, stride=strides)\n",
    "\t\t\t\txf_list.append(xfp)\n",
    "\t\t\t\n",
    "\t\tif patches_size is not None:\n",
    "\t\t\t# will have shape (mb, 1, Nx, Ny, Nt), where mb denotes the number of patches\n",
    "\t\t\txf = torch.concat(xf_list,dim=0)\n",
    "\t\t\t\n",
    "\t\telse:\n",
    "\t\t\txf = xf.unsqueeze(0)\n",
    "\t\t\n",
    "\t\t#create temporal TV vector to detect which patches contain the most motion\n",
    "\t\txfp_tv = (xf[...,1:] - xf[...,:-1]).pow(2).sum(dim=[1,2,3,4]) #contains the TV for all patches\n",
    "\t\t\n",
    "\t\t#normalize to 1 to have a probability vector\n",
    "\t\txfp_tv /= torch.sum(xfp_tv)\n",
    "\t\t\n",
    "\t\t#sort TV in descending order --> xfp_tv_ids[0] is the index of the patch with the most motion\n",
    "\t\tself.samples_weights = xfp_tv\n",
    "\n",
    "\t\t# TODO: Disable before real deployment! \n",
    "\t\t# For testing with still images only: Change the values in samples_weights to be a range of integers from 0 to len(samples_weights)\n",
    "\t\t# Unless I do this, when I run on a set of identical images, it will give me an error:\n",
    "\t\t# RuntimeError: invalid multinomial distribution (encountering probability entry < 0)\n",
    "\t\tself.samples_weights = torch.arange(len(self.samples_weights))\n",
    "\t\t\n",
    "\t\tself.xf = xf\n",
    "\t\tself.len = xf.shape[0]\n",
    "\t\t\n",
    "\t\tif isinstance(sigma, float):\n",
    "\t\t\tself.noise_level = 'constant'\n",
    "\t\t\tself.sigma = sigma\n",
    "\n",
    "\t\telif isinstance(sigma, (tuple, list)):\n",
    "\t\t\tself.noise_level = 'variable'\n",
    "\t\t\tself.sigma_min = sigma[0]\n",
    "\t\t\tself.sigma_max = sigma[1]\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(\"Invalid sigma value provided, must be float, tuple or list.\")\n",
    "\n",
    "\tdef create_dyn_img(self, sample_path: str):\n",
    "\t\t\n",
    "\t\tfiles_path = sample_path\n",
    "\t\tfiles_list = os.listdir(files_path)\n",
    "\t\txf = []\n",
    "\n",
    "\t\tfor file in files_list:\n",
    "\t\t\tif not file.startswith('GT'):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\timage = Image.open(os.path.join(files_path, file))\n",
    "\t\t\t\n",
    "\t\t\t#resize\n",
    "\t\t\tNx_,Ny_ = np.int(np.floor(self.scale_factor * image.width )), np.int(np.floor(self.scale_factor * image.height ))\n",
    "\t\t\timage = image.resize( (Nx_, Ny_) )\n",
    "\t\t\t\n",
    "\t\t\t#convert to grey_scale\n",
    "\t\t\timage = image.convert('L')\n",
    "\t\t\timage_data = np.asarray(image)\n",
    "\t\t\txf.append(image_data)\n",
    "\t\t\t\n",
    "\t\txf = np.stack(xf, axis=-1)\n",
    "\t\t# print(f\"xf shape: {xf.shape}\")\n",
    "\t\t\n",
    "\t\tscale_factor_str = str(self.scale_factor).replace('.','_')\n",
    "\t\tnp.save(os.path.join(sample_path, f\"xf_scale_factor{scale_factor_str}.npy\"), xf)\n",
    "\t\t\n",
    "\t\treturn torch.tensor(xf, dtype = torch.float)\n",
    "\t\t\t\n",
    "\tdef __getitem__(self, index):\n",
    "\n",
    "\t\tstd = torch.std(self.xf[index])\n",
    "\t\tmu = torch.mean(self.xf[index])\n",
    "\n",
    "\t\tx_centred = (self.xf[index]  - mu) / std\n",
    "\n",
    "\t\tif self.noise_level == 'constant':\n",
    "\t\t\tsigma = self.sigma\n",
    "\t\t\t\n",
    "\t\telif self.noise_level == 'variable':\n",
    "\t\t\tsigma = self.sigma_min + torch.rand(1) * ( self.sigma_max - self.sigma_min )\n",
    "\n",
    "\t\tx_centred += sigma * torch.randn(self.xf[index].shape, dtype = self.xf[index].dtype)\n",
    "\n",
    "\t\txnoise = std * x_centred + mu\n",
    "  \n",
    "\t\treturn (\n",
    "\t\t\txnoise.to(device=self.device),\n",
    "   \t\t\tself.xf[index].to(device=self.device)\n",
    "        )\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading image id 0001, 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8218/854378605.py:97: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Nx_,Ny_ = np.int(np.floor(self.scale_factor * image.width )), np.int(np.floor(self.scale_factor * image.height ))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting patches of shape [192, 192, 1]; strides [192, 192, 1]\n",
      "loading image id 0002, 1/1\n",
      "extracting patches of shape [192, 192, 1]; strides [192, 192, 1]\n"
     ]
    }
   ],
   "source": [
    "# CODE TO CREATE DATA LOADER HERE\n",
    "\n",
    "TRAINING = [2, 4, 5, 9, 10]\n",
    "VALIDATION = [11, 13]\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "\n",
    "# %%\n",
    "# Dynamic Image Denoising Dataset\n",
    "\n",
    "data_path = \"../../data/dyn_img_static/tmp/SIDD_Small_sRGB_Only/Data\"\n",
    "# Make sure that the dataset was downloaded successfully\n",
    "# The script to download the MOT17Det dataset can be found here: /data/dyn_img/download_mot_data.py\n",
    "# The data samples can be created with different scaling factors.\n",
    "# Make sure to set extract_data to True when loading the dataset for the first time to create the dynamic images.\n",
    "# Once the data for a specific scaling factor has been created the flag can be set to False.\n",
    "dataset_train = DynamicImageStaticDenoisingDataset(\n",
    "    data_path=data_path,\n",
    "    # ids=TRAINING,     # paper\n",
    "    ids=[\"0001\"],            # testing\n",
    "    scale_factor=0.25,\n",
    "    sigma=[0.1, 0.3],\n",
    "    strides=[192, 192, 1],\n",
    "    patches_size=[192, 192, 1],\n",
    "    # (!) Make sure to set the following flag to True when loading the dataset for the first time.\n",
    "    extract_data=True,\n",
    "    # extract_data=False,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Create training dataloader\n",
    "sampler = WeightedRandomSampler(dataset_train.samples_weights, len(dataset_train.samples_weights))\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=1, sampler=sampler)\n",
    "\n",
    "# Validation dataset (see note above)\n",
    "dataset_valid = DynamicImageStaticDenoisingDataset(\n",
    "    data_path=data_path,\n",
    "    # ids=VALIDATION,   # paper\n",
    "    ids=[\"0002\"],           # testing\n",
    "    scale_factor=0.25,\n",
    "    sigma=[0.1, 0.3],\n",
    "    strides=[192, 192, 1],\n",
    "    patches_size=[192, 192, 1],\n",
    "    # (!) Make sure to set the following flag to True when loading the dataset for the first time.\n",
    "    extract_data=True,\n",
    "    # extract_data=False,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Create validation dataloader \n",
    "sampler = WeightedRandomSampler(dataset_valid.samples_weights, len(dataset_valid.samples_weights))\n",
    "dataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=1, sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Problem introduction\n",
    "\n",
    "Image denoising\n",
    "\n",
    "Assuming the image will have additive Gaussian noise\n",
    "\n",
    "A way to judge whether an image is \"noisy\" is by seeing whether there are a lot of \"changes\" between nearby pixels.\n",
    "\n",
    "In other words, to see whether the gradient is too high.\n",
    "\n",
    "That's why there is a regularisation term in the loss function. We want to penalise large changes in gradient.\n",
    "\n",
    "Mathematically, we treat an image as a vector $\\vec{x}$.\n",
    "\n",
    "Let $\\vec{z}$ denote the vector of the image we want to denoise, and $\\vec{x}$ the output image. We wish $\\vec{x}$ to be as close to the ideal $\\vec{x}_{\\text{true}}$ as possible. Since we don't know this, we use a combination of 2 measures as our targets. One is a data-discrepancy measure. Minimising this measure means we keep as much original information in $\\vec{z}$ as possible. However, keeping all information means we will not remove any noise. Intuitively, we see an image as being noisy when there are a lot of tiny \"changes\". \n",
    "\n",
    "(?) Noise means the important information in the image is still preserved, but the image is not visually pleasing. Denoising helps make the image more visually pleasant but still contains all the imporant information. \n",
    "\n",
    "The loss function is:\n",
    "\n",
    "$$ L(\\vec{x}, \\vec{z}) = \\min_{\\vec{x}} \\frac{1}{2} || \\vec{x} - \\vec{z} ||_{2}^{2} + \\lambda || \\nabla \\vec{x} ||_{1} $$\n",
    "\n",
    "The algorithm to reconstruct the image is Primal-Dual-Hybrid-G...\n",
    "\n",
    "This algorithm requires the regularisation parameters.\n",
    "\n",
    "Good regularisation parameter will help strike a good balance between smoothing out the noisy parts and keeping as much original information as possible.\n",
    "\n",
    "Lambda too high will make the reconstructed image to lose too much information. Too low will not remove a lot of noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE OF THE LOSS FUNCTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Reconstruct an image with PDHG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function for PDHG: Gradient operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradOperators(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def diff_kernel(ndim, mode):\n",
    "        if mode == \"doublecentral\":\n",
    "            kern = torch.tensor((-1, 0, 1))\n",
    "        elif mode == \"central\":\n",
    "            kern = torch.tensor((-1, 0, 1)) / 2\n",
    "        elif mode == \"forward\":\n",
    "            kern = torch.tensor((0, -1, 1))\n",
    "        elif mode == \"backward\":\n",
    "            kern = torch.tensor((-1, 1, 0))\n",
    "        else:\n",
    "            raise ValueError(f\"mode should be one of (central, forward, backward, doublecentral), not {mode}\")\n",
    "        kernel = torch.zeros(ndim, 1, *(ndim * (3,)))\n",
    "        for i in range(ndim):\n",
    "            idx = tuple([i, 0, *(i * (1,)), slice(None), *((ndim - i - 1) * (1,))])\n",
    "            kernel[idx] = kern\n",
    "        return kernel\n",
    "\n",
    "    def __init__(self, dim:int=2, mode:str=\"doublecentral\", padmode:str = \"circular\"):\n",
    "        \"\"\"\n",
    "        An Operator for finite Differences / Gradients\n",
    "        Implements the forward as apply_G and the adjoint as apply_GH.\n",
    "        \n",
    "        Args:\n",
    "            dim (int, optional): Dimension. Defaults to 2.\n",
    "            mode (str, optional): one of doublecentral, central, forward or backward. Defaults to \"doublecentral\".\n",
    "            padmode (str, optional): one of constant, replicate, circular or refelct. Defaults to \"circular\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"kernel\", self.diff_kernel(dim, mode), persistent=False)\n",
    "        self._dim = dim\n",
    "        self._conv = (torch.nn.functional.conv1d, torch.nn.functional.conv2d, torch.nn.functional.conv3d)[dim - 1]\n",
    "        self._convT = (torch.nn.functional.conv_transpose1d, torch.nn.functional.conv_transpose2d, torch.nn.functional.conv_transpose3d)[dim - 1]\n",
    "        self._pad = partial(torch.nn.functional.pad, pad=2 * dim * (1,), mode=padmode)\n",
    "        if mode == 'central':\n",
    "            self._norm = (self.dim) ** (1 / 2)\n",
    "        else:\n",
    "            self._norm = (self.dim * 4) ** (1 / 2)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim\n",
    "    \n",
    "    def apply_G(self, x):\n",
    "        \"\"\"\n",
    "        Forward\n",
    "        \"\"\"\n",
    "        if x.is_complex():\n",
    "            xr = torch.view_as_real(x).moveaxis(-1, 0)\n",
    "        else:\n",
    "            xr = x\n",
    "        xr = xr.reshape(-1, 1, *x.shape[-self.dim :])\n",
    "        xp = self._pad(xr)\n",
    "        y = self._conv(xp, weight=self.kernel, bias=None, padding=0)\n",
    "        if x.is_complex():\n",
    "            y = y.reshape(2, *x.shape[: -self.dim], self.dim, *x.shape[-self.dim :])\n",
    "            y = torch.view_as_complex(y.moveaxis(0, -1).contiguous())\n",
    "        else:\n",
    "            y = y.reshape(*x.shape[0 : -self.dim], self.dim, *x.shape[-self.dim :])\n",
    "        return y\n",
    "\n",
    "    def apply_GH(self, x):\n",
    "        \"\"\"\n",
    "        Adjoint\n",
    "        \"\"\"\n",
    "        if x.is_complex():\n",
    "            xr = torch.view_as_real(x).moveaxis(-1, 0)\n",
    "        else:\n",
    "            xr = x\n",
    "        xr = xr.reshape(-1, self.dim, *x.shape[-self.dim :])\n",
    "        xp = self._pad(xr)\n",
    "        y = self._convT(xp, weight=self.kernel, bias=None, padding=2)\n",
    "        if x.is_complex():\n",
    "            y = y.reshape(2, *x.shape[: -self.dim - 1], *x.shape[-self.dim :])\n",
    "            y = torch.view_as_complex(y.moveaxis(0, -1).contiguous())\n",
    "        else:\n",
    "            y = y.reshape(*x.shape[: -self.dim - 1], *x.shape[-self.dim :])\n",
    "        return y\n",
    "    \n",
    "  \n",
    "    def apply_GHG(self, x):\n",
    "        if x.is_complex():\n",
    "            xr = torch.view_as_real(x).moveaxis(-1, 0)\n",
    "        else:\n",
    "            xr = x\n",
    "        xr = xr.reshape(-1, 1, *x.shape[-self.dim :])\n",
    "        xp = self._pad(xr)\n",
    "        tmp = self._conv(xp, weight=self.kernel, bias=None, padding=0)\n",
    "        tmp = self._pad(tmp)\n",
    "        y = self._convT(tmp, weight=self.kernel, bias=None, padding=2)\n",
    "        if x.is_complex():\n",
    "            y = y.reshape(2, *x.shape)\n",
    "            y = torch.view_as_complex(y.moveaxis(0, -1).contiguous())\n",
    "        else:\n",
    "            y = y.reshape(*x.shape)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x, direction=1):\n",
    "        if direction>0:\n",
    "            return self.apply_G(x)\n",
    "        elif direction<0:\n",
    "            return self.apply_GH(x)\n",
    "        else:\n",
    "            return self.apply_GHG(x)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def normGHG(self):\n",
    "        return self._norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function for PDHG: Clip act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipAct(nn.Module):\n",
    "    def forward(self, x, threshold):\n",
    "        return clipact(x, threshold)\n",
    "\n",
    "\n",
    "def clipact(x, threshold):\n",
    "    is_complex = x.is_complex()\n",
    "    if is_complex:\n",
    "        x = torch.view_as_real(x)\n",
    "        threshold = threshold.unsqueeze(-1)\n",
    "    x = torch.clamp(x, -threshold, threshold)\n",
    "    if is_complex:\n",
    "        x = torch.view_as_complex(x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full PDHG code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE OF THE PDHG RECONSTRUCTION ALGORITHM HERE\n",
    "\n",
    "\n",
    "class DynamicImageStaticPrimalDualNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        T=128,\n",
    "        cnn_block=None,\n",
    "        mode=\"lambda_cnn\",\n",
    "        up_bound=0,\n",
    "        phase=\"training\",\n",
    "    ):\n",
    "        # print(f\"Running: {DynamicImageStaticPrimalDualNN.__name__}\")\n",
    "        super(DynamicImageStaticPrimalDualNN, self).__init__()\n",
    "\n",
    "        # gradient operators and clipping function\n",
    "        dim = 3\n",
    "        self.GradOps = GradOperators(dim, mode=\"forward\", padmode=\"circular\")\n",
    "\n",
    "        # operator norms\n",
    "        self.op_norm_AHA = torch.sqrt(torch.tensor(1.0))\n",
    "        self.op_norm_GHG = torch.sqrt(torch.tensor(12.0))\n",
    "        # operator norm of K = [A, \\nabla]\n",
    "        # https://iopscience.iop.org/article/10.1088/0031-9155/57/10/3065/pdf,\n",
    "        # see page 3083\n",
    "        self.L = torch.sqrt(self.op_norm_AHA**2 + self.op_norm_GHG**2)\n",
    "\n",
    "        # function for projecting\n",
    "        self.ClipAct = ClipAct()\n",
    "\n",
    "        if mode == \"lambda_xyt\":\n",
    "            # one single lambda for x,y and t\n",
    "            self.lambda_reg = nn.Parameter(torch.tensor([-1.5]), requires_grad=True)\n",
    "\n",
    "        elif mode == \"lambda_xy_t\":\n",
    "            # one (shared) lambda for x,y and one lambda for t\n",
    "            self.lambda_reg = nn.Parameter(\n",
    "                torch.tensor([-4.5, -1.5]), requires_grad=True\n",
    "            )\n",
    "\n",
    "        elif mode == \"lambda_cnn\":\n",
    "            # the CNN-block to estimate the lambda regularization map\n",
    "            # must be a CNN yielding a two-channeld output, i.e.\n",
    "            # one map for lambda_cnn_xy and one map for lambda_cnn_t\n",
    "            self.cnn = cnn_block    # NOTE: This is actually the UNET!!! (At least in this project)\n",
    "            self.up_bound = torch.tensor(up_bound)\n",
    "\n",
    "        # number of terations\n",
    "        self.T = T\n",
    "        self.mode = mode\n",
    "\n",
    "        # constants depending on the operators\n",
    "        self.tau = nn.Parameter(\n",
    "            torch.tensor(10.0), requires_grad=True\n",
    "        )  # starting value approximately  1/L\n",
    "        self.sigma = nn.Parameter(\n",
    "            torch.tensor(10.0), requires_grad=True\n",
    "        )  # starting value approximately  1/L\n",
    "\n",
    "        # theta should be in \\in [0,1]\n",
    "        self.theta = nn.Parameter(\n",
    "            torch.tensor(10.0), requires_grad=True\n",
    "        )  # starting value approximately  1\n",
    "\n",
    "        # distinguish between training and test phase;\n",
    "        # during training, the input is padded using \"reflect\" padding, because\n",
    "        # patches are used by reducing the number of temporal points;\n",
    "        # while testing, \"reflect\" padding is used in x,y- direction, while\n",
    "        # circular padding is used in t-direction\n",
    "        self.phase = phase\n",
    "\n",
    "    def get_lambda_cnn(self, x):\n",
    "        # padding\n",
    "        # arbitrarily chosen, maybe better to choose it depending on the\n",
    "        # receptive field of the CNN or so;\n",
    "        # seems to be important in order not to create \"holes\" in the\n",
    "        # lambda_maps in t-direction\n",
    "        npad_xy = 4\n",
    "        # npad_t = 8\n",
    "        npad_t = 0 # TODO: Time dimension should not be necessary for single image input.\n",
    "        # I changed the npad_t to 0 so that I can run on single image input without change the 3D type config. It seems that the number of frames must be greater than npad_t?\n",
    "\n",
    "        pad = (npad_t, npad_t, npad_xy, npad_xy, npad_xy, npad_xy)\n",
    "\n",
    "        if self.phase == \"training\":\n",
    "            x = F.pad(x, pad, mode=\"reflect\")\n",
    "\n",
    "        elif self.phase == \"testing\":\n",
    "            pad_refl = (0, 0, npad_xy, npad_xy, npad_xy, npad_xy)\n",
    "            pad_circ = (npad_t, npad_t, 0, 0, 0, 0)\n",
    "\n",
    "            x = F.pad(x, pad_refl, mode=\"reflect\")\n",
    "            x = F.pad(x, pad_circ, mode=\"circular\")\n",
    "\n",
    "        # estimate parameter map\n",
    "        lambda_cnn = self.cnn(x) # NOTE: The cnn is actually the UNET block!!! (At least in this project)\n",
    "\n",
    "        # crop\n",
    "        neg_pad = tuple([-pad[k] for k in range(len(pad))])\n",
    "        lambda_cnn = F.pad(lambda_cnn, neg_pad)\n",
    "\n",
    "        # double spatial map and stack\n",
    "        lambda_cnn = torch.cat((lambda_cnn[:, 0, ...].unsqueeze(1), lambda_cnn), dim=1)\n",
    "\n",
    "        # constrain map to be striclty positive; further, bound it from below\n",
    "        if self.up_bound > 0:\n",
    "            # constrain map to be striclty positive; further, bound it from below\n",
    "            lambda_cnn = self.up_bound * self.op_norm_AHA * torch.sigmoid(lambda_cnn)\n",
    "        else:\n",
    "            lambda_cnn = 0.1 * self.op_norm_AHA * F.softplus(lambda_cnn)\n",
    "\n",
    "        return lambda_cnn\n",
    "\n",
    "    def forward(self, x, lambda_map=None):\n",
    "        # initial reconstruction\n",
    "        mb, _, Nx, Ny, Nt = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # starting values\n",
    "        xbar = x.clone()\n",
    "        x0 = x.clone()\n",
    "        xnoisy = x.clone()\n",
    "\n",
    "        # dual variable\n",
    "        p = x.clone()\n",
    "        q = torch.zeros(mb, 3, Nx, Ny, Nt, dtype=x.dtype).to(device)\n",
    "\n",
    "        # sigma, tau, theta\n",
    "        sigma = (1 / self.L) * torch.sigmoid(self.sigma)  # \\in (0,1/L)\n",
    "        tau = (1 / self.L) * torch.sigmoid(self.tau)  # \\in (0,1/L)\n",
    "        theta = torch.sigmoid(self.theta)  # \\in (0,1)\n",
    "\n",
    "        # distinguish between the different cases\n",
    "        if self.mode == \"lambda_xyt\":\n",
    "            lambda_reg = F.softplus(self.lambda_reg)  # \\in (0,\\infty)\n",
    "\n",
    "        elif self.mode == \"lambda_xy_t\":\n",
    "            # get xy- and t-lambda\n",
    "            lambda_reg_xy = torch.stack(2 * [self.lambda_reg[0]])\n",
    "            lambda_reg_t = self.lambda_reg[1].unsqueeze(0)\n",
    "\n",
    "            # conatentate xy -and t-lambda\n",
    "            lambda_reg = (\n",
    "                torch.cat([lambda_reg_xy, lambda_reg_t])\n",
    "                .unsqueeze(0)\n",
    "                .unsqueeze(-1)\n",
    "                .unsqueeze(-1)\n",
    "                .unsqueeze(-1)\n",
    "            )\n",
    "            lambda_reg = F.softplus(lambda_reg)\n",
    "\n",
    "        elif self.mode == \"lambda_cnn\":\n",
    "            if lambda_map is None:\n",
    "                # estimate lambda reg from the image\n",
    "                lambda_reg = self.get_lambda_cnn(x)\n",
    "            else:\n",
    "                lambda_reg = lambda_map\n",
    "\n",
    "        # Algorithm 2 - Unrolled PDHG algorithm (page 18)\n",
    "        # TODO: In the paper, L is one of the inputs but not used anywhere in the pseudo code???\n",
    "        for kT in range(self.T):\n",
    "            # update p\n",
    "            p =  (p + sigma * (xbar - xnoisy) ) / (1. + sigma)\n",
    "            # update q\n",
    "            q = self.ClipAct(q + sigma * self.GradOps.apply_G(xbar), lambda_reg)\n",
    "\n",
    "            x1 = x0 - tau * p - tau * self.GradOps.apply_GH(q)\n",
    "\n",
    "            if kT != self.T - 1:\n",
    "                # update xbar\n",
    "                xbar = x1 + theta * (x1 - x0)\n",
    "                x0 = x1\n",
    "\n",
    "        return x1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo PDHG with a single $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO DEMO THE PDHG RECONSTRUCTION WITH A FIXED REGULARISATION PARAMETER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Calculate the gradient\n",
    "\n",
    "The gradient is a Laplacian ?\n",
    "\n",
    "There are $x$ gradient and $y$ gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO CALCULATE THE GRADIENT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### General architecture\n",
    "\n",
    "The whole architecture can be seen as unsupervised: The data only contains (clean) images.\n",
    "\n",
    "The whole model: Input is an image. Output is also an image.\n",
    "\n",
    "The UNET actually only outputs the regularisation parameter map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE OF THE GENERAL ARCHITECTURE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### UNET\n",
    "\n",
    "The specific UNET architecture we use has the following parts:\n",
    "\n",
    "...\n",
    "\n",
    "We use Leaky RELU instead of RELU or Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE OF THE UNET HERE\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    A block of convolutional layers (1D, 2D or 3D)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in,\n",
    "        n_ch_out,\n",
    "        n_convs,\n",
    "        kernel_size=3,\n",
    "        bias=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if dim == 1:\n",
    "            conv_op = nn.Conv1d\n",
    "        if dim == 2:\n",
    "            conv_op = nn.Conv2d\n",
    "        elif dim == 3:\n",
    "            conv_op = nn.Conv3d\n",
    "\n",
    "        padding = int(np.floor(kernel_size / 2))\n",
    "\n",
    "        conv_block_list = []\n",
    "        conv_block_list.extend(\n",
    "            [\n",
    "                conv_op(\n",
    "                    n_ch_in,\n",
    "                    n_ch_out,\n",
    "                    kernel_size,\n",
    "                    padding=padding,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                ),\n",
    "                nn.LeakyReLU(), # NOTE: We use LeakyReLU instead of ReLU!!!\n",
    "                # Can we try using ReLU instead of LeakyReLU?\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i in range(n_convs - 1):\n",
    "            conv_block_list.extend(\n",
    "                [\n",
    "                    conv_op(\n",
    "                        n_ch_out,\n",
    "                        n_ch_out,\n",
    "                        kernel_size,\n",
    "                        padding=padding,\n",
    "                        bias=bias,\n",
    "                        padding_mode=padding_mode,\n",
    "                    ),\n",
    "                    nn.LeakyReLU(), # NOTE: We use LeakyReLU instead of ReLU!!!\n",
    "                    # Can we try using ReLU instead of LeakyReLU?\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in,\n",
    "        n_enc_stages,\n",
    "        n_convs_per_stage,\n",
    "        n_filters,\n",
    "        kernel_size=3,\n",
    "        bias=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        n_ch_list = [n_ch_in]\n",
    "        for ne in range(n_enc_stages):\n",
    "            n_ch_list.append(int(n_filters) * 2**ne)\n",
    "\n",
    "        self.enc_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(\n",
    "                    dim,\n",
    "                    n_ch_list[i],\n",
    "                    n_ch_list[i + 1],\n",
    "                    n_convs_per_stage,\n",
    "                    kernel_size=kernel_size,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                )\n",
    "                for i in range(len(n_ch_list) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if dim == 1:\n",
    "            pool_op = nn.MaxPool1d(2)\n",
    "        elif dim == 2:\n",
    "            pool_op = nn.MaxPool2d(2)\n",
    "        elif dim == 3:\n",
    "            # TODO: Can I make it so that if there is only a single image then it reduces the pool size to 1?\n",
    "            # pool_op = nn.MaxPool3d(2)\n",
    "            pool_op = nn.MaxPool3d(1) # TODO: Change back after making the code working with 2D MaxPool\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dim value: {dim}\")\n",
    "\n",
    "        self.pool = pool_op\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "            x = self.pool(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in,\n",
    "        n_dec_stages,\n",
    "        n_convs_per_stage,\n",
    "        n_filters,\n",
    "        kernel_size=3,\n",
    "        bias=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        n_ch_list = []\n",
    "        for ne in range(n_dec_stages):\n",
    "            n_ch_list.append(int(n_ch_in * (1 / 2) ** ne))\n",
    "\n",
    "        if dim == 1:\n",
    "            conv_op = nn.Conv1d\n",
    "            interp_mode = \"linear\"\n",
    "        elif dim == 2:\n",
    "            conv_op = nn.Conv2d\n",
    "            interp_mode = \"bilinear\"\n",
    "        elif dim == 3:\n",
    "            interp_mode = \"trilinear\"\n",
    "            conv_op = nn.Conv3d\n",
    "\n",
    "        self.interp_mode = interp_mode\n",
    "\n",
    "        padding = int(np.floor(kernel_size / 2))\n",
    "        self.upconvs = nn.ModuleList(\n",
    "            [\n",
    "                conv_op(\n",
    "                    n_ch_list[i],\n",
    "                    n_ch_list[i + 1],\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                )\n",
    "                for i in range(len(n_ch_list) - 1)\n",
    "            ]\n",
    "        )\n",
    "        self.dec_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(\n",
    "                    dim,\n",
    "                    n_ch_list[i],\n",
    "                    n_ch_list[i + 1],\n",
    "                    n_convs_per_stage,\n",
    "                    kernel_size=kernel_size,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                )\n",
    "                for i in range(len(n_ch_list) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.dec_blocks)):\n",
    "            enc_features = encoder_features[i]\n",
    "            enc_features_shape = enc_features.shape\n",
    "            x = nn.functional.interpolate(\n",
    "                x, enc_features_shape[2:], mode=self.interp_mode, align_corners=False\n",
    "            )\n",
    "            x = self.upconvs[i](x)\n",
    "            x = torch.cat([x, enc_features], dim=1)\n",
    "            x = self.dec_blocks[i](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in=2,\n",
    "        n_ch_out=2,\n",
    "        n_enc_stages=3,\n",
    "        n_convs_per_stage=2,\n",
    "        n_filters=16,\n",
    "        kernel_size=3,\n",
    "        res_connection=False,\n",
    "        bias=True,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            dim,\n",
    "            n_ch_in,\n",
    "            n_enc_stages,\n",
    "            n_convs_per_stage,\n",
    "            n_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            dim,\n",
    "            n_filters * (2 ** (n_enc_stages - 1)),\n",
    "            n_enc_stages,\n",
    "            n_convs_per_stage,\n",
    "            n_filters * (n_enc_stages * 2),\n",
    "            kernel_size=kernel_size,\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "\n",
    "        if dim == 1:\n",
    "            conv_op = nn.Conv1d\n",
    "        elif dim == 2:\n",
    "            conv_op = nn.Conv2d\n",
    "        elif dim == 3:\n",
    "            conv_op = nn.Conv3d\n",
    "\n",
    "        self.c1x1 = conv_op(n_filters, n_ch_out, kernel_size=1, padding=0, bias=bias)\n",
    "        if res_connection:\n",
    "            if n_ch_in == n_ch_out:\n",
    "                self.res_connection = lambda x: x\n",
    "            else:\n",
    "                self.res_connection = conv_op(n_ch_in, n_ch_out, 1)\n",
    "        else:\n",
    "            self.res_connection = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_features = self.encoder(x)\n",
    "        dec = self.decoder(enc_features[-1], enc_features[::-1][1:])\n",
    "        out = self.c1x1(dec)\n",
    "        if self.res_connection:\n",
    "            out = out + self.res_connection(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE OF ONE EPOCH TRAINING AND VALIDATION HERE\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(model, data, optimizer, loss_func) -> float:\n",
    "    \"\"\"Perform the training of one epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Model to be trained\n",
    "    data\n",
    "        Dataloader with training data\n",
    "    optimizer\n",
    "        Pytorch optimizer, e.g. Adam\n",
    "    loss_func\n",
    "        Loss function to be calculated, e.g. MSE\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        training loss\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        loss is NaN\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "\n",
    "    # for sample in data: # If cannot install tqdm\n",
    "    for sample in tqdm(data): # tqdm helps show a nice progress bar\n",
    "        optimizer.zero_grad(set_to_none=True)  # Zero your gradients for every batch!\n",
    "        \n",
    "        sample, label = sample\n",
    "        # print(f\"sample: {sample.shape}\")\n",
    "        output = model(sample)\n",
    "        loss = loss_func(label, output)\n",
    "        loss.backward()\n",
    "        \n",
    "        if loss.item() != loss.item():\n",
    "            raise ValueError(\"NaN returned by loss function...\")\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(data.dataset)\n",
    "\n",
    "\n",
    "def validate_epoch(model, data, loss_func) -> float:\n",
    "    \"\"\"Perform the validation of one epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Model to be trained\n",
    "    data\n",
    "        Dataloader with validation data\n",
    "    loss_func\n",
    "        Loss function to be calculated, e.g. MSE\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        validation loss\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    \n",
    "    # for sample in data: # If cannot install tqdm\n",
    "    for sample in tqdm(data): # tqdm helps show a nice progress bar\n",
    "        inputs, labels = sample\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(data.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start training\n",
    "\n",
    "We use Adam optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:06<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING LOSS:  0.0003650111862548834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:02<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING LOSS:  0.00038018165731854323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 29.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION LOSS:  0.0004986642888979986\n"
     ]
    }
   ],
   "source": [
    "# CODE FOR TRAINING LOOP HERE\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# Define CNN block and PDHG-method\n",
    "unet = UNet(dim=3, n_ch_in=1).to(DEVICE)\n",
    "\n",
    "# Constrct primal-dual operator with nn\n",
    "pdhg = DynamicImageStaticPrimalDualNN(\n",
    "    cnn_block=unet, \n",
    "    T=128,\n",
    "    phase=\"training\",\n",
    "    up_bound=0.5,\n",
    "    # Select mode:\n",
    "    mode=\"lambda_cnn\",\n",
    "    # mode=\"lambda_xy_t\",\n",
    "    # mode=\"lambda_xyt\",\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(pdhg.parameters(), lr=1e-4)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 2          # testing\n",
    "# num_epochs = 100      # paper\n",
    "\n",
    "model_states_dir = \"./tmp/states\"\n",
    "os.makedirs(model_states_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Model training\n",
    "    pdhg.train(True)\n",
    "    training_loss = train_epoch(pdhg, dataloader_train, optimizer, loss_function)\n",
    "    pdhg.train(False)\n",
    "    print(\"TRAINING LOSS: \", training_loss)\n",
    "\n",
    "    if (epoch+1) % 2 == 0:\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Model validation\n",
    "            validation_loss = validate_epoch(pdhg, dataloader_valid, loss_function)\n",
    "            print(\"VALIDATION LOSS: \", validation_loss)\n",
    "            torch.save(pdhg.state_dict(), f\"{model_states_dir}/epoch_{str(epoch).zfill(3)}.pt\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Save the entire model\n",
    "torch.save(pdhg, f\"./tmp/model.pt\")\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO VALIDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Inference\n",
    "\n",
    "Demo the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO INFER AND SHOW SOME RESULTS HERE\n",
    "\n",
    "\n",
    "# Use pdhg for prediction\n",
    "# clean_image = \n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataloader_train):\n",
    "        # xf = data[\"xf\"].to(DEVICE)\n",
    "        # y = data[\"y\"].to(DEVICE)\n",
    "        # xf_hat = pdhg(xf)\n",
    "\n",
    "        # print(f\"xf shape: {xf.shape}\")\n",
    "        # print(f\"y shape: {y.shape}\")\n",
    "        # print(f\"xf_hat shape: {xf_hat.shape}\")\n",
    "\n",
    "        # print(f\"type(data): {type(data)}\")\n",
    "        # print(f\"len(data): {len(data)}\")\n",
    "        # print(f\"type(data[0]): {type(data[0])}\")\n",
    "        output = pdhg(data[0])\n",
    "        # print(f\"type(output): {type(output)}\")\n",
    "\n",
    "        # Save output as image\n",
    "        output = output.squeeze().cpu().numpy()\n",
    "        # print(f\"output shape: {output.shape}\")\n",
    "        # print(f\"output type: {output.dtype}\")\n",
    "\n",
    "        # Save to image\n",
    "        output = np.clip(output, 0, 1)\n",
    "        output = (output * 255).astype(np.uint8)\n",
    "        output = Image.fromarray(output)\n",
    "        output.save(f\"./tmp/images/output_{i}.png\")\n",
    "\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
