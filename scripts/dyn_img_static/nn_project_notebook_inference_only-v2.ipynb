{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Requirements\n",
    "```python\n",
    "# numpy==1.23.4\n",
    "# matplotlib\n",
    "torch\n",
    "# scikit-image\n",
    "# odl==0.7.0\n",
    "# dival\n",
    "# wget # for downloading the test data\n",
    "pillow # Image utils\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pillow\n",
    "# !pip install torch\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# NOTE: Importing torch the first time will always take a long time!\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Optional\n",
    "from tqdm import tqdm # progress bar\n",
    "\n",
    "import wandb # Optional, for logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Path: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISABLING_TESTS = False\n",
    "# DISABLING_TESTS = True   # Disable tests for less output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(f\"Using {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(f\"Using {torch.backends.mps.get_device_name(0)} with MPS\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "torch.set_default_device(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Import the image and transform the data\n",
    "\n",
    "Read the image\n",
    "\n",
    "Visualise one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL CODE TO DOWNLOAD THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_npy_file(sample_path: str, scale_factor: float) -> np.ndarray:\t\t\t\n",
    "    scale_factor_str = str(scale_factor).replace('.','_')\n",
    "    xf = np.load(os.path.join(sample_path, f\"xf_scale_factor{scale_factor_str}.npy\"))\n",
    "    xf = torch.tensor(xf, dtype=torch.float)\n",
    "    xf = xf.unsqueeze(0) / 255\n",
    "    return xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO GET AN IMAGE FROM LOCAL FILE SYSTEM\n",
    "# TODO: CHANGE THIS TO YOUR PATH\n",
    "# NOTE: Windows uses \\\\ instead of /\n",
    "def load_images(ids: list, take_npy_files: bool) -> list:\n",
    "    data_path = \"../../data/dyn_img_static/tmp/SIDD_Small_sRGB_Only/Data\"\n",
    "    k = 0\n",
    "\n",
    "    images = []\n",
    "\n",
    "    for folder in os.listdir(data_path):\n",
    "        img_id = folder[:4]\t# The first 4 characters of folder name is the image id (0001, 0002, ..., 0200)\n",
    "        if img_id not in ids:\n",
    "            continue\n",
    "        k += 1\n",
    "        print(f'loading image id {img_id}, {k}/{len(ids)}')\n",
    "\n",
    "        files_path = os.path.join(data_path, folder)\n",
    "\n",
    "        # if take_npy_files:\n",
    "        #     xf = get_npy_file(files_path, scale_factor)\n",
    "        #     images.append(xf)\n",
    "        #     continue\n",
    "\n",
    "        # Use only the ground truth images\n",
    "        file = \"GT_SRGB_010.PNG\"  # GT = Ground Truth\n",
    "\n",
    "        image = Image.open(os.path.join(files_path, file))\n",
    "        assert image.mode == 'RGB', f\"Image mode is not RGB: {image.mode}\" # For now, expect RGB images\n",
    "        # width, height = image.size\n",
    "        # Width and height are NOT always the same\n",
    "        # assert (width, height) == (5328, 3000), f\"Image size is not 5328x3000: {image.size}\" # For now, expect 5328x3000 images\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_images():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for img in load_images([\"0001\"], False):\n",
    "    for img in load_images([\"0065\"], False):\n",
    "        print(img.size)\n",
    "        plt.imshow(img)\n",
    "\n",
    "test_load_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "#### Convert image to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO CONVERT AN IMAGE TO GRAYSCALE\n",
    "def convert_to_grayscale(image: Image) -> Image:\n",
    "    return image.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert_to_grayscale():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for img in load_images([\"0001\"], False):\n",
    "    for img in load_images([\"0065\"], False):\n",
    "        img = convert_to_grayscale(img)\n",
    "        plt.imshow(img, cmap='gray') # cmap='gray' for proper display in black and white. It does not convert the image to grayscale.\n",
    "\n",
    "test_convert_to_grayscale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resize image\n",
    "\n",
    "(Optional) Rescale the image\n",
    "\n",
    "Make the image a bit smaller\n",
    "\n",
    "Example:\n",
    "\n",
    "Original 5328 x 3000\n",
    "\n",
    "Recaled 0.5 to 2664 x 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, scale_factor):\n",
    "    Nx_,Ny_ = np.int(np.floor(scale_factor * image.width )), np.int(np.floor(scale_factor * image.height ))\n",
    "    image = image.resize( (Nx_, Ny_) )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_resize_image():\n",
    "    if DISABLING_TESTS: return\n",
    "    for img in load_images([\"0065\"], False):\n",
    "        img = convert_to_grayscale(img)\n",
    "        img = resize_image(img, 0.05) # Extreme size reduction for demonstration\n",
    "        print(img.size)\n",
    "        plt.imshow(img, cmap='gray') # cmap='gray' for proper display in black and white. It does not convert the image to grayscale.\n",
    "\n",
    "test_resize_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numpy(image):\n",
    "    image_data = np.asarray(image)\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert_to_numpy():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for img in load_images([\"0001\"], False):\n",
    "    for img in load_images([\"0065\"], False):\n",
    "        img = convert_to_grayscale(img)\n",
    "        print(f\"Before conversion: {type(img)}\")\n",
    "        image_data = convert_to_numpy(img)\n",
    "        print(f\"After conversion: {type(image_data)}\")\n",
    "        # plt.imshow still works with numpy array\n",
    "        plt.imshow(image_data, cmap='gray')\n",
    "\n",
    "test_convert_to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to tensor\n",
    "\n",
    "For efficient computation on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(image_data):\n",
    "    xf = []\n",
    "    xf.append(image_data)\n",
    "    xf = np.stack(xf, axis=-1)\n",
    "    xf = torch.tensor(xf, dtype=torch.float)\n",
    "    xf = xf.unsqueeze(0) / 255\n",
    "    return xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert_to_tensor():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for img in load_images([\"0001\"], False):\n",
    "    for img in load_images([\"0065\"], False):\n",
    "        img = convert_to_grayscale(img)\n",
    "        image_data = convert_to_numpy(img)\n",
    "        image_data = convert_to_tensor(image_data)\n",
    "        print(image_data.size())\n",
    "        plt.imshow(image_data.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "\n",
    "test_convert_to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add synthetic noise\n",
    "\n",
    "artificial Gaussian noise\n",
    "\n",
    "Noise can occur in reality.\n",
    "\n",
    "It is difficult to obtain a pair of clean and noisy images of one exact same scene.\n",
    "\n",
    "For training, it is common to add synthetic noise to an image that is considered clean and then try to reconstruct it.\n",
    "\n",
    "There are many types of noise and different ways to add noise. We can add salt-and-pepper noise. (?)We can add more noise in some parts and less in others. We can use a combination of noise-adding strategies to build more robust models.\n",
    "\n",
    "For our purpose, we will focus on Gaussian noise. This is sufficient for most cases. \n",
    "\n",
    "(?) We will add noise with the same probability for each pixel (not using the strategies of focusing on certain regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO ADD NOISE HERE\n",
    "def get_variable_noise(sigma_min, sigma_max):\n",
    "    return sigma_min + torch.rand(1) * (sigma_max - sigma_min)\n",
    "\n",
    "def add_noise(xf: torch.tensor, sigma) -> torch.tensor:\n",
    "    std = torch.std(xf)\n",
    "    mu = torch.mean(xf)\n",
    "\n",
    "    x_centred = (xf  - mu) / std\n",
    "\n",
    "    x_centred += sigma * torch.randn(xf.shape, dtype = xf.dtype)\n",
    "\n",
    "    xnoise = std * x_centred + mu\n",
    "\n",
    "    return xnoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_noise():\n",
    "    if DISABLING_TESTS: return\n",
    "    for rgb_image in load_images([\"0065\"], False):\n",
    "        grayscale_image = convert_to_grayscale(rgb_image)\n",
    "        grayscale_image = resize_image(grayscale_image, 0.05)\n",
    "        image_data = convert_to_numpy(grayscale_image)\n",
    "        image_data = convert_to_tensor(image_data)\n",
    "        constant_noise_img = add_noise(image_data, sigma=0.1)\n",
    "        variable_noise_img = add_noise(image_data, get_variable_noise(\n",
    "            sigma_min=0.1, sigma_max=0.2))\n",
    "        plt.imshow(image_data.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "        plt.imshow(constant_noise_img.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "        plt.imshow(variable_noise_img.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "test_add_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Calculate PSNR\n",
    "\n",
    "PSNR is a common metrics for noisy image.\n",
    "\n",
    "Compare before and after adding synthetic noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(original, compressed): \n",
    "    mse = torch.mean((original - compressed) ** 2) \n",
    "    if(mse == 0): # MSE is zero means no noise is present in the signal. \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    # max_pixel = 255.0\n",
    "    max_pixel = 1.0\n",
    "    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse)) \n",
    "    return psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_PSNR():\n",
    "    if DISABLING_TESTS: return\n",
    "    for rgb_image in load_images([\"0065\"], False):\n",
    "        grayscale_image = convert_to_grayscale(rgb_image)\n",
    "        grayscale_image = resize_image(grayscale_image, 0.5)\n",
    "        image_data = convert_to_numpy(grayscale_image)\n",
    "        image_data = convert_to_tensor(image_data)\n",
    "\n",
    "        print(f\"PSNR of original image: {PSNR(image_data, image_data)} dB\")\n",
    "        plt.imshow(image_data.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "        noisy_img = add_noise(image_data, sigma=0.5)\n",
    "        print(f\"PSNR of constant noise image: {PSNR(image_data, noisy_img):.2f} dB\")\n",
    "        plt.imshow(noisy_img.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "test_PSNR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Reconstruct an image with PDHG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the gradient\n",
    "\n",
    "The gradient is a Laplacian ?\n",
    "\n",
    "There are $x$ gradient and $y$ gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from ...\n",
    "\n",
    "class GradOperators(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def diff_kernel(ndim, mode):\n",
    "        if mode == \"doublecentral\":\n",
    "            kern = torch.tensor((-1, 0, 1))\n",
    "        elif mode == \"central\":\n",
    "            kern = torch.tensor((-1, 0, 1)) / 2\n",
    "        elif mode == \"forward\":\n",
    "            kern = torch.tensor((0, -1, 1))\n",
    "        elif mode == \"backward\":\n",
    "            kern = torch.tensor((-1, 1, 0))\n",
    "        else:\n",
    "            raise ValueError(f\"mode should be one of (central, forward, backward, doublecentral), not {mode}\")\n",
    "        kernel = torch.zeros(ndim, 1, *(ndim * (3,)))\n",
    "        for i in range(ndim):\n",
    "            idx = tuple([i, 0, *(i * (1,)), slice(None), *((ndim - i - 1) * (1,))])\n",
    "            kernel[idx] = kern\n",
    "        return kernel\n",
    "\n",
    "    def __init__(self, dim:int=2, mode:str=\"doublecentral\", padmode:str = \"circular\"):\n",
    "        \"\"\"\n",
    "        An Operator for finite Differences / Gradients\n",
    "        Implements the forward as apply_G and the adjoint as apply_GH.\n",
    "        \n",
    "        Args:\n",
    "            dim (int, optional): Dimension. Defaults to 2.\n",
    "            mode (str, optional): one of doublecentral, central, forward or backward. Defaults to \"doublecentral\".\n",
    "            padmode (str, optional): one of constant, replicate, circular or refelct. Defaults to \"circular\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"kernel\", self.diff_kernel(dim, mode), persistent=False)\n",
    "        self._dim = dim\n",
    "        self._conv = (torch.nn.functional.conv1d, torch.nn.functional.conv2d, torch.nn.functional.conv3d)[dim - 1]\n",
    "        self._convT = (torch.nn.functional.conv_transpose1d, torch.nn.functional.conv_transpose2d, torch.nn.functional.conv_transpose3d)[dim - 1]\n",
    "        self._pad = partial(torch.nn.functional.pad, pad=2 * dim * (1,), mode=padmode)\n",
    "        if mode == 'central':\n",
    "            self._norm = (self.dim) ** (1 / 2)\n",
    "        else:\n",
    "            self._norm = (self.dim * 4) ** (1 / 2)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim\n",
    "    \n",
    "    def apply_G(self, x):\n",
    "        \"\"\"\n",
    "        Forward\n",
    "        \"\"\"\n",
    "        if x.is_complex():\n",
    "            xr = torch.view_as_real(x).moveaxis(-1, 0)\n",
    "        else:\n",
    "            xr = x\n",
    "        xr = xr.reshape(-1, 1, *x.shape[-self.dim :])\n",
    "        xp = self._pad(xr)\n",
    "        y = self._conv(xp, weight=self.kernel, bias=None, padding=0)\n",
    "        if x.is_complex():\n",
    "            y = y.reshape(2, *x.shape[: -self.dim], self.dim, *x.shape[-self.dim :])\n",
    "            y = torch.view_as_complex(y.moveaxis(0, -1).contiguous())\n",
    "        else:\n",
    "            y = y.reshape(*x.shape[0 : -self.dim], self.dim, *x.shape[-self.dim :])\n",
    "        return y\n",
    "\n",
    "    def apply_GH(self, x):\n",
    "        \"\"\"\n",
    "        Adjoint\n",
    "        \"\"\"\n",
    "        if x.is_complex():\n",
    "            xr = torch.view_as_real(x).moveaxis(-1, 0)\n",
    "        else:\n",
    "            xr = x\n",
    "        xr = xr.reshape(-1, self.dim, *x.shape[-self.dim :])\n",
    "        xp = self._pad(xr)\n",
    "        y = self._convT(xp, weight=self.kernel, bias=None, padding=2)\n",
    "        if x.is_complex():\n",
    "            y = y.reshape(2, *x.shape[: -self.dim - 1], *x.shape[-self.dim :])\n",
    "            y = torch.view_as_complex(y.moveaxis(0, -1).contiguous())\n",
    "        else:\n",
    "            y = y.reshape(*x.shape[: -self.dim - 1], *x.shape[-self.dim :])\n",
    "        return y\n",
    "    \n",
    "    def apply_GHG(self, x):\n",
    "        if x.is_complex():\n",
    "            xr = torch.view_as_real(x).moveaxis(-1, 0)\n",
    "        else:\n",
    "            xr = x\n",
    "        xr = xr.reshape(-1, 1, *x.shape[-self.dim :])\n",
    "        xp = self._pad(xr)\n",
    "        tmp = self._conv(xp, weight=self.kernel, bias=None, padding=0)\n",
    "        tmp = self._pad(tmp)\n",
    "        y = self._convT(tmp, weight=self.kernel, bias=None, padding=2)\n",
    "        if x.is_complex():\n",
    "            y = y.reshape(2, *x.shape)\n",
    "            y = torch.view_as_complex(y.moveaxis(0, -1).contiguous())\n",
    "        else:\n",
    "            y = y.reshape(*x.shape)\n",
    "        return y\n",
    "\n",
    "    def forward(self, x, direction=1):\n",
    "        if direction>0:\n",
    "            return self.apply_G(x)\n",
    "        elif direction<0:\n",
    "            return self.apply_GH(x)\n",
    "        else:\n",
    "            return self.apply_GHG(x)\n",
    "\n",
    "    @property\n",
    "    def normGHG(self):\n",
    "        return self._norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function for PDHG: Clip act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from ...\n",
    "\n",
    "class ClipAct(nn.Module):\n",
    "    def forward(self, x, threshold):\n",
    "        return clipact(x, threshold)\n",
    "\n",
    "\n",
    "def clipact(x, threshold):\n",
    "    is_complex = x.is_complex()\n",
    "    if is_complex:\n",
    "        x = torch.view_as_real(x)\n",
    "        threshold = threshold.unsqueeze(-1)\n",
    "    x = torch.clamp(x, -threshold, threshold)\n",
    "    if is_complex:\n",
    "        x = torch.view_as_complex(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only PDHG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, running PDHG with T large (many iterations in PDGH) will make GPU memory full?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from ...\n",
    "\n",
    "def reconstruct_with_PDHG(x, lambda_reg, T):\n",
    "    # NOTE: mb is the number of patches\n",
    "    dim = 3\n",
    "    if dim == 2:\n",
    "        mb, Nx, Ny = x.shape\n",
    "    elif dim == 3:\n",
    "        mb, _, Nx, Ny, Nt = x.shape\n",
    "\n",
    "    device = x.device\n",
    "\n",
    "    # starting values\n",
    "    xbar = x.clone()\n",
    "    x0 = x.clone()\n",
    "    xnoisy = x.clone()\n",
    "\n",
    "    # dual variable\n",
    "    p = x.clone()\n",
    "    q = torch.zeros(mb, dim, Nx, Ny, Nt, dtype=x.dtype).to(device)\n",
    "\n",
    "    # operator norms\n",
    "    op_norm_AHA = torch.sqrt(torch.tensor(1.0))\n",
    "    op_norm_GHG = torch.sqrt(torch.tensor(12.0))\n",
    "    # operator norm of K = [A, \\nabla]\n",
    "    # https://iopscience.iop.org/article/10.1088/0031-9155/57/10/3065/pdf,\n",
    "    # see page 3083\n",
    "    L = torch.sqrt(op_norm_AHA**2 + op_norm_GHG**2)\n",
    "\n",
    "    tau = nn.Parameter(\n",
    "        torch.tensor(10.0), requires_grad=True\n",
    "    )  # starting value approximately  1/L\n",
    "    sigma = nn.Parameter(\n",
    "        torch.tensor(10.0), requires_grad=True\n",
    "    )  # starting value approximately  1/L\n",
    "\n",
    "    # theta should be in \\in [0,1]\n",
    "    theta = nn.Parameter(\n",
    "        torch.tensor(10.0), requires_grad=True\n",
    "    )  # starting value approximately  1\n",
    "\n",
    "    # sigma, tau, theta\n",
    "    sigma = (1 / L) * torch.sigmoid(sigma)  # \\in (0,1/L)\n",
    "    tau = (1 / L) * torch.sigmoid(tau)  # \\in (0,1/L)\n",
    "    theta = torch.sigmoid(theta)  # \\in (0,1)\n",
    "\n",
    "    GradOps = GradOperators(\n",
    "        dim=dim, \n",
    "        mode=\"forward\", padmode=\"circular\")\n",
    "    clip_act = ClipAct()\n",
    "    # Algorithm 2 - Unrolled PDHG algorithm (page 18)\n",
    "    # TODO: In the paper, L is one of the inputs but not used anywhere in the pseudo code???\n",
    "    for kT in range(T):\n",
    "        # update p\n",
    "        p =  (p + sigma * (xbar - xnoisy) ) / (1. + sigma)\n",
    "        # update q\n",
    "        q = clip_act(q + sigma * GradOps.apply_G(xbar), lambda_reg)\n",
    "\n",
    "        x1 = x0 - tau * p - tau * GradOps.apply_GH(q)\n",
    "\n",
    "        if kT != T - 1:\n",
    "            # update xbar\n",
    "            xbar = x1 + theta * (x1 - x0)\n",
    "            x0 = x1\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reconstruct_with_PDHG():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for rgb_image in load_images([\"0001\"], False):\n",
    "    for rgb_image in load_images([\"0065\"], False):\n",
    "        grayscale_image = convert_to_grayscale(rgb_image)\n",
    "        grayscale_image = resize_image(grayscale_image, 0.125)\n",
    "        image_data = convert_to_numpy(grayscale_image)\n",
    "\n",
    "        print(f\"Image data shape: {image_data.shape}\")\n",
    "\n",
    "        image_data = convert_to_tensor(image_data)\n",
    "        print(f\"Image size: {image_data.size()}\")\n",
    "        print(f\"PSNR of original image: {PSNR(image_data, image_data)} dB\")\n",
    "        plt.imshow(image_data.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "        TEST_SIGMA = 0.5  # Relatively high noise\n",
    "        constant_noise_img = add_noise(image_data, sigma=TEST_SIGMA)\n",
    "        print(f\"PSNR of constant noise image: {PSNR(image_data, constant_noise_img):.2f} dB\")\n",
    "        plt.imshow(constant_noise_img.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "        # TEST_LAMBDA = 0.1 # A test value to see the effect of lambda on regularization\n",
    "        # TEST_LAMBDA = 0.2 # A test value to see the effect of lambda on regularization\n",
    "        TEST_LAMBDA = 0.04\n",
    "        pdhg_input = constant_noise_img.unsqueeze(0)\n",
    "        print(f\"PDHG input size: {pdhg_input.size()}\")\n",
    "        reconstructed_img = reconstruct_with_PDHG(\n",
    "            pdhg_input, \n",
    "            lambda_reg=TEST_LAMBDA, \n",
    "            T=128\n",
    "            # T=1000\n",
    "            )\n",
    "        resconstructed_image_data = reconstructed_img\n",
    "        print(f\"Reconstructed image data shape: {resconstructed_image_data.shape}\")\n",
    "        print(f\"PSNR of reconstructed image: {PSNR(image_data, resconstructed_image_data):.2f} dB\")\n",
    "        plt.imshow(resconstructed_image_data.squeeze(0).squeeze(0).to(\"cpu\").detach().numpy(), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "test_reconstruct_with_PDHG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Full Architecture\n",
    "\n",
    "UNET to PDHG\n",
    "\n",
    "The whole architecture can be seen as unsupervised: The data only contains (clean) images.\n",
    "\n",
    "The whole model: Input is an image. Output is also an image.\n",
    "\n",
    "The UNET actually only outputs the regularisation parameter map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from ...\n",
    "\n",
    "class DynamicImageStaticPrimalDualNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        T=128,\n",
    "        cnn_block=None,\n",
    "        mode=\"lambda_cnn\",\n",
    "        up_bound=0,\n",
    "        phase=\"training\",\n",
    "    ):\n",
    "        # print(f\"Running: {DynamicImageStaticPrimalDualNN.__name__}\")\n",
    "        super(DynamicImageStaticPrimalDualNN, self).__init__()\n",
    "\n",
    "        # gradient operators and clipping function\n",
    "        dim = 3\n",
    "        self.GradOps = GradOperators(dim, mode=\"forward\", padmode=\"circular\")\n",
    "\n",
    "        # operator norms\n",
    "        self.op_norm_AHA = torch.sqrt(torch.tensor(1.0))\n",
    "        self.op_norm_GHG = torch.sqrt(torch.tensor(12.0))\n",
    "        # operator norm of K = [A, \\nabla]\n",
    "        # https://iopscience.iop.org/article/10.1088/0031-9155/57/10/3065/pdf,\n",
    "        # see page 3083\n",
    "        self.L = torch.sqrt(self.op_norm_AHA**2 + self.op_norm_GHG**2)\n",
    "\n",
    "        # function for projecting\n",
    "        self.ClipAct = ClipAct()\n",
    "\n",
    "        if mode == \"lambda_xyt\":\n",
    "            # one single lambda for x,y and t\n",
    "            self.lambda_reg = nn.Parameter(torch.tensor([-1.5]), requires_grad=True)\n",
    "\n",
    "        elif mode == \"lambda_xy_t\":\n",
    "            # one (shared) lambda for x,y and one lambda for t\n",
    "            self.lambda_reg = nn.Parameter(\n",
    "                torch.tensor([-4.5, -1.5]), requires_grad=True\n",
    "            )\n",
    "\n",
    "        elif mode == \"lambda_cnn\":\n",
    "            # the CNN-block to estimate the lambda regularization map\n",
    "            # must be a CNN yielding a two-channeld output, i.e.\n",
    "            # one map for lambda_cnn_xy and one map for lambda_cnn_t\n",
    "            self.cnn = cnn_block    # NOTE: This is actually the UNET!!! (At least in this project)\n",
    "            self.up_bound = torch.tensor(up_bound)\n",
    "\n",
    "        # number of terations\n",
    "        self.T = T\n",
    "        self.mode = mode\n",
    "\n",
    "        # constants depending on the operators\n",
    "        self.tau = nn.Parameter(\n",
    "            torch.tensor(10.0), requires_grad=True\n",
    "        )  # starting value approximately  1/L\n",
    "        self.sigma = nn.Parameter(\n",
    "            torch.tensor(10.0), requires_grad=True\n",
    "        )  # starting value approximately  1/L\n",
    "\n",
    "        # theta should be in \\in [0,1]\n",
    "        self.theta = nn.Parameter(\n",
    "            torch.tensor(10.0), requires_grad=True\n",
    "        )  # starting value approximately  1\n",
    "\n",
    "        # distinguish between training and test phase;\n",
    "        # during training, the input is padded using \"reflect\" padding, because\n",
    "        # patches are used by reducing the number of temporal points;\n",
    "        # while testing, \"reflect\" padding is used in x,y- direction, while\n",
    "        # circular padding is used in t-direction\n",
    "        self.phase = phase\n",
    "\n",
    "    def get_lambda_cnn(self, x):\n",
    "        # padding\n",
    "        # arbitrarily chosen, maybe better to choose it depending on the\n",
    "        # receptive field of the CNN or so;\n",
    "        # seems to be important in order not to create \"holes\" in the\n",
    "        # lambda_maps in t-direction\n",
    "        npad_xy = 4\n",
    "        # npad_t = 8\n",
    "        npad_t = 0 # TODO: Time dimension should not be necessary for single image input.\n",
    "        # I changed the npad_t to 0 so that I can run on single image input without change the 3D type config. It seems that the number of frames must be greater than npad_t?\n",
    "\n",
    "        pad = (npad_t, npad_t, npad_xy, npad_xy, npad_xy, npad_xy)\n",
    "\n",
    "        if self.phase == \"training\":\n",
    "            x = F.pad(x, pad, mode=\"reflect\")\n",
    "\n",
    "        elif self.phase == \"testing\":\n",
    "            pad_refl = (0, 0, npad_xy, npad_xy, npad_xy, npad_xy)\n",
    "            pad_circ = (npad_t, npad_t, 0, 0, 0, 0)\n",
    "\n",
    "            x = F.pad(x, pad_refl, mode=\"reflect\")\n",
    "            x = F.pad(x, pad_circ, mode=\"circular\")\n",
    "\n",
    "        # estimate parameter map\n",
    "        lambda_cnn = self.cnn(x) # NOTE: The cnn is actually the UNET block!!! (At least in this project)\n",
    "\n",
    "        # crop\n",
    "        neg_pad = tuple([-pad[k] for k in range(len(pad))])\n",
    "        lambda_cnn = F.pad(lambda_cnn, neg_pad)\n",
    "\n",
    "        # double spatial map and stack\n",
    "        lambda_cnn = torch.cat((lambda_cnn[:, 0, ...].unsqueeze(1), lambda_cnn), dim=1)\n",
    "\n",
    "        # constrain map to be striclty positive; further, bound it from below\n",
    "        if self.up_bound > 0:\n",
    "            # constrain map to be striclty positive; further, bound it from below\n",
    "            lambda_cnn = self.up_bound * self.op_norm_AHA * torch.sigmoid(lambda_cnn)\n",
    "        else:\n",
    "            lambda_cnn = 0.1 * self.op_norm_AHA * F.softplus(lambda_cnn)\n",
    "\n",
    "        return lambda_cnn\n",
    "\n",
    "    def forward(self, x, lambda_map=None):\n",
    "        if lambda_map is None:\n",
    "            # estimate lambda reg from the image\n",
    "            lambda_reg = self.get_lambda_cnn(x)\n",
    "        else:\n",
    "            lambda_reg = lambda_map\n",
    "        x.to(DEVICE)\n",
    "        x1 = reconstruct_with_PDHG(x, lambda_reg, self.T)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Problem introduction\n",
    "\n",
    "Image denoising\n",
    "\n",
    "Assuming the image will have additive Gaussian noise\n",
    "\n",
    "A way to judge whether an image is \"noisy\" is by seeing whether there are a lot of \"changes\" between nearby pixels.\n",
    "\n",
    "In other words, to see whether the gradient is too high.\n",
    "\n",
    "That's why there is a regularisation term in the loss function. We want to penalise large changes in gradient.\n",
    "\n",
    "Mathematically, we treat an image as a vector $\\vec{x}$.\n",
    "\n",
    "Let $\\vec{z}$ denote the vector of the image we want to denoise, and $\\vec{x}$ the output image. We wish $\\vec{x}$ to be as close to the ideal $\\vec{x}_{\\text{true}}$ as possible. Since we don't know this, we use a combination of 2 measures as our targets. One is a data-discrepancy measure. Minimising this measure means we keep as much original information in $\\vec{z}$ as possible. However, keeping all information means we will not remove any noise. Intuitively, we see an image as being noisy when there are a lot of tiny \"changes\". \n",
    "\n",
    "(?) Noise means the important information in the image is still preserved, but the image is not visually pleasing. Denoising helps make the image more visually pleasant but still contains all the imporant information. \n",
    "\n",
    "The loss function is:\n",
    "\n",
    "$$ L(\\vec{x}, \\vec{z}) = \\min_{\\vec{x}} \\frac{1}{2} || \\vec{x} - \\vec{z} ||_{2}^{2} + \\lambda || \\nabla \\vec{x} ||_{1} $$\n",
    "\n",
    "The algorithm to reconstruct the image is Primal-Dual-Hybrid-G...\n",
    "\n",
    "This algorithm requires the regularisation parameters.\n",
    "\n",
    "Good regularisation parameter will help strike a good balance between smoothing out the noisy parts and keeping as much original information as possible.\n",
    "\n",
    "Lambda too high will make the reconstructed image to lose too much information. Too low will not remove a lot of noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE OF THE LOSS FUNCTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Divide image into patches\n",
    "\n",
    "Cut into 192 x 192 small patches. Each patch is one input (right?)\n",
    "\n",
    "??? Should I try to make the patch size divisible by the image dimension: 222 * 125\n",
    "\n",
    "5328 = 222 * 24\n",
    "3000 = 125 * 24\n",
    "\n",
    "2664 = 222 * 12\n",
    "3000 = 125 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from ...\n",
    "\n",
    "def extract_patches_3d(x, kernel_size, padding=0, stride=1, dilation=1):\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size, kernel_size)\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding, padding)\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride, stride)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation, dilation)\n",
    "\n",
    "    def get_dim_blocks(dim_in, dim_kernel_size, dim_padding = 0, dim_stride = 1, dim_dilation = 1):\n",
    "        dim_out = (dim_in + 2 * dim_padding - dim_dilation * (dim_kernel_size - 1) - 1) // dim_stride + 1\n",
    "        return dim_out\n",
    "\n",
    "    channels = x.shape[-4]\n",
    "    d_dim_in = x.shape[-3]\n",
    "    h_dim_in = x.shape[-2]\n",
    "    w_dim_in = x.shape[-1]\n",
    "    d_dim_out = get_dim_blocks(d_dim_in, kernel_size[0], padding[0], stride[0], dilation[0])\n",
    "    h_dim_out = get_dim_blocks(h_dim_in, kernel_size[1], padding[1], stride[1], dilation[1])\n",
    "    w_dim_out = get_dim_blocks(w_dim_in, kernel_size[2], padding[2], stride[2], dilation[2])\n",
    "\n",
    "    # (B, C, D, H, W)\n",
    "    x = x.view(-1, \n",
    "               channels, \n",
    "               d_dim_in, \n",
    "               h_dim_in * w_dim_in)                                                     \n",
    "    # (B, C, D, H * W)\n",
    "\n",
    "    x = torch.nn.functional.unfold(x, \n",
    "                                   kernel_size=(kernel_size[0], 1), \n",
    "                                   padding=(padding[0], 0), \n",
    "                                   stride=(stride[0], 1), \n",
    "                                   dilation=(dilation[0], 1))                   \n",
    "    # (B, C * kernel_size[0], d_dim_out * H * W)\n",
    "\n",
    "    x = x.view(-1, \n",
    "               channels * kernel_size[0] * d_dim_out, \n",
    "               h_dim_in, \n",
    "               w_dim_in)                                   \n",
    "    # (B, C * kernel_size[0] * d_dim_out, H, W)\n",
    "\n",
    "    x = torch.nn.functional.unfold(x, \n",
    "                                   kernel_size=(kernel_size[1], kernel_size[2]), \n",
    "                                   padding=(padding[1], padding[2]), \n",
    "                                   stride=(stride[1], stride[2]), \n",
    "                                   dilation=(dilation[1], dilation[2]))        \n",
    "    # (B, C * kernel_size[0] * d_dim_out * kernel_size[1] * kernel_size[2], h_dim_out, w_dim_out)\n",
    "\n",
    "    x = x.view(-1, channels, kernel_size[0], d_dim_out, kernel_size[1], kernel_size[2], h_dim_out, w_dim_out)  \n",
    "    # (B, C, kernel_size[0], d_dim_out, kernel_size[1], kernel_size[2], h_dim_out, w_dim_out)  \n",
    "\n",
    "    x = x.permute(0, 1, 3, 6, 7, 2, 4, 5)\n",
    "    # (B, C, d_dim_out, h_dim_out, w_dim_out, kernel_size[0], kernel_size[1], kernel_size[2])\n",
    "\n",
    "    x = x.contiguous().view(-1, channels, kernel_size[0], kernel_size[1], kernel_size[2])\n",
    "    # (B * d_dim_out * h_dim_out * w_dim_out, C, kernel_size[0], kernel_size[1], kernel_size[2])\n",
    "\n",
    "    x_patches = x\n",
    "    return x_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_extract_patches_3d():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for rgb_image in load_images([\"0001\"], False):\n",
    "    for rgb_image in load_images([\"0065\"], False):\n",
    "        grayscale_image = convert_to_grayscale(rgb_image)\n",
    "        grayscale_image = resize_image(grayscale_image, 0.25)\n",
    "        image_data = convert_to_numpy(grayscale_image)\n",
    "        image_data = convert_to_tensor(image_data)\n",
    "        print(f\"Image data size: {image_data.size()}\")\n",
    "        plt.imshow(image_data.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "        # Extract patches\n",
    "        x_patches = extract_patches_3d(\n",
    "            image_data, \n",
    "            kernel_size=[192, 192, 1], \n",
    "            padding=0, \n",
    "            stride=[192, 192, 1], \n",
    "            dilation=1)\n",
    "        print(f\"x_patches size: {x_patches.size()}\")\n",
    "        assert x_patches.size() == torch.Size([15, 1, 192, 192, 1])\n",
    "        fig, ax = plt.subplots(3, 5, figsize=(9,6), layout=\"compressed\")\n",
    "        for i in range(15):\n",
    "            patch = x_patches[i]\n",
    "            plt.subplot(3, 5, i+1).imshow(patch.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "            plt.subplot(3, 5, i+1).set_xticks([])\n",
    "            plt.subplot(3, 5, i+1).set_yticks([])\n",
    "        # Remove x and y ticks\n",
    "        plt.axis('off')\n",
    "        plt.show();\n",
    "\n",
    "test_extract_patches_3d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Stitch the patches together\n",
    "\n",
    "Prep for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from ...\n",
    "\n",
    "def stitch_patches_back_to_image(x_patches, img_h, img_w, k_h, k_w):\n",
    "    num_rows = img_h // k_h\n",
    "    num_cols = img_w // k_w\n",
    "\n",
    "    rows = []\n",
    "    for i_row in range(num_rows):\n",
    "        row = []\n",
    "        for i_col in range(num_cols):\n",
    "            i = i_row*num_cols + i_col\n",
    "            patch = x_patches[i]\n",
    "            row.append(patch)\n",
    "        rows.append(torch.cat(row, dim=2))\n",
    "\n",
    "    x_stitched = torch.cat(rows, dim=1) # Concatenate the rows\n",
    "    return x_stitched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stitch_patches_back_to_image():\n",
    "    if DISABLING_TESTS: return\n",
    "    for rgb_image in load_images([\"0065\"], False):\n",
    "        grayscale_image = convert_to_grayscale(rgb_image)\n",
    "        grayscale_image = resize_image(grayscale_image, 0.25)\n",
    "        image_data = convert_to_numpy(grayscale_image)\n",
    "        image_data = convert_to_tensor(image_data)\n",
    "        print(f\"Image data size: {image_data.size()}\")\n",
    "\n",
    "        # Extract patches\n",
    "        x_patches = extract_patches_3d(\n",
    "            image_data, \n",
    "            kernel_size=[192, 192, 1], \n",
    "            padding=0, \n",
    "            stride=[192, 192, 1], \n",
    "            dilation=1)\n",
    "        print(f\"x_patches size: {x_patches.size()}\")\n",
    "\n",
    "        # Stitch patches back to image\n",
    "        img_h = image_data.size(1)\n",
    "        img_w = image_data.size(2)\n",
    "        k_h = 192\n",
    "        k_w = 192\n",
    "        print(f\"img_h: {img_h}, img_w: {img_w}, k_h: {k_h}, k_w: {k_w}\")\n",
    "        x_combined = stitch_patches_back_to_image(\n",
    "            x_patches,\n",
    "            img_h=img_h,\n",
    "            img_w=img_w,\n",
    "            k_h=k_h,\n",
    "            k_w=k_w)\n",
    "        print(f\"x_combined size: {x_combined.size()}\")\n",
    "        plt.imshow(x_combined.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "test_stitch_patches_back_to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Data loading class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from ...\n",
    "\n",
    "class DynamicImageStaticDenoisingDataset(Dataset):\n",
    "\t\n",
    "\tdef __init__(\n",
    "\t\tself, \n",
    "\t\tdata_path: str, \n",
    "\t\tids: list,\n",
    "\t\tscale_factor = 0.5, \n",
    "\t\tsigma=0.23,  \n",
    "\t\tpatches_size = None,\n",
    "\t\tstrides= None,\n",
    "\t\textract_data=True,\n",
    "\t\tdevice: str = \"cuda\"\n",
    "\t):\n",
    "\t\tself.device = device\n",
    "\t\tself.scale_factor = scale_factor\n",
    "\n",
    "\t\tids = [str(x).zfill(2) for x in ids]\n",
    "\n",
    "\t\txf_list = []\n",
    "  \n",
    "\t\tk = 0\n",
    "\t\t# for k, img_id in enumerate(ids):\n",
    "\t\tfor folder in os.listdir(data_path):\n",
    "\t\t\timg_id = folder[:4]\t# The first 4 characters of folder name is the image id (0001, 0002, ..., 0200)\n",
    "\t\t\tif img_id not in ids:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tk += 1\n",
    "\t\t\tprint(f'loading image id {img_id}, {k}/{len(ids)}')\n",
    "\t\t\t# sample_path = os.path.join(data_path, f\"MOT17-{img_id}\")\n",
    "\t\t\tsample_path = os.path.join(data_path, folder)\n",
    "\t\t\tif extract_data:\n",
    "\t\t\t\txf = self.create_dyn_img(sample_path)\n",
    "\t\t\telse:\n",
    "\t\t\t\tscale_factor_str = str(self.scale_factor).replace('.','_')\n",
    "\t\t\t\txf = np.load(os.path.join(sample_path, f\"xf_scale_factor{scale_factor_str}.npy\"))\n",
    "\t\t\t\t\n",
    "\t\t\txf = torch.tensor(xf, dtype=torch.float)\n",
    "\t\t\txf = xf.unsqueeze(0) / 255\n",
    "\t\t\t\n",
    "\t\t\tif patches_size is not None:\n",
    "\t\t\t\t\n",
    "\t\t\t\tprint(f\"extracting patches of shape {patches_size}; strides {strides}\")\n",
    "\t\t\t\txf_patches = extract_patches_3d(xf.contiguous(), patches_size, stride=strides)\n",
    "\t\t\t\txf_list.append(xf_patches)\n",
    "\t\t\t\n",
    "\t\tif patches_size is not None:\n",
    "\t\t\t# will have shape (mb, 1, Nx, Ny, Nt), where mb denotes the number of patches\n",
    "\t\t\txf = torch.concat(xf_list,dim=0)\n",
    "\t\t\t\n",
    "\t\telse:\n",
    "\t\t\txf = xf.unsqueeze(0)\n",
    "\t\t\n",
    "\t\t#create temporal TV vector to detect which patches contain the most motion\n",
    "\t\txf_patches_tv = (xf[...,1:] - xf[...,:-1]).pow(2).sum(dim=[1,2,3,4]) #contains the TV for all patches\n",
    "\t\t\n",
    "\t\t#normalize to 1 to have a probability vector\n",
    "\t\txf_patches_tv /= torch.sum(xf_patches_tv)\n",
    "\t\t\n",
    "\t\t#sort TV in descending order --> xfp_tv_ids[0] is the index of the patch with the most motion\n",
    "\t\tself.samples_weights = xf_patches_tv\n",
    "\n",
    "\t\t# TODO: Investigate\n",
    "\t\t# Change the values in samples_weights to be a range of integers from 0 to len(samples_weights)\n",
    "\t\t# Unless I do this, when I run on a set of identical images, it will give me an error:\n",
    "\t\t# RuntimeError: invalid multinomial distribution (encountering probability entry < 0)\n",
    "\t\tself.samples_weights = torch.arange(len(self.samples_weights))\n",
    "\t\t\n",
    "\t\tself.xf = xf\n",
    "\t\tself.len = xf.shape[0]\n",
    "\t\t\n",
    "\t\tif isinstance(sigma, float):\n",
    "\t\t\tself.noise_level = 'constant'\n",
    "\t\t\tself.sigma = sigma\n",
    "\n",
    "\t\telif isinstance(sigma, (tuple, list)):\n",
    "\t\t\tself.noise_level = 'variable'\n",
    "\t\t\tself.sigma_min = sigma[0]\n",
    "\t\t\tself.sigma_max = sigma[1]\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(\"Invalid sigma value provided, must be float, tuple or list.\")\n",
    "\n",
    "\tdef create_dyn_img(self, sample_path: str):\n",
    "\t\t\n",
    "\t\tfiles_path = sample_path\n",
    "\t\tfiles_list = os.listdir(files_path)\n",
    "\t\txf = []\n",
    "\n",
    "\t\tfor file in files_list:\n",
    "\t\t\tif not file.startswith('GT'):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\timage = Image.open(os.path.join(files_path, file))\n",
    "\t\t\t\n",
    "\t\t\t#resize\n",
    "\t\t\tNx_,Ny_ = np.int(np.floor(self.scale_factor * image.width )), np.int(np.floor(self.scale_factor * image.height ))\n",
    "\t\t\timage = image.resize( (Nx_, Ny_) )\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\timage = image.convert('L') #convert to grey_scale\n",
    "\t\t\timage_data = np.asarray(image)\n",
    "\t\t\txf.append(image_data)\n",
    "\t\t\t\n",
    "\t\txf = np.stack(xf, axis=-1)\n",
    "\t\t\n",
    "\t\tscale_factor_str = str(self.scale_factor).replace('.','_')\n",
    "\t\tnp.save(os.path.join(sample_path, f\"xf_scale_factor{scale_factor_str}.npy\"), xf)\n",
    "\t\t\n",
    "\t\treturn xf\n",
    "\t\t\t\n",
    "\tdef __getitem__(self, index):\n",
    "\n",
    "\t\tstd = torch.std(self.xf[index])\n",
    "\t\tmu = torch.mean(self.xf[index])\n",
    "\n",
    "\t\tx_centred = (self.xf[index]  - mu) / std\n",
    "\n",
    "\t\tif self.noise_level == 'constant':\n",
    "\t\t\tsigma = self.sigma\n",
    "\t\t\t\n",
    "\t\telif self.noise_level == 'variable':\n",
    "\t\t\tsigma = self.sigma_min + torch.rand(1) * ( self.sigma_max - self.sigma_min )\n",
    "\n",
    "\t\tx_centred += sigma * torch.randn(self.xf[index].shape, dtype = self.xf[index].dtype)\n",
    "\n",
    "\t\txnoise = std * x_centred + mu\n",
    "  \n",
    "\t\treturn (\n",
    "\t\t\txnoise.to(device=self.device),\n",
    "   \t\t\tself.xf[index].to(device=self.device)\n",
    "        )\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from ...\n",
    "\n",
    "# Ids are in the form of 4 character strings \"0001\" to \"0200\"\n",
    "TRAINING = [str(x).zfill(4) for x in range(54, 74)]\n",
    "print(f\"TRAINING: {TRAINING}\")\n",
    "\n",
    "VALIDATION = [\"0065\"] # TODO: Decide what to use for validation\n",
    "\n",
    "data_path = \"../../data/dyn_img_static/tmp/SIDD_Small_sRGB_Only/Data\"\n",
    "# Make sure that the dataset was downloaded successfully\n",
    "# The data samples can be created with different scaling factors.\n",
    "# Make sure to set extract_data to True when loading the dataset for the first time to create the dynamic images.\n",
    "# Once the data for a specific scaling factor has been created the flag can be set to False.\n",
    "dataset_train = DynamicImageStaticDenoisingDataset(\n",
    "    data_path=data_path,\n",
    "    # ids=TRAINING,     \n",
    "    ids=[\"0065\"],       # testing\n",
    "    scale_factor=0.5,\n",
    "    # sigma=[0.1, 0.3],\n",
    "    # sigma=[0.3, 0.3],\n",
    "    sigma=[1, 1],\n",
    "    # strides=[192, 192, 1],\n",
    "    # patches_size=[192, 192, 1],\n",
    "    strides=[512, 512, 1],\n",
    "    patches_size=[512, 512, 1],\n",
    "    # (!) Make sure to set the following flag to True when loading the dataset for the first time.\n",
    "    extract_data=True,\n",
    "    # extract_data=False,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Create training dataloader\n",
    "sampler = WeightedRandomSampler(dataset_train.samples_weights, len(dataset_train.samples_weights))\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=1, sampler=sampler)\n",
    "\n",
    "# Validation dataset (see note above)\n",
    "dataset_valid = DynamicImageStaticDenoisingDataset(\n",
    "    data_path=data_path,\n",
    "    # ids=VALIDATION,   \n",
    "    ids=[\"0065\"],       # testing\n",
    "    scale_factor=0.5,\n",
    "    # sigma=[0.1, 0.3],\n",
    "    # sigma=[0.3, 0.3],\n",
    "    sigma=[1, 1],\n",
    "    # strides=[192, 192, 1],\n",
    "    # patches_size=[192, 192, 1],\n",
    "    strides=[512, 512, 1],\n",
    "    patches_size=[512, 512, 1],\n",
    "    # (!) Make sure to set the following flag to True when loading the dataset for the first time.\n",
    "    extract_data=True,\n",
    "    # extract_data=False,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Create validation dataloader \n",
    "sampler = WeightedRandomSampler(dataset_valid.samples_weights, len(dataset_valid.samples_weights))\n",
    "dataloader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=1, sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### UNET\n",
    "\n",
    "The specific UNET architecture we use has the following parts:\n",
    "\n",
    "...\n",
    "\n",
    "We use Leaky RELU instead of RELU or Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code was taken from ...\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    A block of convolutional layers (1D, 2D or 3D)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in,\n",
    "        n_ch_out,\n",
    "        n_convs,\n",
    "        kernel_size=3,\n",
    "        bias=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if dim == 1:\n",
    "            conv_op = nn.Conv1d\n",
    "        if dim == 2:\n",
    "            conv_op = nn.Conv2d\n",
    "        elif dim == 3:\n",
    "            conv_op = nn.Conv3d\n",
    "\n",
    "        padding = int(np.floor(kernel_size / 2))\n",
    "\n",
    "        conv_block_list = []\n",
    "        conv_block_list.extend(\n",
    "            [\n",
    "                conv_op(\n",
    "                    n_ch_in,\n",
    "                    n_ch_out,\n",
    "                    kernel_size,\n",
    "                    padding=padding,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                ),\n",
    "                nn.LeakyReLU(), # NOTE: We use LeakyReLU instead of ReLU!!!\n",
    "                # # Can we try using ReLU instead of LeakyReLU?\n",
    "                # nn.ReLU(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i in range(n_convs - 1):\n",
    "            conv_block_list.extend(\n",
    "                [\n",
    "                    conv_op(\n",
    "                        n_ch_out,\n",
    "                        n_ch_out,\n",
    "                        kernel_size,\n",
    "                        padding=padding,\n",
    "                        bias=bias,\n",
    "                        padding_mode=padding_mode,\n",
    "                    ),\n",
    "                    nn.LeakyReLU(), # NOTE: We use LeakyReLU instead of ReLU!!!\n",
    "                    # # Can we try using ReLU instead of LeakyReLU?\n",
    "                    # nn.ReLU(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in,\n",
    "        n_enc_stages,\n",
    "        n_convs_per_stage,\n",
    "        n_filters,\n",
    "        kernel_size=3,\n",
    "        bias=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        n_ch_list = [n_ch_in]\n",
    "        for ne in range(n_enc_stages):\n",
    "            n_ch_list.append(int(n_filters) * 2**ne)\n",
    "\n",
    "        self.enc_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(\n",
    "                    dim,\n",
    "                    n_ch_list[i],\n",
    "                    n_ch_list[i + 1],\n",
    "                    n_convs_per_stage,\n",
    "                    kernel_size=kernel_size,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                )\n",
    "                for i in range(len(n_ch_list) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if dim == 1:\n",
    "            pool_op = nn.MaxPool1d(2)\n",
    "        elif dim == 2:\n",
    "            pool_op = nn.MaxPool2d(2)\n",
    "        elif dim == 3:\n",
    "            # TODO: Can I make it so that if there is only a single image then it reduces the pool size to 1?\n",
    "            # pool_op = nn.MaxPool3d(2)\n",
    "            pool_op = nn.MaxPool3d(1) # TODO: Change back after making the code working with 2D MaxPool\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dim value: {dim}\")\n",
    "\n",
    "        self.pool = pool_op\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "            x = self.pool(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in,\n",
    "        n_dec_stages,\n",
    "        n_convs_per_stage,\n",
    "        # n_filters,\n",
    "        kernel_size=3,\n",
    "        bias=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        n_ch_list = []\n",
    "        for ne in range(n_dec_stages):\n",
    "            n_ch_list.append(int(n_ch_in * (1 / 2) ** ne))\n",
    "\n",
    "        if dim == 1:\n",
    "            conv_op = nn.Conv1d\n",
    "            interp_mode = \"linear\"\n",
    "        elif dim == 2:\n",
    "            conv_op = nn.Conv2d\n",
    "            interp_mode = \"bilinear\"\n",
    "        elif dim == 3:\n",
    "            interp_mode = \"trilinear\"\n",
    "            conv_op = nn.Conv3d\n",
    "\n",
    "        self.interp_mode = interp_mode\n",
    "\n",
    "        padding = int(np.floor(kernel_size / 2))\n",
    "        self.upconvs = nn.ModuleList(\n",
    "            [\n",
    "                conv_op(\n",
    "                    n_ch_list[i],\n",
    "                    n_ch_list[i + 1],\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                )\n",
    "                for i in range(len(n_ch_list) - 1)\n",
    "            ]\n",
    "        )\n",
    "        self.dec_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(\n",
    "                    dim,\n",
    "                    n_ch_list[i],\n",
    "                    n_ch_list[i + 1],\n",
    "                    n_convs_per_stage,\n",
    "                    kernel_size=kernel_size,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                )\n",
    "                for i in range(len(n_ch_list) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.dec_blocks)):\n",
    "            enc_features = encoder_features[i]\n",
    "            enc_features_shape = enc_features.shape\n",
    "            x = nn.functional.interpolate(\n",
    "                x, enc_features_shape[2:], mode=self.interp_mode, align_corners=False\n",
    "            )\n",
    "            x = self.upconvs[i](x)\n",
    "            x = torch.cat([x, enc_features], dim=1)\n",
    "            x = self.dec_blocks[i](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in=2,\n",
    "        n_ch_out=2,\n",
    "        n_enc_stages=3,\n",
    "        # n_enc_stages=4,\n",
    "        n_convs_per_stage=2,\n",
    "        # n_convs_per_stage=3,\n",
    "        n_filters=16,\n",
    "        kernel_size=3,\n",
    "        res_connection=False,\n",
    "        bias=True,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            dim,\n",
    "            n_ch_in,\n",
    "            n_enc_stages,\n",
    "            n_convs_per_stage,\n",
    "            n_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            dim,\n",
    "            n_filters * (2 ** (n_enc_stages - 1)),\n",
    "            n_enc_stages,\n",
    "            n_convs_per_stage,\n",
    "            n_filters * (n_enc_stages * 2),\n",
    "            kernel_size=kernel_size,\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "\n",
    "        if dim == 1:\n",
    "            conv_op = nn.Conv1d\n",
    "        elif dim == 2:\n",
    "            conv_op = nn.Conv2d\n",
    "        elif dim == 3:\n",
    "            conv_op = nn.Conv3d\n",
    "\n",
    "        self.c1x1 = conv_op(n_filters, n_ch_out, kernel_size=1, padding=0, bias=bias)\n",
    "        if res_connection:\n",
    "            if n_ch_in == n_ch_out:\n",
    "                self.res_connection = lambda x: x\n",
    "            else:\n",
    "                self.res_connection = conv_op(n_ch_in, n_ch_out, 1)\n",
    "        else:\n",
    "            self.res_connection = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_features = self.encoder(x)\n",
    "        dec = self.decoder(enc_features[-1], enc_features[::-1][1:])\n",
    "        out = self.c1x1(dec)\n",
    "        if self.res_connection:\n",
    "            out = out + self.res_connection(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from ...\n",
    "\n",
    "def train_epoch(model, data, optimizer, loss_func) -> float:\n",
    "    \"\"\"Perform the training of one epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Model to be trained\n",
    "    data\n",
    "        Dataloader with training data\n",
    "    optimizer\n",
    "        Pytorch optimizer, e.g. Adam\n",
    "    loss_func\n",
    "        Loss function to be calculated, e.g. MSE\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        training loss\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        loss is NaN\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "\n",
    "    # for sample in data: # If cannot install tqdm\n",
    "    for sample in tqdm(data): # tqdm helps show a nice progress bar\n",
    "        optimizer.zero_grad(set_to_none=True)  # Zero your gradients for every batch!\n",
    "        \n",
    "        sample, label = sample\n",
    "        # print(f\"sample: {sample.shape}\")\n",
    "        output = model(sample)\n",
    "        loss = loss_func(label, output)\n",
    "        loss.backward()\n",
    "        \n",
    "        if loss.item() != loss.item():\n",
    "            raise ValueError(\"NaN returned by loss function...\")\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(data.dataset)\n",
    "\n",
    "\n",
    "def validate_epoch(model, data, loss_func) -> float:\n",
    "    \"\"\"Perform the validation of one epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "        Model to be trained\n",
    "    data\n",
    "        Dataloader with validation data\n",
    "    loss_func\n",
    "        Loss function to be calculated, e.g. MSE\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        validation loss\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    \n",
    "    # for sample in data: # If cannot install tqdm\n",
    "    for sample in tqdm(data): # tqdm helps show a nice progress bar\n",
    "        inputs, labels = sample\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(data.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Use wandb to log the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Use wandb to log the training process\n",
    "!wandb login\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'image-denoising-1'\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"image-denoising-1\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"MSELoss\",\n",
    "    \"architecture\": \"UNET-PDHG\",\n",
    "    \"up_bound\": 0.5,\n",
    "    \"dataset\": \"SIDD_Small-img_0065_only\",\n",
    "    \"scale_factor\": 0.5,\n",
    "    \"kernel_size\": 512,\n",
    "    \"sigma\": 1,\n",
    "    \"T\": 128,\n",
    "    \"activation\": \"LeakyReLU\",\n",
    "    \"epochs\": 10_000,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start training\n",
    "\n",
    "We use Adam optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from ...\n",
    "\n",
    "# Define CNN block and PDHG-method\n",
    "unet = UNet(dim=3, n_ch_in=1).to(DEVICE)\n",
    "\n",
    "# Construct primal-dual operator with nn\n",
    "pdhg = DynamicImageStaticPrimalDualNN(\n",
    "    cnn_block=unet, \n",
    "    T=128, # Can use a bit bigger patch\n",
    "    # T=1000, # Can only use small patch. More than 1000 and the GPU memory runs out? Why?\n",
    "    phase=\"training\",\n",
    "    up_bound=0.5,\n",
    "    # Select mode:\n",
    "    mode=\"lambda_cnn\",\n",
    "    # mode=\"lambda_xy_t\",\n",
    "    # mode=\"lambda_xyt\",\n",
    ").to(DEVICE)\n",
    "# pdhg.load_state_dict(torch.load(\"./tmp/states/2024_05_24_23_04_31.pt\"))\n",
    "\n",
    "optimizer = torch.optim.Adam(pdhg.parameters(), lr=1e-4)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "# num_epochs = 2          # testing\n",
    "num_epochs = 10_000      # paper\n",
    "\n",
    "\n",
    "# model_name = \"img_0065-scale_0_5-kernel_512-sigma_0_3-T_128-ReLU\"\n",
    "model_name = \"img_0065-scale_0_5-kernel_512-sigma_1-T_128-LeakyReLU\"\n",
    "# model_name = \"scene_03-img_0054_to_0074-scale_0_5-kernel_512-sigma_0_3-T_128\"\n",
    "\n",
    "\n",
    "# NEW MDOEL\n",
    "time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "# model_states_dir = f\"./tmp/model_{model_name}_{time}\"\n",
    "\n",
    "# If want to continue training from a current version, add the information here\n",
    "model_states_dir = \"./tmp/model_img_0065-scale_0_5-kernel_512-sigma_1-T_128-LeakyReLU_2024_05_26_15_55_13\"\n",
    "pdhg = torch.load(f\"{model_states_dir}/model_epoch_522.pt\")\n",
    "pdhg.train(True)\n",
    "\n",
    "start_epoch = 522\n",
    "os.makedirs(model_states_dir, exist_ok=True)\n",
    "\n",
    "# for epoch in tqdm(range(68, num_epochs)):\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "    # Model training\n",
    "    pdhg.train(True)\n",
    "    training_loss = train_epoch(pdhg, dataloader_train, optimizer, loss_function)\n",
    "    pdhg.train(False)\n",
    "    print(\"TRAINING LOSS: \", training_loss)\n",
    "\n",
    "    wandb.log({\"training_loss\": training_loss})\n",
    "\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        torch.save(pdhg, f\"{model_states_dir}/{current_model_name}.pt\")\n",
    "        print(f\"\\tEpoch: {epoch+1}\")\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Model validation\n",
    "            validation_loss = validate_epoch(pdhg, dataloader_valid, loss_function)\n",
    "            print(\"VALIDATION LOSS: \", validation_loss)\n",
    "            time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "            current_model_name = f\"model_epoch_{epoch+1}\"\n",
    "\n",
    "            wandb.log({\"validation_loss\": validation_loss})\n",
    "\n",
    "            wandb.log_model(f\"{model_states_dir}/{current_model_name}.pt\", name=f\"model_epoch_{epoch+1}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "wandb.finish()\n",
    "# Save the entire model\n",
    "torch.save(pdhg, f\"{model_states_dir}/final_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Inference\n",
    "\n",
    "Demo the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print image and patches function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_and_patches(image, patches, image_name, clean_image, n_rows, n_cols, folder, log_file, showing=False):\n",
    "    showing = False # TODO: Remove this\n",
    "    print(f\"Shape of {image_name} image: {image.shape}\", file=log_file)\n",
    "    print(f\"PSNR of {image_name} image: {PSNR(clean_image, image)} dB\", file=log_file)\n",
    "    print(f\"PSNR of {image_name} image: {PSNR(clean_image, image)} dB\")\n",
    "    print(f\"Shape of {image_name} patches: {patches.shape}\", file=log_file)\n",
    "    print(f\"Max value in {image_name} image: {torch.max(image)}\", file=log_file)\n",
    "    print(f\"Min value in {image_name} image: {torch.min(image)}\", file=log_file)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(9,6))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "    plt.savefig(f\"{folder}/image_{image_name}.png\", bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    fig, ax = plt.subplots(n_rows, n_cols, figsize=(9,6), layout=\"compressed\")\n",
    "    for i_row in range(n_rows):\n",
    "        row = []\n",
    "        for i_col in range(n_cols):\n",
    "            i = i_row*n_cols + i_col\n",
    "            patch = patches[i]\n",
    "            plt.subplot(n_rows, n_cols, i+1).imshow(\n",
    "                patch.squeeze(0).to(\"cpu\"), \n",
    "            cmap='gray')\n",
    "            # Remove x and y ticks\n",
    "            plt.subplot(n_rows, n_cols, i+1).set_xticks([])\n",
    "            plt.subplot(n_rows, n_cols, i+1).set_yticks([])\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f\"{folder}/patches_{image_name}.png\", bbox_inches='tight', pad_inches=0)\n",
    "    if showing:\n",
    "        plt.show();\n",
    "    else:\n",
    "        plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patches_side_by_side(patches_clean, patches_noisy, patches_denoised_single_lambda, patches_denoised_lambda_map, folder, log_file, showing=False):\n",
    "    print(f\"Shape of patches_clean: {patches_clean.shape}\", file=log_file)\n",
    "    print(f\"Shape of patches_noisy: {patches_noisy.shape}\", file=log_file)\n",
    "    print(f\"Shape of patches_denoised_single_lambda: {patches_denoised_single_lambda.shape}\", file=log_file)\n",
    "    print(f\"Shape of patches_denoised_lambda_map: {patches_denoised_lambda_map.shape}\", file=log_file)\n",
    "    for i in range(len(patches_clean)):\n",
    "        fig, ax = plt.subplots(1, 4, figsize=(9,6))\n",
    "\n",
    "        # Clean image\n",
    "        patch_clean = patches_clean[i]\n",
    "        plt.subplot(1, 4, 1).imshow(patch_clean.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.subplot(1, 4, 1).set_xticks([])\n",
    "        plt.subplot(1, 4, 1).set_yticks([])\n",
    "\n",
    "        # Noisy image\n",
    "        patch_noisy = patches_noisy[i]\n",
    "        plt.subplot(1, 4, 2).imshow(patch_noisy.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.subplot(1, 4, 2).set_xticks([])\n",
    "        plt.subplot(1, 4, 2).set_yticks([])\n",
    "\n",
    "        # Denoised image using single lambda (traditional method)\n",
    "        patch_denoised_single_lambda = patches_denoised_single_lambda[i]\n",
    "        plt.subplot(1, 4, 3).imshow(patch_denoised_single_lambda.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.subplot(1, 4, 3).set_xticks([])\n",
    "        plt.subplot(1, 4, 3).set_yticks([])\n",
    "\n",
    "        # Denoised image using lambda map (our method)\n",
    "        patch_denoised_lambda_map = patches_denoised_lambda_map[i]\n",
    "        plt.subplot(1, 4, 4).imshow(patch_denoised_lambda_map.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.subplot(1, 4, 4).set_xticks([])\n",
    "        plt.subplot(1, 4, 4).set_yticks([])\n",
    "\n",
    "        plt.savefig(f\"{folder}/patch_{i}.png\", bbox_inches='tight', pad_inches=0)\n",
    "        if showing:\n",
    "            plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO INFER AND SHOW SOME RESULTS HERE\n",
    "\n",
    "\n",
    "# Use pdhg for prediction\n",
    "def test_denoise(model_name, model_folder, trained_on):\n",
    "    \"\"\"\n",
    "    Testing denoising with pre-trained parameters. We can \n",
    "    try any image we want. If the image is larger than 192x192, \n",
    "    we have to crop it to 192x192 patches and then stitch the\n",
    "    patches back together.\n",
    "\n",
    "    TODO: If the size is not divisible by 192, we have to pad\n",
    "    the image with zeros to make it divisible by 192. Then we\n",
    "    remove the padding before/after stitching the patches back\n",
    "    together.\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): # IMPORTANT! Need no_grad to free GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Get a clean image\n",
    "        img_id = \"0065\"  # scene 003\n",
    "        # img_id = \"0102\"\n",
    "        # img_id = \"0044\"  # scene 002\n",
    "        # img_id = \"0055\"\n",
    "        # img_id = \"0030\"\n",
    "        # scale = 0.25\n",
    "        scale = 0.5\n",
    "        clean_image = load_images([img_id], False)[0]\n",
    "        clean_image = convert_to_grayscale(clean_image)\n",
    "        clean_image = resize_image(clean_image, scale)\n",
    "        clean_image = convert_to_tensor(clean_image)\n",
    "\n",
    "        # k_w, k_h = 192, 192\n",
    "        k_w, k_h = 512, 512\n",
    "        kernel_size = [k_w, k_h, 1]\n",
    "        stride = [k_w, k_h, 1]\n",
    "        img_h = clean_image.size(1)\n",
    "        img_w = clean_image.size(2)\n",
    "        n_rows = img_h // k_h\n",
    "        n_cols = img_w // k_w\n",
    "        # best_lambda = 0.03 # Found by brute force on image 0065 kernel 192x192, sigma 0.3\n",
    "        # best_lambda = 0.05 # Found by brute force on image 0065 kernel 512x512, sigma 0.3\n",
    "        best_lambda = 0.2 # Found by brute force on image 0065 kernel 512x512, sigma 1\n",
    "        # sigma = 0.3\n",
    "        sigma = 1\n",
    "        # activation = \"ReLU\"\n",
    "        activation = \"LeakyReLU\"\n",
    "\n",
    "        folder_name = f\"./tmp/images/presentation-img_{img_id}-scale_{str(scale).replace('.', '_')}-sigma_{str(sigma).replace('.', '_')}-best_lambda_{str(best_lambda).replace('.', '_')}-kernel_{k_w}-model_{model_name}-activation_{activation}-trained_on_{trained_on}-time_{datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "        \n",
    "\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "        with open(f\"{folder_name}/logs.txt\", \"w\") as log_file:\n",
    "            print(f\"Image ID: {img_id}\", file=log_file)\n",
    "            print(f\"Scale: {scale}\", file=log_file)\n",
    "            print(f\"Kernel size: {kernel_size}\", file=log_file)\n",
    "            print(f\"Stride: {stride}\", file=log_file)\n",
    "            print(f\"Best lambda: {best_lambda}\", file=log_file)\n",
    "            print(f\"Sigma: {sigma}\", file=log_file)\n",
    "            print(f\"Model name: {model_name}\", file=log_file)\n",
    "            # print(f\"Model folder: {model_folder}\", file=log_file)\n",
    "            print(f\"(Need to verify) Trained on: {trained_on}\", file=log_file)\n",
    "            # print(f\"Folder name: {folder_name}\", file=log_file)\n",
    "\n",
    "\n",
    "            # Crop image to size divisible by kernel size\n",
    "            clean_image = clean_image[:, :kernel_size[0]*n_rows, :kernel_size[0]*n_cols]\n",
    "            x_patches_clean = extract_patches_3d(\n",
    "                clean_image, \n",
    "                kernel_size=kernel_size, padding=0, stride=stride, dilation=1)\n",
    "            save_image_and_patches(\n",
    "                clean_image, x_patches_clean, \"clean\", \n",
    "                clean_image, n_rows, n_cols, folder_name, log_file=log_file, \n",
    "                # showing=True\n",
    "                )\n",
    "\n",
    "            # Add noise to the image\n",
    "            noisy_image = add_noise(clean_image, sigma=sigma)\n",
    "            x_patches = extract_patches_3d(\n",
    "                noisy_image, \n",
    "                kernel_size=kernel_size, padding=0, stride=stride, dilation=1)\n",
    "            save_image_and_patches(\n",
    "                noisy_image, x_patches, \"noisy\",\n",
    "                clean_image, n_rows, n_cols, folder_name, log_file=log_file,  \n",
    "                # showing=True\n",
    "                )\n",
    "\n",
    "\n",
    "            # Denoise the image using a single lambda\n",
    "            # best_lambda = 0.1 # Purposefully using the wrong lambda to see the effect\n",
    "            x_denoised_single_lambda = reconstruct_with_PDHG(noisy_image.unsqueeze(0), best_lambda, T=128)\n",
    "            x_denoised_single_lambda = x_denoised_single_lambda.squeeze(0)\n",
    "            x_denoised_single_lambda_patches = extract_patches_3d(\n",
    "                x_denoised_single_lambda, \n",
    "                kernel_size=kernel_size, padding=0, stride=stride, dilation=1)\n",
    "            save_image_and_patches(\n",
    "                x_denoised_single_lambda, x_denoised_single_lambda_patches, \"denoised_single_lambda\", \n",
    "                clean_image, n_rows, n_cols, folder_name, log_file=log_file,\n",
    "                # showing=True\n",
    "                )\n",
    "        \n",
    "            \n",
    "            # Denoise the patches using our method\n",
    "            torch.cuda.empty_cache()\n",
    "            unet = UNet(dim=3, n_ch_in=1).to(DEVICE)\n",
    "            total_params = sum(p.numel() for p in unet.parameters())\n",
    "            print(f\"UNet total parameters: {total_params}\", file=log_file)\n",
    "            # pdhg_state_dicts = torch.load(f\"{model_folder}/{model_name}.pt\")\n",
    "            # pdhg = DynamicImageStaticPrimalDualNN(\n",
    "            #     cnn_block=unet, \n",
    "            #     # T=128,\n",
    "            #     T=1000,\n",
    "            #     phase=\"training\",\n",
    "            #     up_bound=0.5,\n",
    "            #     mode=\"lambda_cnn\",\n",
    "            # ).to(DEVICE)\n",
    "            # pdhg.load_state_dict(pdhg_state_dicts)\n",
    "            pdhg = torch.load(f\"{model_folder}/{model_name}.pt\")\n",
    "            pdhg.eval()\n",
    "            final_lambda_reg_max = 0\n",
    "            final_lambda_reg_min = 1\n",
    "            x_patches_denoised = []\n",
    "            for i in range(len(x_patches)):\n",
    "                noisy_patch = x_patches[i]\n",
    "                noisy_patch = noisy_patch.unsqueeze(0)\n",
    "                # torch.cuda.empty_cache()\n",
    "                denoised_patch = pdhg(noisy_patch)\n",
    "\n",
    "                # Analyse lambda map\n",
    "                lambda_reg = pdhg.cnn(noisy_patch)\n",
    "                # print(f\"lambda_reg.shape: {lambda_reg.shape}\")\n",
    "                # print(f\"lambda_reg: {lambda_reg}\")\n",
    "                # Find the max and min values of lambda_reg\n",
    "                lambda_reg_max = torch.max(lambda_reg)\n",
    "                lambda_reg_min = torch.min(lambda_reg)\n",
    "                # print(f\"lambda_reg_max: {lambda_reg_max}\")\n",
    "                # print(f\"lambda_reg_min: {lambda_reg_min}\")\n",
    "                if lambda_reg_max > final_lambda_reg_max:\n",
    "                    final_lambda_reg_max = lambda_reg_max\n",
    "                if lambda_reg_min < final_lambda_reg_min:\n",
    "                    final_lambda_reg_min = lambda_reg_min\n",
    "                # torch.cuda.empty_cache()\n",
    "\n",
    "                denoised_patch = denoised_patch.squeeze(0)\n",
    "                x_patches_denoised.append(denoised_patch)\n",
    "                # free_cuda_cache(noisy_patch)\n",
    "\n",
    "            print(f\"final_lambda_reg_max: {final_lambda_reg_max}\", file=log_file)\n",
    "            print(f\"final_lambda_reg_min: {final_lambda_reg_min}\", file=log_file)\n",
    "\n",
    "            x_patches_denoised = torch.stack(x_patches_denoised)\n",
    "\n",
    "            # free_cuda_cache(pdhg)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            image_denoised_lambda_map = stitch_patches_back_to_image(\n",
    "                x_patches_denoised,\n",
    "                img_h=clean_image.size(1),\n",
    "                img_w=clean_image.size(2),\n",
    "                k_h=k_h,\n",
    "                k_w=k_w)\n",
    "            # Clip to 0 and 1\n",
    "            image_denoised_lambda_map = torch.clamp(image_denoised_lambda_map, 0, 1)\n",
    "            save_image_and_patches(\n",
    "                image_denoised_lambda_map, x_patches_denoised, \"denoised_lambda_map\", \n",
    "                clean_image, n_rows, n_cols, folder_name, log_file=log_file,\n",
    "                # showing=True\n",
    "                )\n",
    "\n",
    "            save_patches_side_by_side(\n",
    "                x_patches_clean, x_patches, x_denoised_single_lambda_patches, x_patches_denoised, \n",
    "                folder_name, log_file=log_file,\n",
    "                # showing=True\n",
    "                )\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# test_denoise(pdhg_state_dicts = torch.load(f\"./tmp/states/2024_05_24_23_04_31.pt\"))\n",
    "# test_denoise(pdhg_state_dicts = torch.load(f\"./tmp/states/2024_05_24_23_29_27.pt\"))\n",
    "# test_denoise(pdhg_state_dicts = torch.load(f\"./tmp/good states/192 by 192 by 1 - trained on image 65 only/2024_05_24_23_29_27.pt\"))\n",
    "# test_denoise(pdhg_state_dicts = torch.load(f\"./tmp/states - trained on all 200 images kernel size 192/2024_05_25_19_36_13.pt\"))\n",
    "# test_denoise(pdhg_state_dicts = torch.load(f\"./tmp/states - trained on all 200 images kernel size 192/2024_05_25_19_36_13.pt\"))\n",
    "test_denoise(\n",
    "    # model_name=\"img_0065-scale_0_5-kernel_512-sigma_0_3-T_128_2024_05_26_09_38_07_epoch_9774\",\n",
    "    # model_name=\"img_0065-scale_0_5-kernel_512-sigma_0_3-T_128_2024_05_26_07_10_36_epoch_8000\",\n",
    "    # model_name=\"model_epoch_408\",\n",
    "    # model_name=\"model_epoch_138\",\n",
    "    model_name=\"model_epoch_1562\",\n",
    "    # model_folder=f\"./tmp/states\",\n",
    "    # model_folder=f\"./tmp/model_scene_03-img_0054_to_0074-scale_0_5-kernel_512-sigma_0_3-T_128_2024_05_26_12_55_44\",\n",
    "    # model_folder=f\"./tmp/model_img_0065-scale_0_5-kernel_512-sigma_0_3-T_128-ReLU_2024_05_26_15_11_22\",\n",
    "    model_folder=f\"./tmp/model_img_0065-scale_0_5-kernel_512-sigma_1-T_128-LeakyReLU_2024_05_26_15_55_13\",\n",
    "    trained_on=\"img_0065\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_force_single_lambda():\n",
    "    \"\"\"\n",
    "    To compare this with our method\n",
    "    \"\"\"\n",
    "\n",
    "    # Try \n",
    "    with torch.no_grad(): # IMPORTANT! Need no_grad to free GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Get a clean image\n",
    "        # img_id = \"0065\"\n",
    "        img_id = \"0044\"\n",
    "        # img_id = \"0102\"\n",
    "        # img_id = \"0055\"\n",
    "        # img_id = \"0030\"\n",
    "        # scale = 0.25\n",
    "        scale = 0.5\n",
    "        clean_image = load_images([img_id], False)[0]\n",
    "        clean_image = convert_to_grayscale(clean_image)\n",
    "        clean_image = resize_image(clean_image, scale)\n",
    "        clean_image = convert_to_tensor(clean_image)\n",
    "\n",
    "        # k_w, k_h = 192, 192\n",
    "        k_w, k_h = 512, 512\n",
    "        kernel_size = [k_w, k_h, 1]\n",
    "        stride = [k_w, k_h, 1]\n",
    "        img_h = clean_image.size(1)\n",
    "        img_w = clean_image.size(2)\n",
    "        n_rows = img_h // k_h\n",
    "        n_cols = img_w // k_w\n",
    "        sigma = 0.3\n",
    "        # 0065: 0.05\n",
    "        # 0044: 0.05\n",
    "        # 0102: 0.02\n",
    "        # sigma = 1\n",
    "\n",
    "        # Crop image to size divisible by kernel size\n",
    "        clean_image = clean_image[:, :kernel_size[0]*n_rows, :kernel_size[0]*n_cols]\n",
    "\n",
    "        # Add noise to the image\n",
    "        noisy_image = add_noise(clean_image, sigma=sigma)\n",
    "\n",
    "        best_psnr = 0\n",
    "        best_lambda = -100\n",
    "        min_lambda = 0.01\n",
    "        max_lambda = 0.12\n",
    "        for single_lambda in tqdm(np.linspace(min_lambda, max_lambda, 10)):\n",
    "\n",
    "            x_denoised = reconstruct_with_PDHG(noisy_image.unsqueeze(0), single_lambda, T=128)\n",
    "            # print(f\"x_denoised size: {x_denoised.size()}\")\n",
    "\n",
    "            x_denoised_psnr = PSNR(clean_image, x_denoised)\n",
    "            print(f\"single_lambda: {single_lambda}, PSNR: {x_denoised_psnr}\")\n",
    "            if x_denoised_psnr > best_psnr:\n",
    "                best_psnr = x_denoised_psnr\n",
    "                best_lambda = single_lambda\n",
    "\n",
    "        print(f\"Best PSNR: {best_psnr} dB\")\n",
    "        print(f\"Best lambda: {best_lambda}\")\n",
    "\n",
    "        x_denoised = reconstruct_with_PDHG(noisy_image.unsqueeze(0), best_lambda, T=128)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(9,6))\n",
    "        plt.imshow(x_denoised.squeeze(0).squeeze(0).to(\"cpu\").detach().numpy(), cmap='gray')\n",
    "        plt.show();\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "brute_force_single_lambda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Results\n",
    "\n",
    "Even though the PSNR increases slightly, there are still a few problems:\n",
    "\n",
    "- The smaller details like text on a book almost always get lost.\n",
    "- The model only works on small patches so combining the patches create a very unnatural-looking image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Todo\n",
    "\n",
    "I noticed that some GPU memory (about 1 GB) is still occupied after all code was done running. When I restarted the kernel all GPU memory is then freed. Is there somewhere else I should try to force emptying GPU cache?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
