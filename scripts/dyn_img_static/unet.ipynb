{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (enc_convs): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (pools): ModuleList(\n",
      "    (0-1): 2 x MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (bottleneck): Sequential(\n",
      "    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (up_convs): ModuleList(\n",
      "    (0): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    (1): ConvTranspose3d(32, 16, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "  )\n",
      "  (dec_convs): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv3d(16, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers=4, base_filters=64, activation='relu', conv_dim=2):\n",
    "        super(UNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.base_filters = base_filters\n",
    "        self.activation = activation\n",
    "        self.conv_dim = conv_dim\n",
    "        \n",
    "        # Choose convolution and pooling layers based on the specified dimension\n",
    "        if conv_dim == 2:\n",
    "            self.Conv = nn.Conv2d\n",
    "            self.ConvTranspose = nn.ConvTranspose2d\n",
    "            self.MaxPool = nn.MaxPool2d\n",
    "        elif conv_dim == 3:\n",
    "            self.Conv = nn.Conv3d\n",
    "            self.ConvTranspose = nn.ConvTranspose3d\n",
    "            self.MaxPool = nn.MaxPool3d\n",
    "        else:\n",
    "            raise ValueError(\"conv_dim must be 2 or 3\")\n",
    "\n",
    "        # Encoder\n",
    "        self.enc_convs = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_f = in_channels if i == 0 else base_filters * (2 ** (i - 1))\n",
    "            out_f = base_filters * (2 ** i)\n",
    "            self.enc_convs.append(self.double_conv(in_f, out_f))\n",
    "            self.pools.append(self.MaxPool(2))\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.double_conv(base_filters * (2 ** (num_layers - 1)), base_filters * (2 ** num_layers))\n",
    "\n",
    "        # Decoder\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        self.dec_convs = nn.ModuleList()\n",
    "        for i in range(num_layers - 1, -1, -1):\n",
    "            in_f = base_filters * (2 ** (i + 1))\n",
    "            out_f = base_filters * (2 ** i)\n",
    "            self.up_convs.append(self.ConvTranspose(in_f, out_f, kernel_size=2, stride=2))\n",
    "            self.dec_convs.append(self.double_conv(in_f, out_f))\n",
    "\n",
    "        # Final output layer\n",
    "        self.final_conv = self.Conv(base_filters, out_channels, kernel_size=1)\n",
    "    \n",
    "    def double_conv(self, in_channels, out_channels):\n",
    "        activation_func = self.get_activation(self.activation)\n",
    "        return nn.Sequential(\n",
    "            self.Conv(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            activation_func(),\n",
    "            self.Conv(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            activation_func()\n",
    "        )\n",
    "    \n",
    "    def get_activation(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return nn.ReLU\n",
    "        elif activation == 'leaky_relu':\n",
    "            return nn.LeakyReLU\n",
    "        elif activation == 'elu':\n",
    "            return nn.ELU\n",
    "        elif activation == 'sigmoid':\n",
    "            return nn.Sigmoid\n",
    "        elif activation == 'tanh':\n",
    "            return nn.Tanh\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc_outs = []\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_convs[i](x)\n",
    "            enc_outs.append(x)\n",
    "            x = self.pools[i](x)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Decoder\n",
    "        for i in range(self.num_layers - 1, -1, -1):\n",
    "            x = self.up_convs[i](x)\n",
    "            x = torch.cat([x, enc_outs[i]], dim=1)\n",
    "            x = self.dec_convs[i](x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "# input_shape = (1, 128, 128)  # Adjust this according to your data\n",
    "input_shape = (1, 512, 512)\n",
    "in_channels = input_shape[0]\n",
    "# out_channels = 1  # For binary segmentation\n",
    "out_channels = 2\n",
    "# num_layers = 4\n",
    "num_layers = 3\n",
    "# base_filters = 64\n",
    "base_filters = 16\n",
    "# activation = 'relu'  # 'relu', 'leaky_relu', 'elu', 'sigmoid', 'tanh'\n",
    "activation = 'leaky_relu'  # 'relu', 'leaky_relu', 'elu', 'sigmoid', 'tanh'\n",
    "# conv_dim = 2  # 2 for 2D convolutions, 3 for 3D convolutions\n",
    "conv_dim = 3  # 2 for 2D convolutions, 3 for 3D convolutions\n",
    "\n",
    "model = UNet(in_channels, out_channels, num_layers, base_filters, activation, conv_dim)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch==2.3.0\n",
      "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torchvision) (1.23.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torchvision) (10.2.0)\n",
      "Collecting nvidia-nccl-cu12==2.20.5\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (1.12)\n",
      "Collecting triton==2.3.0\n",
      "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: networkx in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (4.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: filelock in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from torch==2.3.0->torchvision) (3.13.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /mnt/c/Users/t/Documents/GIT/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n",
      "Installing collected packages: triton, nvidia-nccl-cu12, torch, torchvision\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "Successfully installed nvidia-nccl-cu12-2.20.5 torch-2.3.0 torchvision-0.18.0 triton-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class NoisyImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None, noise_level=0.1):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.transform = transform\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Add noise\n",
    "        noise = torch.randn_like(image) * self.noise_level\n",
    "        noisy_image = image + noise\n",
    "        noisy_image = torch.clamp(noisy_image, 0, 1)\n",
    "        \n",
    "        return noisy_image, image\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "image_dir = 'path/to/your/images'\n",
    "dataset = NoisyImageDataset(image_dir=image_dir, transform=transform, noise_level=0.1)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
