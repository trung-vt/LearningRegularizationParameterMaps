{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code was taken from ...\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    A block of convolutional layers (1D, 2D or 3D)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in,\n",
    "        n_ch_out,\n",
    "        n_convs,\n",
    "        kernel_size=3,\n",
    "        bias=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if dim == 1:\n",
    "            conv_op = nn.Conv1d\n",
    "        if dim == 2:\n",
    "            conv_op = nn.Conv2d\n",
    "        elif dim == 3:\n",
    "            conv_op = nn.Conv3d\n",
    "\n",
    "        padding = int(np.floor(kernel_size / 2))\n",
    "\n",
    "        conv_block_list = []\n",
    "        conv_block_list.extend(\n",
    "            [\n",
    "                conv_op(\n",
    "                    n_ch_in,\n",
    "                    n_ch_out,\n",
    "                    kernel_size,\n",
    "                    padding=padding,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                ),\n",
    "                nn.LeakyReLU(), # NOTE: We use LeakyReLU instead of ReLU!!!\n",
    "                # # Can we try using ReLU instead of LeakyReLU?\n",
    "                # nn.ReLU(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i in range(n_convs - 1):\n",
    "            conv_block_list.extend(\n",
    "                [\n",
    "                    conv_op(\n",
    "                        n_ch_out,\n",
    "                        n_ch_out,\n",
    "                        kernel_size,\n",
    "                        padding=padding,\n",
    "                        bias=bias,\n",
    "                        padding_mode=padding_mode,\n",
    "                    ),\n",
    "                    nn.LeakyReLU(), # NOTE: We use LeakyReLU instead of ReLU!!!\n",
    "                    # # Can we try using ReLU instead of LeakyReLU?\n",
    "                    # nn.ReLU(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in,\n",
    "        n_enc_stages,\n",
    "        n_convs_per_stage,\n",
    "        n_filters,\n",
    "        kernel_size=3,\n",
    "        bias=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ############################\n",
    "        # print(f\"Encoder: n_ch_in: {n_ch_in}, n_enc_stages: {n_enc_stages}, n_convs_per_stage: {n_convs_per_stage}, n_filters: {n_filters}, kernel_size: {kernel_size}, bias: {bias}, padding_mode: {padding_mode}\")\n",
    "        # ############################\n",
    "\n",
    "        n_ch_list = [n_ch_in]\n",
    "        # n_ch_list = []\n",
    "        for ne in range(n_enc_stages):\n",
    "            n_ch_list.append(int(n_filters) * 2**ne) # Why n_filters here but n_ch_in in the Decoder?\n",
    "\n",
    "        # ############################\n",
    "        # print(f\"n_ch_list: {n_ch_list}\")\n",
    "        # ############################\n",
    "\n",
    "        self.enc_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(\n",
    "                    dim,\n",
    "                    n_ch_list[i],\n",
    "                    n_ch_list[i + 1],\n",
    "                    n_convs_per_stage,\n",
    "                    kernel_size=kernel_size,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                )\n",
    "                for i in range(len(n_ch_list) - 1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # ############################\n",
    "        # print(f\"Encoding Blocks: {self.enc_blocks}\")\n",
    "        # ############################\n",
    "\n",
    "        if dim == 1:\n",
    "            pool_op = nn.MaxPool1d(2)\n",
    "        elif dim == 2:\n",
    "            pool_op = nn.MaxPool2d(2)\n",
    "        elif dim == 3:\n",
    "            # TODO: Can I make it so that if there is only a single image then it reduces the pool size to 1?\n",
    "            # pool_op = nn.MaxPool3d(2)\n",
    "            pool_op = nn.MaxPool3d(1) # TODO: Change back after making the code working with 2D MaxPool\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dim value: {dim}\")\n",
    "\n",
    "        self.pool = pool_op\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            features.append(x)\n",
    "            x = self.pool(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in,\n",
    "        n_dec_stages,\n",
    "        n_convs_per_stage,\n",
    "        n_filters,\n",
    "        kernel_size=3,\n",
    "        bias=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ############################\n",
    "        # print(f\"Decoder: n_ch_in: {n_ch_in}, n_dec_stages: {n_dec_stages}, n_convs_per_stage: {n_convs_per_stage}, n_filters: {n_filters}, kernel_size: {kernel_size}, bias: {bias}, padding_mode: {padding_mode}\")\n",
    "        # ############################\n",
    "\n",
    "        n_ch_list = []\n",
    "        for ne in range(n_dec_stages):\n",
    "            n_ch_list.append(int(n_ch_in * (1 / 2) ** ne)) # Why n_ch_in here but n_filters in the Encoder?\n",
    "            # n_ch_list.append(int(n_filters * (1 / 2) ** ne)) # Why n_ch_in here but n_filters in the Encoder?\n",
    "        # n_ch_list.append(n_ch_in)\n",
    "\n",
    "        # ############################\n",
    "        # print(f\"n_ch_list: {n_ch_list}\")\n",
    "        # ############################\n",
    "\n",
    "        if dim == 1:\n",
    "            conv_op = nn.Conv1d\n",
    "            interp_mode = \"linear\"\n",
    "        elif dim == 2:\n",
    "            conv_op = nn.Conv2d\n",
    "            interp_mode = \"bilinear\"\n",
    "        elif dim == 3:\n",
    "            interp_mode = \"trilinear\"\n",
    "            conv_op = nn.Conv3d\n",
    "\n",
    "        self.interp_mode = interp_mode\n",
    "\n",
    "        padding = int(np.floor(kernel_size / 2))\n",
    "        self.upconvs = nn.ModuleList(\n",
    "            [\n",
    "                conv_op(\n",
    "                    n_ch_list[i],\n",
    "                    n_ch_list[i + 1],\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=padding,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                )\n",
    "                for i in range(len(n_ch_list) - 1)\n",
    "            ]\n",
    "        )\n",
    "        self.dec_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(\n",
    "                    dim,\n",
    "                    n_ch_list[i],\n",
    "                    n_ch_list[i + 1],\n",
    "                    n_convs_per_stage,\n",
    "                    kernel_size=kernel_size,\n",
    "                    bias=bias,\n",
    "                    padding_mode=padding_mode,\n",
    "                )\n",
    "                for i in range(len(n_ch_list) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # ############################\n",
    "        # print(f\"Padding: {padding}\")\n",
    "        # print(f\"Upconvs: {self.upconvs}\")\n",
    "        # print(f\"Decoding Blocks: {self.dec_blocks}\")\n",
    "        # ############################\n",
    "\n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.dec_blocks)):\n",
    "            enc_features = encoder_features[i]\n",
    "            enc_features_shape = enc_features.shape\n",
    "            x = nn.functional.interpolate(\n",
    "                x, enc_features_shape[2:], mode=self.interp_mode, align_corners=False\n",
    "            )\n",
    "            x = self.upconvs[i](x)\n",
    "            x = torch.cat([x, enc_features], dim=1) # IMPORTANT: Copy-and-Crop\n",
    "            x = self.dec_blocks[i](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        n_ch_in=2,\n",
    "        n_ch_out=2,\n",
    "        n_enc_stages=3,\n",
    "        # n_enc_stages=4,\n",
    "        n_convs_per_stage=2,\n",
    "        # n_convs_per_stage=3,\n",
    "        n_filters=16,\n",
    "        kernel_size=3,\n",
    "        res_connection=False,\n",
    "        bias=True,\n",
    "        padding_mode=\"zeros\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            dim=dim,\n",
    "            n_ch_in=n_ch_in,\n",
    "            n_enc_stages=n_enc_stages,\n",
    "            n_convs_per_stage=n_convs_per_stage,\n",
    "            n_filters=n_filters,\n",
    "            kernel_size=kernel_size,\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            dim=dim,\n",
    "            n_ch_in=n_filters * (2 ** (n_enc_stages - 1)),\n",
    "            # n_ch_in=n_ch_in,\n",
    "            n_dec_stages=n_enc_stages,\n",
    "            n_convs_per_stage=n_convs_per_stage,\n",
    "            n_filters=n_filters * (n_enc_stages * 2),\n",
    "            kernel_size=kernel_size,\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "\n",
    "        if dim == 1:\n",
    "            conv_op = nn.Conv1d\n",
    "        elif dim == 2:\n",
    "            conv_op = nn.Conv2d\n",
    "        elif dim == 3:\n",
    "            conv_op = nn.Conv3d\n",
    "\n",
    "        self.c1x1 = conv_op(n_filters, n_ch_out, kernel_size=1, padding=0, bias=bias)\n",
    "        if res_connection:\n",
    "            if n_ch_in == n_ch_out:\n",
    "                self.res_connection = lambda x: x\n",
    "            else:\n",
    "                self.res_connection = conv_op(n_ch_in, n_ch_out, 1)\n",
    "        else:\n",
    "            self.res_connection = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_features = self.encoder(x)\n",
    "        dec = self.decoder(enc_features[-1], enc_features[::-1][1:])\n",
    "        out = self.c1x1(dec)\n",
    "        if self.res_connection:\n",
    "            out = out + self.res_connection(x)\n",
    "        return out\n",
    "\n",
    "unet = UNet(dim=3, n_ch_in=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
