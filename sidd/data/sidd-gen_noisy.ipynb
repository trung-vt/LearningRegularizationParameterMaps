{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time_importing_torch = True\n",
    "first_time_importing_torchvision = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing torch ...\n",
      "Importing torch took 4.029273986816406e-05 seconds\n",
      "Importing torchvision ...\n",
      "Importing torchvision took 11.043753862380981 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# NOTE: Importing torch the first time will always take a long time!\n",
    "if first_time_importing_torch:\n",
    "    print(f\"Importing torch ...\")\n",
    "    import_torch_start_time = time.time() \n",
    "import torch\n",
    "if first_time_importing_torch:\n",
    "    import_torch_end_time = time.time()\n",
    "    print(f\"Importing torch took {import_torch_end_time - import_torch_start_time} seconds\")\n",
    "    first_time_importing_torch = False\n",
    "\n",
    "if first_time_importing_torchvision:\n",
    "    print(f\"Importing torchvision ...\")\n",
    "    import_torchvision_start_time = time.time()\n",
    "import torchvision\n",
    "if first_time_importing_torchvision:\n",
    "    import_torchvision_end_time = time.time()\n",
    "    print(f\"Importing torchvision took {import_torchvision_end_time - import_torchvision_start_time} seconds\")\n",
    "    first_time_importing_torchvision = False\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHEST_XRAY_BASE_DATA_PATH = \"../data/chest_xray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: CHANGE THIS TO YOUR PATH\n",
    "# # NOTE: Windows uses \\\\ instead of /\n",
    "# def load_images_chest_xray(data_path: str, ids: list) -> list:\n",
    "#     files = os.listdir(data_path)\n",
    "#     jpeg_files = [f for f in files if f.endswith(\".jpeg\")]\n",
    "\n",
    "#     images = []\n",
    "#     for id in tqdm(ids):\n",
    "#         if id >= len(jpeg_files): continue\n",
    "#         # print(f\"Loading image {id} from {data_path}\")\n",
    "#         image = Image.open(os.path.join(data_path, jpeg_files[id]))\n",
    "#         images.append(image)\n",
    "    \n",
    "#     return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_load_images_chest_xray(stage=\"test\", label=\"NORMAL\"):\n",
    "#     for img in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/{stage}/{label}\", [0]):\n",
    "#         print(img.size)\n",
    "#         plt.imshow(img, cmap='gray')\n",
    "#     plt.show();\n",
    "\n",
    "# test_load_images_chest_xray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "\n",
    "torch.set_default_device(DEVICE)\n",
    "\n",
    "PIL_to_tensor = transforms.PILToTensor(\n",
    ")\n",
    "\n",
    "tensor_to_PIL = transforms.ToPILImage(\n",
    ")\n",
    "# From docstring:     Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape\n",
    "# H x W x C to a PIL Image while adjusting the value range depending on the ``mode``.\n",
    "# Args:\n",
    "#     mode (`PIL.Image mode`_): color space and pixel depth of input data (optional).\n",
    "#         If ``mode`` is ``None`` (default) there are some assumptions made about the input data:\n",
    "#         - If the input has 4 channels, the ``mode`` is assumed to be ``RGBA``.\n",
    "#         - If the input has 3 channels, the ``mode`` is assumed to be ``RGB``.\n",
    "#         - If the input has 2 channels, the ``mode`` is assumed to be ``LA`` (L with alpha). \n",
    "#         - If the input has 1 channel, the ``mode`` is determined by the data type (i.e ``int``, ``float``, ``short``).\n",
    "# .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes\n",
    "\n",
    "\n",
    "# # Define a transform to convert PIL image to a Torch tensor \n",
    "# transform_ = transforms.Compose([ \n",
    "#     transforms.PILToTensor(),\n",
    "# ])\n",
    "# transform = transforms.PILToTensor() \n",
    "# Convert the PIL image to Torch tensor \n",
    "# img_tensor = transform(image) \n",
    "  \n",
    "\n",
    "\n",
    "def get_variable_noise(sigma_min, sigma_max):\n",
    "    return sigma_min + torch.rand(1) * (sigma_max - sigma_min)\n",
    "\n",
    "def add_noise(xf: torch.tensor, sigma) -> torch.tensor:\n",
    "    std = torch.std(xf)\n",
    "    mu = torch.mean(xf)\n",
    "\n",
    "    x_centred = (xf  - mu) / std\n",
    "\n",
    "    x_centred += sigma * torch.randn(xf.shape, dtype = xf.dtype)\n",
    "\n",
    "    xnoise = std * x_centred + mu\n",
    "\n",
    "    del std, mu, x_centred\n",
    "\n",
    "    return xnoise\n",
    "\n",
    "\n",
    "def tensor_2d_to_image(tensor_2d: torch.tensor) -> Image:\n",
    "    tensor_2d_np = tensor_2d.to(\"cpu\").detach().numpy()\n",
    "    tensor_2d_np_int = (tensor_2d_np * 255.0).clip(0, 255.0).astype(np.uint8)\n",
    "    print(f\"type(tensor_2d_np_int) = {type(tensor_2d_np_int)}\")\n",
    "    print(f\"shape: {tensor_2d_np_int.shape}\")\n",
    "    image = Image.fromarray(tensor_2d_np_int)\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_noisy_image(rgb: Image, sigma) -> Image:\n",
    "    # im_2d = torch.tensor(np.array(rgb)) / 255.0\n",
    "\n",
    "    im_2d = PIL_to_tensor(rgb).to(DEVICE).squeeze(0) / 255.0\n",
    "\n",
    "    # max_value = torch.max(im_2d)\n",
    "    # min_value = torch.min(im_2d)\n",
    "    # print(f\"max_value = {max_value}, min_value = {min_value}\")\n",
    "    # assert max_value <= 1.0 and min_value >= 0.0\n",
    "\n",
    "\n",
    "    noisy_2d = add_noise(im_2d, sigma=sigma)\n",
    "\n",
    "    # noisy_im_v1 = tensor_2d_to_image(noisy_2d)\n",
    "\n",
    "    noisy_2d = torch.clamp((noisy_2d * 255), 0, 255).to(torch.uint8)\n",
    "    noisy_im_v2 = tensor_to_PIL(noisy_2d) \n",
    "\n",
    "    # plt.imshow(noisy_im_v1, cmap='gray')\n",
    "    # plt.title(f\"v1\")\n",
    "    # plt.show();\n",
    "    # plt.imshow(noisy_im_v2, cmap='gray')\n",
    "    # plt.title(f\"v2\")\n",
    "    # plt.show();\n",
    "\n",
    "    # return noisy_im_v1\n",
    "    return noisy_im_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_gen_noisy_images():\n",
    "#     # img = load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/test/NORMAL\", [0])[0]\n",
    "#     img = Image.open(\"../data/chest_xray/test/NORMAL/IM-0001-0001.jpeg\")\n",
    "#     plt.imshow(img, cmap='gray')\n",
    "#     plt.title(\"Original image\")\n",
    "#     plt.show();\n",
    "    \n",
    "#     noisy_img = get_noisy_image(img, 0.9)\n",
    "#     plt.imshow(noisy_img, cmap='gray')\n",
    "#     plt.title(\"Noisy image\")\n",
    "#     plt.show();\n",
    "\n",
    "# test_gen_noisy_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_noisy_images_for_sigma(data_path, subfolder, label, files, output_path, sigma, overwrite=False):\n",
    "    for file in tqdm(files):\n",
    "        output_file = f\"{output_path}/{file}\"\n",
    "        if not overwrite and os.path.exists(output_file):\n",
    "            continue\n",
    "\n",
    "        img = Image.open(f\"{data_path}/{subfolder}/{label}/{file}\")\n",
    "        noisy_img = get_noisy_image(img, sigma)\n",
    "        noisy_img.save(output_file)\n",
    "\n",
    "\n",
    "def gen_noisy_images(sigmas, overwrite=False):\n",
    "    sidd_data_path = f\"../data/images/medium\"\n",
    "    base_input_path = f\"{sidd_data_path}/Data\"\n",
    "    ground_truth = \"GT\"\n",
    "    for sigma in sigmas:\n",
    "        sigma = round(sigma, 1)\n",
    "        print(f\"Generating noisy images for sigma={sigma} (overwrite={overwrite})\")\n",
    "        sigma_str = str(sigma).replace(\".\", \"_\")\n",
    "        base_output_path = f\"{sidd_data_path}/rgb_noise_{sigma_str}\"\n",
    "        os.makedirs(base_output_path, exist_ok=True)\n",
    "        subfolders = os.listdir(base_input_path)\n",
    "        for subfolder in tqdm(subfolders):\n",
    "            input_path = f\"{base_input_path}/{subfolder}\"\n",
    "            output_path = f\"{base_output_path}/{subfolder}\"\n",
    "\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "            input_files = os.listdir(input_path)\n",
    "            for file in input_files:\n",
    "\n",
    "                if not file.endswith(\".PNG\") or not ground_truth in file:\n",
    "                    continue\n",
    "\n",
    "                output_file = f\"{output_path}/{file}\"\n",
    "                if not overwrite and os.path.exists(output_file):\n",
    "                    continue\n",
    "\n",
    "                img = Image.open(f\"{input_path}/{file}\")\n",
    "                noisy_img = get_noisy_image(img, sigma)\n",
    "                noisy_img.save(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating noisy images for sigma=0.1 (overwrite=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [17:44<00:00,  6.65s/it]\n"
     ]
    }
   ],
   "source": [
    "gen_noisy_images([0.1], overwrite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
