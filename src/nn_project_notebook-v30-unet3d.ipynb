{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time_importing_torch = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing torch ...\n",
      "Importing torch took 13.045548915863037 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# NOTE: Importing torch the first time will always take a long time!\n",
    "import time\n",
    "# NOTE: Importing torch the first time will always take a long time!\n",
    "if first_time_importing_torch:\n",
    "    print(f\"Importing torch ...\")\n",
    "    import_torch_start_time = time.time() \n",
    "import torch\n",
    "if first_time_importing_torch:\n",
    "    import_torch_end_time = time.time()\n",
    "    print(f\"Importing torch took {import_torch_end_time - import_torch_start_time} seconds\")\n",
    "    first_time_importing_torch = False\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "\n",
    "from skimage.metrics import structural_similarity\n",
    "# from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Optional\n",
    "from tqdm import tqdm # progress bar\n",
    "\n",
    "import wandb # Optional, for logging\n",
    "\n",
    "import json\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/t/Documents/GIT/DISSERTATION/LearningRegularizationParameterMaps/venv/bin/python\n",
      "Torch version: 2.3.0+cu121\n",
      "Path: /mnt/c/Users/t/Documents/GIT/DISSERTATION/LearningRegularizationParameterMaps/src\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Path: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISABLING_TESTS = False\n",
    "DISABLING_TESTS = True   # Disable tests for less output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(f\"Using {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(f\"Using {torch.backends.mps.get_device_name(0)} with MPS\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "torch.set_default_device(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIDD_DATA_PATH = \"../data/dyn_img_static/tmp/SIDD_Small_sRGB_Only/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHEST_XRAY_BASE_DATA_PATH = \"../data/chest_xray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'project': 'chest_xray', 'dataset': '../data/chest_xray', 'train_data_path': '../data/chest_xray/train/NORMAL', 'val_data_path': '../data/chest_xray/val/NORMAL', 'test_data_path': '../data/chest_xray/test/NORMAL', 'train_num_samples': 200, 'val_num_samples': 8, 'test_num_samples': 1, 'resize_square': 256, 'min_sigma': 0.1, 'max_sigma': 0.5, 'batch_size': 1, 'random_seed': 42, 'architecture': 'UNET-PDHG', 'in_channels': 1, 'out_channels': 2, 'init_filters': 32, 'n_blocks': 3, 'activation': 'LeakyReLU', 'downsampling_kernel': (2, 2, 1), 'downsampling_mode': 'max', 'upsampling_kernel': (2, 2, 1), 'upsampling_mode': 'linear_interpolation', 'optimizer': 'Adam', 'learning_rate': 0.0001, 'loss_function': 'MSELoss', 'up_bound': 0, 'T': 256, 'epochs': 10000, 'device': 'cuda:0', 'wandb_mode': 'online', 'save_epoch_wandb': 10000, 'save_epoch_local': 10, 'save_dir': 'tmp_2'}\n"
     ]
    }
   ],
   "source": [
    "def get_config():\n",
    "    CHEST_XRAY_BASE_DATA_PATH = \"../data/chest_xray\"\n",
    "    return {\n",
    "        \"project\": \"chest_xray\",\n",
    "        \"dataset\": CHEST_XRAY_BASE_DATA_PATH,\n",
    "        \"train_data_path\": f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\",\n",
    "        \"val_data_path\": f\"{CHEST_XRAY_BASE_DATA_PATH}/val/NORMAL\",\n",
    "        \"test_data_path\": f\"{CHEST_XRAY_BASE_DATA_PATH}/test/NORMAL\",\n",
    "        \"train_num_samples\": 200,\n",
    "        \"val_num_samples\": 8,\n",
    "        \"test_num_samples\": 1,\n",
    "\n",
    "        # \"patch\": 512,\n",
    "        # \"stride\": 512,\n",
    "        \"resize_square\": 256,\n",
    "        \"min_sigma\": 0.1,\n",
    "        \"max_sigma\": 0.5,\n",
    "        \"batch_size\": 1,\n",
    "        \"random_seed\": 42,\n",
    "\n",
    "        \"architecture\": \"UNET-PDHG\",\n",
    "        \"in_channels\": 1,\n",
    "        \"out_channels\": 2,\n",
    "        \"init_filters\": 32,\n",
    "        \"n_blocks\": 3,\n",
    "        \"activation\": \"LeakyReLU\",\n",
    "        \"downsampling_kernel\": (2, 2, 1),\n",
    "        \"downsampling_mode\": \"max\",\n",
    "        \"upsampling_kernel\": (2, 2, 1),\n",
    "        \"upsampling_mode\": \"linear_interpolation\",\n",
    "\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"loss_function\": \"MSELoss\",\n",
    "\n",
    "        # \"up_bound\": 0.5,\n",
    "        \"up_bound\": 0,\n",
    "        \"T\": 256, # Higher T, NET does not have to try as hard? Less overfitting?\n",
    "\n",
    "        \"epochs\": 10_000,\n",
    "        \"device\": \"cuda:0\",\n",
    "\n",
    "        \"wandb_mode\": \"online\",\n",
    "        \"save_epoch_wandb\": 10_000,\n",
    "        \"save_epoch_local\": 10,\n",
    "        \"save_dir\": \"tmp_2\",\n",
    "    }\n",
    "\n",
    "print(get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Import the image and transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # REMEMBER TO COMMENT THIS OUT IF THE DATA HAS BEEN DOWNLOADED!\n",
    "# !wget https://competitions.codalab.org/my/datasets/download/a26784fe-cf33-48c2-b61f-94b299dbc0f2\n",
    "# !unzip \"a26784fe-cf33-48c2-b61f-94b299dbc0f2\" -d ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load SIDD images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_npy_file(sample_path: str, scale_factor: float) -> np.ndarray:\t\t\t\n",
    "    scale_factor_str = str(scale_factor).replace('.','_')\n",
    "    xf = np.load(os.path.join(sample_path, f\"xf_scale_factor{scale_factor_str}.npy\"))\n",
    "    xf = torch.tensor(xf, dtype=torch.float)\n",
    "    xf = xf.unsqueeze(0) / 255\n",
    "    return xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CHANGE THIS TO YOUR PATH\n",
    "# NOTE: Windows uses \\\\ instead of /\n",
    "def load_images_SIDD(ids: list, take_npy_files: bool) -> list:\n",
    "    data_path = SIDD_DATA_PATH\n",
    "    k = 0\n",
    "\n",
    "    images = []\n",
    "\n",
    "    for folder in os.listdir(data_path):\n",
    "        img_id = folder[:4]\t# The first 4 characters of folder name is the image id (0001, 0002, ..., 0200)\n",
    "        if img_id not in ids:\n",
    "            continue\n",
    "        k += 1\n",
    "        print(f'loading image id {img_id}, {k}/{len(ids)}')\n",
    "\n",
    "        files_path = os.path.join(data_path, folder)\n",
    "\n",
    "        # if take_npy_files:\n",
    "        #     xf = get_npy_file(files_path, scale_factor)\n",
    "        #     images.append(xf)\n",
    "        #     continue\n",
    "\n",
    "        # Use only the ground truth images\n",
    "        file = \"GT_SRGB_010.PNG\"  # GT = Ground Truth\n",
    "\n",
    "        image = Image.open(os.path.join(files_path, file))\n",
    "        assert image.mode == 'RGB', f\"Image mode is not RGB: {image.mode}\" # For now, expect RGB images\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_images_SIDD():\n",
    "    if DISABLING_TESTS: return\n",
    "    for img in load_images_SIDD([\"0065\"], False):\n",
    "        print(img.size)\n",
    "        plt.imshow(img)\n",
    "\n",
    "test_load_images_SIDD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Chest X-ray images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CHANGE THIS TO YOUR PATH\n",
    "# NOTE: Windows uses \\\\ instead of /\n",
    "def load_images_chest_xray(data_path: str, ids: list) -> list:\n",
    "    files = os.listdir(data_path)\n",
    "    jpeg_files = [f for f in files if f.endswith(\".jpeg\")]\n",
    "\n",
    "    images = []\n",
    "    for id in tqdm(ids):\n",
    "        if id >= len(jpeg_files): continue\n",
    "        # print(f\"Loading image {id} from {data_path}\")\n",
    "        image = Image.open(os.path.join(data_path, jpeg_files[id]))\n",
    "        images.append(image)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_images_chest_xray(stage=\"train\", label=\"NORMAL\"):\n",
    "    if DISABLING_TESTS: return\n",
    "    for img in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/{stage}/{label}\", [0]):\n",
    "        print(img.size)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "    plt.show();\n",
    "\n",
    "test_load_images_chest_xray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "#### Convert image to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayscale(image: Image) -> Image:\n",
    "    return image.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert_to_grayscale():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for img in load_images_SIDD([\"0065\"], False):\n",
    "    for img in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "        img = convert_to_grayscale(img)\n",
    "        plt.imshow(img, cmap='gray') # cmap='gray' for proper display in black and white. It does not convert the image to grayscale.\n",
    "\n",
    "test_convert_to_grayscale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_square(image: Image) -> Image:\n",
    "    width, height = image.size\n",
    "    new_size = min(width, height)\n",
    "    left = (width - new_size) / 2\n",
    "    top = (height - new_size) / 2\n",
    "    right = (width + new_size) / 2\n",
    "    bottom = (height + new_size) / 2\n",
    "    return image.crop((left, top, right, bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_crop_to_square():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for img in load_images_SIDD([\"0083\"], False):\n",
    "    for img in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show();\n",
    "        img = crop_to_square(img)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "test_crop_to_square()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_square_and_resize(image: Image, side_len: int) -> Image:\n",
    "    image = crop_to_square(image)\n",
    "    return image.resize(size=(side_len, side_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_crop_to_square_and_resize():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for img in load_images_SIDD([\"0083\"], False):\n",
    "    for img in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show();\n",
    "        img = crop_to_square_and_resize(img, 120)\n",
    "        print(img.size)\n",
    "        plt.imshow(img, cmap='gray') # cmap='gray' for proper display in black and white. It does not convert the image to grayscale.\n",
    "        plt.show();\n",
    "\n",
    "test_crop_to_square_and_resize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numpy(image):\n",
    "    image_data = np.asarray(image)\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert_to_numpy():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for img in load_images_SIDD([\"0083\"], False):\n",
    "    for img in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "        img = convert_to_grayscale(img)\n",
    "        print(f\"Before conversion: {type(img)}\")\n",
    "        image_data = convert_to_numpy(img)\n",
    "        print(f\"After conversion: {type(image_data)}\")\n",
    "        # plt.imshow still works with numpy array\n",
    "        plt.imshow(image_data, cmap='gray')\n",
    "\n",
    "test_convert_to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to tensor\n",
    "\n",
    "For efficient computation on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor_4D(image_numpy):\n",
    "    # xf = []\n",
    "    # xf.append(image_numpy)\n",
    "    # xf = np.stack(xf, axis=-1)\n",
    "    # xf = torch.tensor(xf, dtype=torch.float)\n",
    "    xf = torch.tensor(image_numpy, dtype=torch.float)\n",
    "    xf = xf.unsqueeze(0)\n",
    "    xf = xf.unsqueeze(-1)\n",
    "    xf = xf / 255 # Normalise from [0, 255] to [0, 1]\n",
    "    return xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert_to_tensor():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for image in load_images_SIDD([\"0083\"], False):\n",
    "    for image in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "        image = convert_to_grayscale(image)\n",
    "        image_numpy = convert_to_numpy(image)\n",
    "        image_tensor_4D = convert_to_tensor_4D(image_numpy)\n",
    "        print(image_tensor_4D.size())\n",
    "        plt.imshow(image_tensor_4D.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "\n",
    "\n",
    "test_convert_to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add synthetic noise\n",
    "\n",
    "<!-- artificial Gaussian noise\n",
    "\n",
    "Noise can occur in reality.\n",
    "\n",
    "It is difficult to obtain a pair of clean and noisy images of one exact same scene.\n",
    "\n",
    "For training, it is common to add synthetic noise to an image that is considered clean and then try to reconstruct it.\n",
    "\n",
    "There are many types of noise and different ways to add noise. We can add salt-and-pepper noise. (?)We can add more noise in some parts and less in others. We can use a combination of noise-adding strategies to build more robust models.\n",
    "\n",
    "For our purpose, we will focus on Gaussian noise. This is sufficient for most cases. \n",
    "\n",
    "(?) We will add noise with the same probability for each pixel (not using the strategies of focusing on certain regions) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable_noise(sigma_min, sigma_max):\n",
    "    return sigma_min + torch.rand(1) * (sigma_max - sigma_min)\n",
    "\n",
    "def add_noise(xf: torch.tensor, sigma) -> torch.tensor:\n",
    "    std = torch.std(xf)\n",
    "    mu = torch.mean(xf)\n",
    "\n",
    "    x_centred = (xf  - mu) / std\n",
    "\n",
    "    x_centred += sigma * torch.randn(xf.shape, dtype = xf.dtype)\n",
    "\n",
    "    xnoise = std * x_centred + mu\n",
    "\n",
    "    del std, mu, x_centred\n",
    "\n",
    "    return xnoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_noise():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for rgb_image in load_images_SIDD([\"0083\"], False):\n",
    "    for rgb_image in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "        grayscale_image = convert_to_grayscale(rgb_image)\n",
    "        grayscale_image = crop_to_square_and_resize(grayscale_image, 120)\n",
    "        print(f\"grayscale_image.size: {grayscale_image.size}\")\n",
    "        image_numpy = convert_to_numpy(grayscale_image)\n",
    "        image_tensor_4D = convert_to_tensor_4D(image_numpy)\n",
    "        constant_noise_img = add_noise(image_tensor_4D, sigma=0.1)\n",
    "        variable_noise_img = add_noise(image_tensor_4D, get_variable_noise(\n",
    "            sigma_min=0.1, sigma_max=0.2))\n",
    "        plt.imshow(grayscale_image, cmap='gray')\n",
    "        plt.show();\n",
    "        plt.imshow(constant_noise_img.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "        plt.imshow(variable_noise_img.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "test_add_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Calculate PSNR\n",
    "\n",
    "PSNR is a common metrics for noisy image.\n",
    "\n",
    "Compare before and after adding synthetic noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(original, compressed): \n",
    "    mse = torch.mean((original - compressed) ** 2) \n",
    "    if(mse == 0): # MSE is zero means no noise is present in the signal. \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    # max_pixel = 255.0\n",
    "    max_pixel = 1.0\n",
    "    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse)) \n",
    "\n",
    "    del mse\n",
    "\n",
    "    return psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_PSNR():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for rgb_image in load_images_SIDD([\"0083\"], False):\n",
    "    for rgb_image in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "        grayscale_image = convert_to_grayscale(rgb_image)\n",
    "        grayscale_image = crop_to_square_and_resize(grayscale_image, 120)\n",
    "        image_numpy = convert_to_numpy(grayscale_image)\n",
    "        image_tensor_4D = convert_to_tensor_4D(image_numpy)\n",
    "\n",
    "        print(f\"PSNR of original image: {PSNR(image_tensor_4D, image_tensor_4D)} dB\")\n",
    "        plt.imshow(image_tensor_4D.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "        noisy_image_tensor_4D = add_noise(image_tensor_4D, sigma=0.5)\n",
    "        print(f\"PSNR of constant noise image: {PSNR(noisy_image_tensor_4D, image_tensor_4D):.2f} dB\")\n",
    "        plt.imshow(noisy_image_tensor_4D.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "\n",
    "test_PSNR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Calculate SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSIM(tensor_2D_a: torch.Tensor, tensor_2D_b: torch.Tensor, data_range: float=1) -> float:\n",
    "    return structural_similarity(\n",
    "        tensor_2D_a.to(\"cpu\").detach().numpy(), \n",
    "        tensor_2D_b.to(\"cpu\").detach().numpy(),\n",
    "        data_range=data_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_SSIM(sigma=0.5):\n",
    "    if DISABLING_TESTS: return\n",
    "    # for rgb_image in load_images_SIDD([\"0083\"], False):\n",
    "    for rgb_image in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "        grayscale_image = convert_to_grayscale(rgb_image)\n",
    "        grayscale_image = crop_to_square_and_resize(grayscale_image, 120)\n",
    "        image_numpy = convert_to_numpy(grayscale_image)\n",
    "        image_tensor_4D = convert_to_tensor_4D(image_numpy)\n",
    "\n",
    "        image_tensor_2D = image_tensor_4D.squeeze(0).squeeze(-1)\n",
    "        print(f\"image_tensor_2D: {image_tensor_2D.size()}\")\n",
    "        print(f\"SSIM of original image: {SSIM(image_tensor_2D, image_tensor_2D)}\")\n",
    "        plt.imshow(image_tensor_2D.cpu(), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "        noisy_image_tensor_2D = add_noise(image_tensor_2D, sigma=sigma)\n",
    "        print(f\"noisy_image_tensor_2D: {noisy_image_tensor_2D.size()}\")\n",
    "        print(f\"SSIM of noisy image (sigma={sigma}): {SSIM(noisy_image_tensor_2D, image_tensor_2D):.2f}\")\n",
    "        plt.imshow(noisy_image_tensor_2D.cpu(), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "test_SSIM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Reconstruct an image with PDHG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the gradient\n",
    "\n",
    "<!-- The gradient is a Laplacian ?\n",
    "\n",
    "There are $x$ gradient and $y$ gradient -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "class GradOperators(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def diff_kernel(ndim, mode):\n",
    "        if mode == \"doublecentral\":\n",
    "            kern = torch.tensor((-1, 0, 1))\n",
    "        elif mode == \"central\":\n",
    "            kern = torch.tensor((-1, 0, 1)) / 2\n",
    "        elif mode == \"forward\":\n",
    "            kern = torch.tensor((0, -1, 1))\n",
    "        elif mode == \"backward\":\n",
    "            kern = torch.tensor((-1, 1, 0))\n",
    "        else:\n",
    "            raise ValueError(f\"mode should be one of (central, forward, backward, doublecentral), not {mode}\")\n",
    "        kernel = torch.zeros(ndim, 1, *(ndim * (3,)))\n",
    "        for i in range(ndim):\n",
    "            idx = tuple([i, 0, *(i * (1,)), slice(None), *((ndim - i - 1) * (1,))])\n",
    "            kernel[idx] = kern\n",
    "        return kernel\n",
    "\n",
    "    def __init__(self, dim:int=2, mode:str=\"doublecentral\", padmode:str = \"circular\"):\n",
    "        \"\"\"\n",
    "        An Operator for finite Differences / Gradients\n",
    "        Implements the forward as apply_G and the adjoint as apply_GH.\n",
    "        \n",
    "        Args:\n",
    "            dim (int, optional): Dimension. Defaults to 2.\n",
    "            mode (str, optional): one of doublecentral, central, forward or backward. Defaults to \"doublecentral\".\n",
    "            padmode (str, optional): one of constant, replicate, circular or refelct. Defaults to \"circular\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"kernel\", self.diff_kernel(dim, mode), persistent=False)\n",
    "        self._dim = dim\n",
    "        self._conv = (torch.nn.functional.conv1d, torch.nn.functional.conv2d, torch.nn.functional.conv3d)[dim - 1]\n",
    "        self._convT = (torch.nn.functional.conv_transpose1d, torch.nn.functional.conv_transpose2d, torch.nn.functional.conv_transpose3d)[dim - 1]\n",
    "        self._pad = partial(torch.nn.functional.pad, pad=2 * dim * (1,), mode=padmode)\n",
    "        if mode == 'central':\n",
    "            self._norm = (self.dim) ** (1 / 2)\n",
    "        else:\n",
    "            self._norm = (self.dim * 4) ** (1 / 2)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim\n",
    "    \n",
    "    def apply_G(self, x):\n",
    "        \"\"\"\n",
    "        Forward\n",
    "        \"\"\"\n",
    "        if x.is_complex():\n",
    "            xr = torch.view_as_real(x).moveaxis(-1, 0)\n",
    "        else:\n",
    "            xr = x\n",
    "        xr = xr.reshape(-1, 1, *x.shape[-self.dim :])\n",
    "        xp = self._pad(xr)\n",
    "        y = self._conv(xp, weight=self.kernel, bias=None, padding=0)\n",
    "        if x.is_complex():\n",
    "            y = y.reshape(2, *x.shape[: -self.dim], self.dim, *x.shape[-self.dim :])\n",
    "            y = torch.view_as_complex(y.moveaxis(0, -1).contiguous())\n",
    "        else:\n",
    "            y = y.reshape(*x.shape[0 : -self.dim], self.dim, *x.shape[-self.dim :])\n",
    "\n",
    "        del x, xr, xp\n",
    "\n",
    "        return y\n",
    "\n",
    "    def apply_GH(self, x):\n",
    "        \"\"\"\n",
    "        Adjoint\n",
    "        \"\"\"\n",
    "        if x.is_complex():\n",
    "            xr = torch.view_as_real(x).moveaxis(-1, 0)\n",
    "        else:\n",
    "            xr = x\n",
    "        xr = xr.reshape(-1, self.dim, *x.shape[-self.dim :])\n",
    "        xp = self._pad(xr)\n",
    "        y = self._convT(xp, weight=self.kernel, bias=None, padding=2)\n",
    "        if x.is_complex():\n",
    "            y = y.reshape(2, *x.shape[: -self.dim - 1], *x.shape[-self.dim :])\n",
    "            y = torch.view_as_complex(y.moveaxis(0, -1).contiguous())\n",
    "        else:\n",
    "            y = y.reshape(*x.shape[: -self.dim - 1], *x.shape[-self.dim :])\n",
    "\n",
    "        del x, xr, xp\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def apply_GHG(self, x):\n",
    "        if x.is_complex():\n",
    "            xr = torch.view_as_real(x).moveaxis(-1, 0)\n",
    "        else:\n",
    "            xr = x\n",
    "        xr = xr.reshape(-1, 1, *x.shape[-self.dim :])\n",
    "        xp = self._pad(xr)\n",
    "        tmp = self._conv(xp, weight=self.kernel, bias=None, padding=0)\n",
    "        tmp = self._pad(tmp)\n",
    "        y = self._convT(tmp, weight=self.kernel, bias=None, padding=2)\n",
    "        if x.is_complex():\n",
    "            y = y.reshape(2, *x.shape)\n",
    "            y = torch.view_as_complex(y.moveaxis(0, -1).contiguous())\n",
    "        else:\n",
    "            y = y.reshape(*x.shape)\n",
    "\n",
    "        del x, xr, xp, tmp\n",
    "\n",
    "        return y\n",
    "\n",
    "    def forward(self, x, direction=1):\n",
    "        if direction>0:\n",
    "            return self.apply_G(x)\n",
    "        elif direction<0:\n",
    "            return self.apply_GH(x)\n",
    "        else:\n",
    "            return self.apply_GHG(x)\n",
    "\n",
    "    @property\n",
    "    def normGHG(self):\n",
    "        return self._norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function for PDHG: Clip act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "class ClipAct(nn.Module):\n",
    "    def forward(self, x, threshold):\n",
    "        return clipact(x, threshold)\n",
    "\n",
    "\n",
    "def clipact(x, threshold):\n",
    "    is_complex = x.is_complex()\n",
    "    if is_complex:\n",
    "        x = torch.view_as_real(x)\n",
    "        threshold = threshold.unsqueeze(-1)\n",
    "    x = torch.clamp(x, -threshold, threshold)\n",
    "    if is_complex:\n",
    "        x = torch.view_as_complex(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only PDHG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, running PDHG with T large (many iterations in PDGH) will make GPU memory full?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "def reconstruct_with_PDHG(\n",
    "        x_dynamic_image_tensor_5D, lambda_reg, T, \n",
    "        # lambda_reg_container=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reconstructs the image using the PDHG algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        dynamic_image_tensor_5D: The (noisy) (dynamic) image tensor.\n",
    "        Size of the tensor: (`patches`, `channels`, `Nx`, `Ny`, `Nt`) where\n",
    "        \n",
    "        - `patches`: number of patches\n",
    "        - `channels`: number of (colour) channels\n",
    "        - `Nx`: number of pixels in x\n",
    "        - `Ny`: number of pixels in y\n",
    "        - `Nt`: number of time steps (frames)\n",
    "\n",
    "        lambda_reg: The regularization parameter. Can be a scalar or a tensor of suitable size.\n",
    "        T: Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        The reconstructed image tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    dim = 3\n",
    "    patches, channels, Nx, Ny, Nt = x_dynamic_image_tensor_5D.shape\n",
    "    \n",
    "    assert channels == 1, \"Only grayscale images are supported.\"\n",
    "\n",
    "    device = x_dynamic_image_tensor_5D.device\n",
    "\n",
    "    # starting values\n",
    "    xbar = x_dynamic_image_tensor_5D.clone()\n",
    "    x0 = x_dynamic_image_tensor_5D.clone()\n",
    "    xnoisy = x_dynamic_image_tensor_5D.clone()\n",
    "\n",
    "    # dual variable\n",
    "    p = x_dynamic_image_tensor_5D.clone()\n",
    "    q = torch.zeros(patches, dim, Nx, Ny, Nt, dtype=x_dynamic_image_tensor_5D.dtype).to(device)\n",
    "\n",
    "    # operator norms\n",
    "    op_norm_AHA = torch.sqrt(torch.tensor(1.0))\n",
    "    op_norm_GHG = torch.sqrt(torch.tensor(12.0))\n",
    "    # operator norm of K = [A, \\nabla]\n",
    "    # https://iopscience.iop.org/article/10.1088/0031-9155/57/10/3065/pdf,\n",
    "    # see page 3083\n",
    "    L = torch.sqrt(op_norm_AHA**2 + op_norm_GHG**2)\n",
    "\n",
    "    tau = nn.Parameter(\n",
    "        torch.tensor(10.0), requires_grad=True\n",
    "    )  # starting value approximately  1/L\n",
    "    sigma = nn.Parameter(\n",
    "        torch.tensor(10.0), requires_grad=True\n",
    "    )  # starting value approximately  1/L\n",
    "\n",
    "    # theta should be in \\in [0,1]\n",
    "    theta = nn.Parameter(\n",
    "        torch.tensor(10.0), requires_grad=True\n",
    "    )  # starting value approximately  1\n",
    "\n",
    "    # sigma, tau, theta\n",
    "    sigma = (1 / L) * torch.sigmoid(sigma)  # \\in (0,1/L)\n",
    "    tau = (1 / L) * torch.sigmoid(tau)  # \\in (0,1/L)\n",
    "    theta = torch.sigmoid(theta)  # \\in (0,1)\n",
    "\n",
    "    GradOps = GradOperators(\n",
    "        dim=dim, \n",
    "        mode=\"forward\", padmode=\"circular\")\n",
    "    clip_act = ClipAct()\n",
    "    # Algorithm 2 - Unrolled PDHG algorithm (page 18)\n",
    "    # TODO: In the paper, L is one of the inputs but not used anywhere in the pseudo code???\n",
    "    for kT in range(T):\n",
    "        # update p\n",
    "        p =  (p + sigma * (xbar - xnoisy) ) / (1. + sigma)\n",
    "        # update q\n",
    "        q = clip_act(q + sigma * GradOps.apply_G(xbar), lambda_reg)\n",
    "\n",
    "        x1 = x0 - tau * p - tau * GradOps.apply_GH(q)\n",
    "\n",
    "        if kT != T - 1:\n",
    "            # update xbar\n",
    "            xbar = x1 + theta * (x1 - x0)\n",
    "            x0 = x1\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    del x_dynamic_image_tensor_5D\n",
    "    del xbar, x0, xnoisy\n",
    "    del p, q\n",
    "    del op_norm_AHA, op_norm_GHG, L\n",
    "    del tau, sigma, theta\n",
    "    del GradOps\n",
    "    del clip_act\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # if lambda_reg_container is not None:\n",
    "    #     assert isinstance(lambda_reg_container, list), f\"lambda_reg_container should be a list, not {type(lambda_reg_container)}\"\n",
    "    #     lambda_reg_container.append(lambda_reg) # For comparison\n",
    "\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reconstruct_with_PDHG():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for rgb_image in load_images_SIDD([\"0083\"], False):\n",
    "    for rgb_image in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "        grayscale_image = convert_to_grayscale(rgb_image)\n",
    "        grayscale_image = crop_to_square_and_resize(grayscale_image, 512)\n",
    "        image_numpy = convert_to_numpy(grayscale_image)\n",
    "\n",
    "        image_tensor_4D = convert_to_tensor_4D(image_numpy)\n",
    "        print(f\"Image tensor size: {image_tensor_4D.size()}\")\n",
    "        assert len(image_tensor_4D.size()) == 4, \"The image should be 4D\"\n",
    "        plt.imshow(image_tensor_4D.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "        TEST_SIGMA = 0.5  # Relatively high noise\n",
    "        noisy_image_tensor_4D = add_noise(image_tensor_4D, sigma=TEST_SIGMA)\n",
    "        print(f\"PSNR of constant noise image: {PSNR(image_tensor_4D, noisy_image_tensor_4D):.2f} dB\")\n",
    "        print(f\"SSIM of constant noise image: {SSIM(image_tensor_4D.squeeze(0).squeeze(-1), noisy_image_tensor_4D.squeeze(0).squeeze(-1)):.2f}\")\n",
    "        plt.imshow(noisy_image_tensor_4D.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "        TEST_LAMBDA = 0.04\n",
    "        pdhg_input_tensor_5D = noisy_image_tensor_4D.unsqueeze(0)\n",
    "        print(f\"PDHG input size: {pdhg_input_tensor_5D.size()}\")\n",
    "        assert len(pdhg_input_tensor_5D.size()) == 5, \"The input for PDHG should be 5D\"\n",
    "        denoised_image_tensor_5D = reconstruct_with_PDHG(\n",
    "            pdhg_input_tensor_5D, \n",
    "            lambda_reg=TEST_LAMBDA, \n",
    "            T=128)\n",
    "        \n",
    "        denoised_image_tensor_5D = torch.clamp(denoised_image_tensor_5D, 0, 1) # Clip the values to 0 and 1\n",
    "        psnr_value_denoised = PSNR(image_tensor_4D, denoised_image_tensor_5D.squeeze(0))\n",
    "        print(f\"PSNR of reconstructed image: {psnr_value_denoised:.2f} dB\")\n",
    "        denoised_image_numpy_3D = denoised_image_tensor_5D.squeeze(0).squeeze(0).to(\"cpu\").detach().numpy()\n",
    "        plt.imshow(denoised_image_numpy_3D, cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\"\"\n",
    "In this example, a lot of noise has been applied to the original image. The PDHG algorithm tries to reconstruct the image from the noisy image. It did remove some noise and improved the PSNR value. However, the quality has been degraded significantly. We will see whether we can improve this by learning a set of parameters map.\n",
    "\"\"\")\n",
    "    \n",
    "    # The lambda parameter is the regularization parameter. The higher the lambda, the more the regularization. The T parameter is the number of iterations. The higher the T, the more the iterations. The PSNR value is the Peak Signal to Noise Ratio. The higher the PSNR, the better the reconstruction.\n",
    "\n",
    "test_reconstruct_with_PDHG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Full Architecture\n",
    "\n",
    "<!-- UNET to PDHG\n",
    "\n",
    "The whole architecture can be seen as unsupervised: The data only contains (clean) images.\n",
    "\n",
    "The whole model: Input is an image. Output is also an image.\n",
    "\n",
    "The UNET actually only outputs the regularisation parameter map. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "class DynamicImageStaticPrimalDualNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        T=128,\n",
    "        cnn_block=None,\n",
    "        mode=\"lambda_cnn\",\n",
    "        up_bound=0,\n",
    "        phase=\"training\",\n",
    "    ):\n",
    "        # print(f\"Running: {DynamicImageStaticPrimalDualNN.__name__}\")\n",
    "        super(DynamicImageStaticPrimalDualNN, self).__init__()\n",
    "\n",
    "        # gradient operators and clipping function\n",
    "        dim = 3\n",
    "        self.GradOps = GradOperators(dim, mode=\"forward\", padmode=\"circular\")\n",
    "\n",
    "        # operator norms\n",
    "        self.op_norm_AHA = torch.sqrt(torch.tensor(1.0))\n",
    "        self.op_norm_GHG = torch.sqrt(torch.tensor(12.0))\n",
    "        # operator norm of K = [A, \\nabla]\n",
    "        # https://iopscience.iop.org/article/10.1088/0031-9155/57/10/3065/pdf,\n",
    "        # see page 3083\n",
    "        self.L = torch.sqrt(self.op_norm_AHA**2 + self.op_norm_GHG**2)\n",
    "\n",
    "        # function for projecting\n",
    "        self.ClipAct = ClipAct()\n",
    "\n",
    "        if mode == \"lambda_xyt\":\n",
    "            # one single lambda for x,y and t\n",
    "            self.lambda_reg = nn.Parameter(torch.tensor([-1.5]), requires_grad=True)\n",
    "\n",
    "        elif mode == \"lambda_xy_t\":\n",
    "            # one (shared) lambda for x,y and one lambda for t\n",
    "            self.lambda_reg = nn.Parameter(\n",
    "                torch.tensor([-4.5, -1.5]), requires_grad=True\n",
    "            )\n",
    "\n",
    "        elif mode == \"lambda_cnn\":\n",
    "            # the CNN-block to estimate the lambda regularization map\n",
    "            # must be a CNN yielding a two-channeld output, i.e.\n",
    "            # one map for lambda_cnn_xy and one map for lambda_cnn_t\n",
    "            self.cnn = cnn_block    # NOTE: This is actually the UNET!!! (At least in this project)\n",
    "            self.up_bound = torch.tensor(up_bound)\n",
    "\n",
    "        # number of terations\n",
    "        self.T = T\n",
    "        self.mode = mode\n",
    "\n",
    "        # constants depending on the operators\n",
    "        self.tau = nn.Parameter(\n",
    "            torch.tensor(10.0), requires_grad=True\n",
    "        )  # starting value approximately  1/L\n",
    "        self.sigma = nn.Parameter(\n",
    "            torch.tensor(10.0), requires_grad=True\n",
    "        )  # starting value approximately  1/L\n",
    "\n",
    "        # theta should be in \\in [0,1]\n",
    "        self.theta = nn.Parameter(\n",
    "            torch.tensor(10.0), requires_grad=True\n",
    "        )  # starting value approximately  1\n",
    "\n",
    "        # distinguish between training and test phase;\n",
    "        # during training, the input is padded using \"reflect\" padding, because\n",
    "        # patches are used by reducing the number of temporal points;\n",
    "        # while testing, \"reflect\" padding is used in x,y- direction, while\n",
    "        # circular padding is used in t-direction\n",
    "        self.phase = phase\n",
    "\n",
    "    def get_lambda_cnn(self, x):\n",
    "        # padding\n",
    "        # arbitrarily chosen, maybe better to choose it depending on the\n",
    "        # receptive field of the CNN or so;\n",
    "        # seems to be important in order not to create \"holes\" in the\n",
    "        # lambda_maps in t-direction\n",
    "        npad_xy = 4\n",
    "        # npad_t = 8\n",
    "        npad_t = 0 # TODO: Time dimension should not be necessary for single image input.\n",
    "        # I changed the npad_t to 0 so that I can run on single image input without change the 3D type config. It seems that the number of frames must be greater than npad_t?\n",
    "\n",
    "        pad = (npad_t, npad_t, npad_xy, npad_xy, npad_xy, npad_xy)\n",
    "\n",
    "        if self.phase == \"training\":\n",
    "            x = F.pad(x, pad, mode=\"reflect\")\n",
    "\n",
    "        elif self.phase == \"testing\":\n",
    "            pad_refl = (0, 0, npad_xy, npad_xy, npad_xy, npad_xy)\n",
    "            pad_circ = (npad_t, npad_t, 0, 0, 0, 0)\n",
    "\n",
    "            x = F.pad(x, pad_refl, mode=\"reflect\")\n",
    "            x = F.pad(x, pad_circ, mode=\"circular\")\n",
    "\n",
    "        # estimate parameter map\n",
    "        lambda_cnn = self.cnn(x) # NOTE: The cnn is actually the UNET block!!! (At least in this project)\n",
    "\n",
    "        # crop\n",
    "        neg_pad = tuple([-pad[k] for k in range(len(pad))])\n",
    "        lambda_cnn = F.pad(lambda_cnn, neg_pad)\n",
    "\n",
    "        # double spatial map and stack\n",
    "        lambda_cnn = torch.cat((lambda_cnn[:, 0, ...].unsqueeze(1), lambda_cnn), dim=1)\n",
    "\n",
    "        # constrain map to be striclty positive; further, bound it from below\n",
    "        if self.up_bound > 0:\n",
    "            # constrain map to be striclty positive; further, bound it from below\n",
    "            lambda_cnn = self.up_bound * self.op_norm_AHA * torch.sigmoid(lambda_cnn)\n",
    "        else:\n",
    "            lambda_cnn = 0.1 * self.op_norm_AHA * F.softplus(lambda_cnn)\n",
    "\n",
    "        del pad\n",
    "        del x\n",
    "        del neg_pad\n",
    "\n",
    "        return lambda_cnn\n",
    "\n",
    "    def forward(\n",
    "            self, x, lambda_map=None, \n",
    "            # lambda_reg_container=None,\n",
    "    ):\n",
    "        if lambda_map is None:\n",
    "            # estimate lambda reg from the image\n",
    "            lambda_reg = self.get_lambda_cnn(x)\n",
    "        else:\n",
    "            lambda_reg = lambda_map\n",
    "\n",
    "        # if lambda_reg_container is not None:\n",
    "        #     assert type(lambda_reg_container) == list, f\"lambda_reg_container should be a list, not {type(lambda_reg_container)}\"\n",
    "        #     lambda_reg_container.append(lambda_reg) # For comparison\n",
    "\n",
    "        x.to(DEVICE)\n",
    "        x1 = reconstruct_with_PDHG(x, lambda_reg, self.T)\n",
    "\n",
    "        del lambda_reg\n",
    "        del x\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Data loading class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "class DynamicImageStaticDenoisingDataset(Dataset):\n",
    "\t\n",
    "\tdef __init__(\n",
    "\t\tself, \n",
    "\t\tdata_path: str, \n",
    "\t\tids: list,\n",
    "\t\t# scale_factor = 0.5, \n",
    "\t\t# patches_size = None,\n",
    "\t\t# strides= None,\n",
    "\t\tresize_square = 120,\n",
    "\t\tsigma = (0.1, 0.5),  \n",
    "\t\tdevice: str = \"cuda\"\n",
    "\t):\n",
    "\t\tself.device = device\n",
    "\t\t# self.scale_factor = scale_factor\n",
    "\t\tself.resize_square = resize_square\n",
    "\n",
    "\t\txray_images = load_images_chest_xray(data_path, ids)\n",
    "\n",
    "\t\txf_list = []\n",
    "\t\tfor image in xray_images:\n",
    "\t\t\timage = crop_to_square_and_resize(image, self.resize_square)\n",
    "\t\t\timage = image.convert('L') #convert to grey_scale\n",
    "\t\t\timage_data = np.asarray(image)\n",
    "\t\t\txf = torch.tensor(image_data, dtype=torch.float)\n",
    "\t\t\t# Assume image is in [0, 255] range\n",
    "\t\t\txf = xf / 255\n",
    "\t\t\tassert len(xf.size()) == 2, f\"Expected 2D tensor, got {xf.size()}\"\n",
    "\t\t\txf = xf.unsqueeze(0) # Add channel dimension\n",
    "\t\t\txf = xf.unsqueeze(-1) # Add time dimension. TODO: For legacy dynamic image code only. Will remove later.\n",
    "\t\t\txf_list.append(xf)\n",
    "\t\txf = torch.stack(xf_list, dim=0) # will have shape (mb, 1, Nx, Ny, Nt), where mb denotes the number of patches\n",
    "\t\tassert len(xf.size()) == 5, f\"Expected 5D tensor, got {xf.size()}\"\n",
    "\t\tassert xf.size(1) == 1, f\"Expected 1 channel, got {xf.size(1)}\"\n",
    "\t\tassert xf.size(2) == self.resize_square, f\"Expected width (Nx) of {self.resize_square}, got {xf.size(-3)}\"\n",
    "\t\tassert xf.size(3) == self.resize_square, f\"Expected height (Ny) of {self.resize_square}, got {xf.size(-2)}\"\n",
    "\t\tassert xf.size(4) == 1, f\"Expected 1 time step, got {xf.size(-1)}\"\n",
    "\n",
    "\t\t#create temporal TV vector to detect which patches contain the most motion\n",
    "\t\txf_patches_tv = (xf[...,1:] - xf[...,:-1]).pow(2).sum(dim=[1,2,3,4]) #contains the TV for all patches\n",
    "\t\t\n",
    "\t\t#normalize to 1 to have a probability vector\n",
    "\t\txf_patches_tv /= torch.sum(xf_patches_tv)\n",
    "\t\t\n",
    "\t\t#sort TV in descending order --> xfp_tv_ids[0] is the index of the patch with the most motion\n",
    "\t\tself.samples_weights = xf_patches_tv\n",
    "\n",
    "\t\t# # TODO: Investigate\n",
    "\t\t# # Change the values in samples_weights to be a range of integers from 0 to len(samples_weights)\n",
    "\t\t# # Unless I do this, when I run on a set of identical images, it will give me an error:\n",
    "\t\t# # RuntimeError: invalid multinomial distribution (encountering probability entry < 0)\n",
    "\t\t# self.samples_weights = torch.arange(len(self.samples_weights))\n",
    "\t\t\n",
    "\t\tself.xf = xf\n",
    "\t\tself.len = xf.shape[0]\n",
    "\t\t\n",
    "\t\tself.sigma_min = sigma[0]\n",
    "\t\tself.sigma_max = sigma[1]\n",
    "\t\t\n",
    "\t\t\t\n",
    "\tdef __getitem__(self, index):\n",
    "\n",
    "\t\tsigma = self.sigma_min + torch.rand(1) * ( self.sigma_max - self.sigma_min )\n",
    "\n",
    "\t\tx_noise = add_noise(self.xf[index], sigma)\n",
    "\n",
    "\t\tdel sigma\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\tx_noise.to(device=self.device),\n",
    "   \t\t\tself.xf[index].to(device=self.device)\n",
    "        )\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### UNET\n",
    "\n",
    "The specific UNET architecture we use has the following parts:\n",
    "\n",
    "...\n",
    "\n",
    "We use Leaky RELU instead of RELU or Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_parts.py as a reference\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels: int, out_channels: int, n_dimensions=3, activation=\"LeakyReLU\"):\n",
    "        super(DoubleConv, self).__init__()\n",
    "\n",
    "        def get_conv(in_channels, out_channels):\n",
    "            # 1-dimensional convolution is not supported\n",
    "            if n_dimensions == 3:\n",
    "                return nn.Conv3d(in_channels, out_channels, kernel_size=(3, 3, 1), padding=(1, 1, 0))\n",
    "            elif n_dimensions == 2:\n",
    "                return nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported number of dimensions: {n_dimensions}\")\n",
    "\n",
    "        def get_activation():\n",
    "            if activation == \"LeakyReLU\":\n",
    "                return nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "            elif activation == \"ReLU\":\n",
    "                return nn.ReLU(inplace=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            get_conv(in_channels, out_channels), get_activation(),\n",
    "            get_conv(out_channels, out_channels), get_activation())\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv_block(x)\n",
    "        \n",
    "\n",
    "class EncodeBlock3d(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels: int, n_dimensions=3,\n",
    "            activation=\"LeakyReLU\",\n",
    "            downsampling_kernel=(2, 2, 1), downsampling_mode=\"max\"):\n",
    "        super(EncodeBlock3d, self).__init__()\n",
    "\n",
    "        len = downsampling_kernel[0] # Assume kernel has shape (len, len, 1)\n",
    "        assert downsampling_kernel == (len, len, 1), f\"Expected a flat square kernel like {(len, len, 1)}, got {downsampling_kernel}\"\n",
    "        stride = (2, 2, 1) # Stride 2x2 to halve each side \n",
    "        padding = ((len-1)//2, (len-1)//2, 0) # Padding (len-1) // 2 to exactly halve each side \n",
    "        if downsampling_mode == \"max\":\n",
    "            self.pool = nn.MaxPool3d(\n",
    "                kernel_size=downsampling_kernel, stride=stride, padding=padding)\n",
    "        elif downsampling_mode == \"avg\":\n",
    "            self.pool = nn.AvgPool3d(\n",
    "                kernel_size=downsampling_kernel, stride=stride, padding=padding)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling method: {downsampling_mode}\")\n",
    "\n",
    "        self.double_conv = DoubleConv(in_channels, in_channels * 2, n_dimensions, activation=activation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.pool(x)\n",
    "        x = self.double_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class DecodeBlock3d(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels: int, n_dimensions=3, \n",
    "            activation=\"LeakyReLU\",\n",
    "            upsampling_kernel=(2, 2, 1), upsampling_mode=\"linear_interpolation\"):\n",
    "        super(DecodeBlock3d, self).__init__()\n",
    "\n",
    "        if upsampling_mode == \"linear_interpolation\":\n",
    "            self.upsampling = nn.Sequential(\n",
    "                nn.Upsample(\n",
    "                    scale_factor=(2, 2, 1), # Assume the shape is (Nx, Ny, 1) where Nx is the image width and Ny is the image height.\n",
    "                    mode='trilinear', align_corners=True), # What difference does it make in the end if align_corners is True or False? Preserving symmetry?\n",
    "                # 1x1x1 convolution to reduce the number of channels while keeping the size the same\n",
    "                nn.Conv3d(\n",
    "                    in_channels, in_channels // 2, \n",
    "                    kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0))\n",
    "            )\n",
    "        elif upsampling_mode == \"transposed_convolution\":  \n",
    "            len = upsampling_kernel[0] # Assume kernel has shape (len, len, 1)\n",
    "            assert upsampling_kernel == (len, len, 1), f\"Expected a flat square kernel like {(len, len, 1)}, got {upsampling_kernel}\"\n",
    "            stride = (2, 2, 1) # Stride 2x2 to double each side \n",
    "            padding = ((len-1)//2, (len-1)//2, 0) # Padding (len-1) // 2 to exactly double each side    \n",
    "            self.upsampling = nn.ConvTranspose3d(\n",
    "                in_channels, in_channels // 2, \n",
    "                kernel_size=upsampling_kernel, stride=stride, padding=padding, \n",
    "                output_padding=padding # TODO: Should this be the same as padding?\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported upsampling method: {upsampling_mode}\")\n",
    "        \n",
    "        self.double_conv = DoubleConv(in_channels, in_channels // 2, n_dimensions, activation=activation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_encoder_output: torch.Tensor):\n",
    "        x = self.upsampling(x)\n",
    "        x = torch.cat([x_encoder_output, x], dim=1)   # skip-connection. No cropping since we ensure that the size is the same.\n",
    "        x = self.double_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class UNet3d(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=1, out_channels=2, init_filters=32, n_blocks=3,\n",
    "            activation=\"LeakyReLU\",\n",
    "            downsampling_kernel=(2, 2, 1), downsampling_mode=\"max\",\n",
    "            upsampling_kernel=(2, 2, 1), upsampling_mode=\"linear_interpolation\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Assume that input is 5D tensor of shape (batch_size, channels, Nx, Ny, Nt)\n",
    "        where Nx is the image width and Ny is the image height.\n",
    "        Assume that batch_size = 1, channels = 1, Nx = Ny (square image), Nt = 1 (static image).\n",
    "        NOTE: The convention used in pytorch documentation is (batch_size, channels, Nt, Ny, Nx).\n",
    "        \"channels\" is equivalent to the number of filters or features.\n",
    "\n",
    "        Our paper (figure 2):\n",
    "            - in_channels = 1\n",
    "            - out_channels = 2\n",
    "            - init_filters = 32\n",
    "            - n_blocks = 3\n",
    "            - pooling: max pooling 2x2\n",
    "            - pool padding = 1\n",
    "                - 1 padding will keep the size of the \"image\" the same after each convolution. The skip-connection will NOT crop the encoder's output.\n",
    "            - upsampling kernel: 2x2 ?\n",
    "            - up_mode: linear interpolation\n",
    "\n",
    "        U-Net paper (2015, Olaf Ronneberger https://arxiv.org/abs/1505.04597):\n",
    "            - in_channels = 1\n",
    "            - out_channels = 2\n",
    "            - init_filters = 64\n",
    "            - n_blocks = 4\n",
    "            - pooling: max pooling 2x2\n",
    "            - pool padding = 0\n",
    "                - 0 padding will reduce the size of the \"image\" by 2 in each dimension after each convolution. The skip-connection will have to crop the encoder's output to match the decoder's input.\n",
    "            - upsampling kernel: 2x2\n",
    "            - up_mode: ? (linear interpolation or transposed convolution)\n",
    "        \"\"\"\n",
    "        super(UNet3d, self).__init__()\n",
    "        \n",
    "        self.c0x0 = DoubleConv( # TODO: Find a better name\n",
    "            in_channels=in_channels, \n",
    "            out_channels=init_filters,\n",
    "            activation=activation\n",
    "        )\n",
    "        self.encoder = nn.ModuleList([\n",
    "            EncodeBlock3d(\n",
    "                in_channels=init_filters * 2**i,\n",
    "                activation=activation,\n",
    "                downsampling_kernel=downsampling_kernel,\n",
    "                downsampling_mode=downsampling_mode\n",
    "            ) for i in range(n_blocks)\n",
    "        ])\n",
    "        self.decoder = nn.ModuleList([\n",
    "            DecodeBlock3d(\n",
    "                in_channels=init_filters * 2**(n_blocks-i),\n",
    "                activation=activation, \n",
    "                upsampling_kernel=upsampling_kernel,\n",
    "                upsampling_mode=upsampling_mode\n",
    "            ) for i in range(n_blocks)\n",
    "        ])\n",
    "        # 1x1x1 convo\n",
    "        self.c1x1 = nn.Conv3d( # TODO: Find a better name\n",
    "            in_channels=init_filters,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Assume that x is 5D tensor of shape (batch_size, channels, Nx, Ny, Nt)\n",
    "        # where Nx is the image width and Ny is the image height.\n",
    "        # Aslo assume that batch_size = 1, channels = 1, Nx = Ny (square image), Nt = 1 (static image).\n",
    "        # NOTE: The convention used in pytorch documentation is (batch_size, channels, Nt, Ny, Nx).\n",
    "        assert len(x.size()) == 5, f\"Expected 5D tensor, got {x.size()}\"\n",
    "        batch_size, channels, Nx, Ny, Nt = x.size()\n",
    "        assert channels == 1, f\"Expected 1 channel, got {channels}\" # TODO: Allow multiple channels (colour images)\n",
    "        assert Nx == Ny, f\"Expected square image, got ({Nx}, {Ny})\" # TODO: Allow non-square images\n",
    "        assert Nt == 1, f\"Expected 1 time step, got {Nt}\" # TODO: Allow multiple time steps (dynamic images, video)\n",
    "        assert batch_size == 1, f\"Expected batch size 1, got {batch_size}\" # TODO: Might train with larger batch size\n",
    "\n",
    "        n_blocks = len(self.encoder)\n",
    "        assert Nx >= 2**n_blocks, f\"Expected width (Nx) of at least {2**n_blocks}, got {Nx}\"\n",
    "        assert Ny >= 2**n_blocks, f\"Expected height (Ny) of at least {2**n_blocks}, got {Ny}\"\n",
    "\n",
    "        x = self.c0x0(x)\n",
    "\n",
    "        encoder_outputs = []\n",
    "        for i, enc_block in enumerate(self.encoder):\n",
    "            encoder_outputs.append(x)\n",
    "            x = enc_block(x)\n",
    "        for i, dec_block in enumerate(self.decoder):\n",
    "            x = dec_block(x, encoder_outputs[-i-1]) # skip-connection inside\n",
    "            \n",
    "        x = self.c1x1(x)\n",
    "\n",
    "        for enc_output in encoder_outputs:\n",
    "            del enc_output\n",
    "        del encoder_outputs\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_and_clear_cuda(expected, actual):\n",
    "    try:\n",
    "        assert expected == actual\n",
    "    except AssertionError:\n",
    "        print(f\"!!! ERROR !!! Expected: {expected}, got {actual}\")\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "def test_unet_3d():  \n",
    "    if DISABLING_TESTS: return  \n",
    "    input_tensor = torch.randn(1, 1, 512, 512, 1)  # batch size of 1, 1 channel, 512x512x1 volume\n",
    "    \n",
    "    config = get_config()\n",
    "\n",
    "    # Example usage\n",
    "    model = UNet3d(\n",
    "        init_filters=32,\n",
    "        n_blocks=config[\"n_blocks\"],\n",
    "        activation=\"ReLU\",\n",
    "        downsampling_kernel=(2, 2, 1),\n",
    "        downsampling_mode=config[\"downsampling_mode\"],\n",
    "        upsampling_kernel=(2, 2, 1),\n",
    "        upsampling_mode=config[\"upsampling_mode\"],\n",
    "    )\n",
    "    output = model(input_tensor)\n",
    "    print(f\"UNet output shape: {output.shape}\")\n",
    "    assert_and_clear_cuda((1, 2, 512, 512, 1), output.shape)\n",
    "\n",
    "\n",
    "    conv_3d = nn.Conv3d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "    conv_3d_output = conv_3d(input_tensor)\n",
    "    print(f\"Conv3d output shape: {conv_3d_output.shape}\")\n",
    "    assert_and_clear_cuda((1, 64, 512, 512, 1), conv_3d_output.shape)\n",
    "\n",
    "\n",
    "    double_conv_3d = DoubleConv(64, 128)\n",
    "    double_conv_output = double_conv_3d(conv_3d_output)\n",
    "    print(f\"{DoubleConv.__name__} output shape: {double_conv_output.shape}\")\n",
    "    assert_and_clear_cuda((1, 128, 512, 512, 1), double_conv_output.shape)\n",
    "\n",
    "\n",
    "    max_3d = nn.MaxPool3d((3, 3, 1), stride=(2, 2, 1), padding=(1, 1, 0))\n",
    "    max_3d_output_1 = max_3d(input_tensor)\n",
    "    print(f\"MaxPool3d output 1 shape: {max_3d_output_1.shape}\")\n",
    "    assert_and_clear_cuda((1, 1, 256, 256, 1), max_3d_output_1.shape)\n",
    "\n",
    "    max_3d_input = torch.randn(1, 128, 512, 512, 1)\n",
    "    max_3d_output_2 = max_3d(max_3d_input)\n",
    "    print(f\"MaxPool3d output 2 shape: {max_3d_output_2.shape}\")\n",
    "    assert_and_clear_cuda((1, 128, 256, 256, 1), max_3d_output_2.shape)\n",
    "\n",
    "    conv_transpose_3d = nn.ConvTranspose3d(\n",
    "        1024, 512, \n",
    "        kernel_size=(3, 3, 1), \n",
    "        stride=(2, 2, 1), \n",
    "        padding=(1, 1, 0), \n",
    "        output_padding=(1, 1, 0)\n",
    "    )\n",
    "    conv_transpose_3d_input = torch.randn(1, 1024, 32, 32, 1)\n",
    "    conv_transpose_3d_output = conv_transpose_3d(conv_transpose_3d_input)\n",
    "    print(f\"ConvTranspose3d output shape: {conv_transpose_3d_output.shape}\")\n",
    "    assert_and_clear_cuda((1, 512, 64, 64, 1), conv_transpose_3d_output.shape)\n",
    "\n",
    "\n",
    "    up_sample = nn.Upsample(\n",
    "        scale_factor=(2, 2, 1), \n",
    "        mode='trilinear', align_corners=True) # What difference does it make if align_corners is True or False?\n",
    "    up_sample_output = up_sample(input_tensor)\n",
    "    print(f\"Upsample output shape: {up_sample_output.shape}\")\n",
    "    assert_and_clear_cuda((1, 1, 1024, 1024, 1), up_sample_output.shape)\n",
    "                    \n",
    "\n",
    "    # # print(f\"\\n{model}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # # Delete the model and the output tensor\n",
    "    # del model\n",
    "    # del output\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "test_unet_3d()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from ...\n",
    "\n",
    "def get_datasets(config):\n",
    "    min_sigma = config[\"min_sigma\"]\n",
    "    max_sigma = config[\"max_sigma\"]\n",
    "    resize_square = config[\"resize_square\"]\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    train_num_samples = config[\"train_num_samples\"]\n",
    "    train_ids = list(range(0, train_num_samples))\n",
    "    dataset_train = DynamicImageStaticDenoisingDataset(\n",
    "        data_path=config[\"train_data_path\"],\n",
    "        ids=train_ids,\n",
    "        sigma=(min_sigma, max_sigma),\n",
    "        resize_square=resize_square,\n",
    "        # strides=[120, 120, 1],\n",
    "        # patches_size=[120, 120, 1],\n",
    "        # strides=[256, 256, 1], # stride < patch will allow overlapping patches, maybe good to blend the patches?\n",
    "        # strides=[512, 512, 1],\n",
    "        # patches_size=[512, 512, 1],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    val_num_samples = config[\"val_num_samples\"]\n",
    "    val_ids = list(range(0, val_num_samples))\n",
    "    dataset_valid = DynamicImageStaticDenoisingDataset(\n",
    "        data_path=config[\"val_data_path\"],\n",
    "        ids=val_ids,\n",
    "        sigma=(min_sigma, max_sigma),\n",
    "        resize_square=resize_square,\n",
    "        # strides=[120, 120, 1],\n",
    "        # patches_size=[120, 120, 1],\n",
    "        # strides=[256, 256, 1], # stride < patch will allow overlapping patches, maybe good to blend the patches?\n",
    "        # strides=[512, 512, 1],\n",
    "        # patches_size=[512, 512, 1],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    test_num_samples = config[\"test_num_samples\"]\n",
    "    test_ids = list(range(0, test_num_samples))\n",
    "    dataset_test = DynamicImageStaticDenoisingDataset(\n",
    "        data_path=config[\"test_data_path\"],\n",
    "        ids=test_ids,\n",
    "        sigma=(min_sigma, max_sigma),\n",
    "        resize_square=resize_square,\n",
    "        # strides=[120, 120, 1],\n",
    "        # patches_size=[120, 120, 1],\n",
    "        # strides=[256, 256, 1], # stride < patch will allow overlapping patches, maybe good to blend the patches?\n",
    "        # strides=[512, 512, 1],\n",
    "        # patches_size=[512, 512, 1],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(f\"Number of training samples: {len(dataset_train)}\")\n",
    "    print(f\"Number of validation samples: {len(dataset_valid)}\")\n",
    "    print(f\"Number of test samples: {len(dataset_test)}\")\n",
    "\n",
    "    return dataset_train, dataset_valid, dataset_test\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloaders(config):\n",
    "\n",
    "    dataset_train, dataset_valid, dataset_test = get_datasets(config)\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    device = config[\"device\"]\n",
    "    random_seed = config[\"random_seed\"]\n",
    "\n",
    "    # Create training dataloader\n",
    "    # train_sampler = WeightedRandomSampler(dataset_train.samples_weights, len(dataset_train.samples_weights))\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=batch_size, \n",
    "        # sampler=train_sampler,\n",
    "        generator=torch.Generator(device=device).manual_seed(random_seed),\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Create validation dataloader \n",
    "    # val_sampler = WeightedRandomSampler(dataset_valid.samples_weights, len(dataset_valid.samples_weights))\n",
    "    dataloader_valid = torch.utils.data.DataLoader(\n",
    "        dataset_valid, batch_size=batch_size, \n",
    "        # sampler=val_sampler,\n",
    "        generator=torch.Generator(device=device).manual_seed(random_seed),\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # Create test dataloader\n",
    "    test_sampler = WeightedRandomSampler(dataset_test.samples_weights, len(dataset_test.samples_weights))\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=batch_size, \n",
    "        # sampler=test_sampler,\n",
    "        generator=torch.Generator(device=device).manual_seed(random_seed),\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        dataloader_train, \n",
    "        dataloader_valid, \n",
    "        dataloader_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataloader():\n",
    "    if DISABLING_TESTS: return\n",
    "    dataloader_train, dataloader_valid, dataloader_test = get_dataloaders(get_config())\n",
    "    for i, (x, y) in enumerate(dataloader_train):\n",
    "        print(f\"Batch {i+1}\")\n",
    "        print(f\"x size: {x.size()}\")\n",
    "        print(f\"y size: {y.size()}\")\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(x.squeeze(0).squeeze(0).squeeze(-1).to(\"cpu\"), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(y.squeeze(0).squeeze(0).squeeze(-1).to(\"cpu\"), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show();\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "    del dataloader_train\n",
    "    del dataloader_valid\n",
    "    del dataloader_test\n",
    "\n",
    "test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "# def train_iteration(optimizer, model, loss_func, sample):\n",
    "#     optimizer.zero_grad(set_to_none=True)  # Zero your gradients for every batch!\n",
    "#     noisy_image, clean_image = sample\n",
    "#     denoised_image = model(noisy_image)\n",
    "#     loss = loss_func(denoised_image, clean_image)\n",
    "#     loss.backward()\n",
    "    \n",
    "#     if loss.item() != loss.item():\n",
    "#         raise ValueError(\"NaN returned by loss function...\")\n",
    "\n",
    "#     optimizer.step()\n",
    "\n",
    "#     denoised_image = denoised_image.squeeze(0).squeeze(0).squeeze(-1)\n",
    "#     clean_image = clean_image.squeeze(0).squeeze(0).squeeze(-1)\n",
    "\n",
    "#     psnr = PSNR(denoised_image, clean_image)\n",
    "#     ssim = SSIM(denoised_image, clean_image)\n",
    "\n",
    "#     return loss.item(), psnr, ssim\n",
    "\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, loss_func) -> float:\n",
    "    running_loss = 0.\n",
    "    running_psnr = 0.\n",
    "    running_ssim = 0.\n",
    "    num_batches = len(data_loader)\n",
    "    # for sample in tqdm(data_loader): # tqdm helps show a nice progress bar\n",
    "    for sample in data_loader:\n",
    "        # loss, psnr, ssim = train_iteration(optimizer, model, loss_func, sample)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)  # Zero your gradients for every batch! TODO: Why?\n",
    "        noisy_image_5d, clean_image_5d = sample\n",
    "        denoised_image_5d = model(noisy_image_5d)\n",
    "        loss = loss_func(denoised_image_5d, clean_image_5d)\n",
    "\n",
    "        loss.backward()\n",
    "        if loss.item() != loss.item():\n",
    "            raise ValueError(\"NaN returned by loss function...\")\n",
    "        optimizer.step()\n",
    "\n",
    "        denoised_image_2d = denoised_image_5d.squeeze(0).squeeze(0).squeeze(-1)\n",
    "        clean_image_2d = clean_image_5d.squeeze(0).squeeze(0).squeeze(-1)\n",
    "        psnr = PSNR(denoised_image_2d, clean_image_2d)\n",
    "        ssim = SSIM(denoised_image_2d, clean_image_2d)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_psnr += psnr\n",
    "        running_ssim += ssim\n",
    "\n",
    "        # Free up memory\n",
    "        del loss \n",
    "        del denoised_image_5d # Delete output of model\n",
    "        del denoised_image_2d # Delete auxiliary variable\n",
    "        del clean_image_2d # Delete auxiliary variable\n",
    "        del noisy_image_5d # Noisy image is generated each time so can delete it\n",
    "        del clean_image_5d # TODO: Is this a copy that I can delete or a reference to the original?\n",
    "\n",
    "    avg_loss = running_loss / num_batches\n",
    "    avg_psnr = running_psnr / num_batches\n",
    "    avg_ssim = running_ssim / num_batches\n",
    "\n",
    "    del running_loss\n",
    "    del running_psnr\n",
    "    del running_ssim\n",
    "    del num_batches\n",
    "\n",
    "    return avg_loss, avg_psnr, avg_ssim\n",
    "\n",
    "\n",
    "# def validate_iteration(model, loss_func, sample):\n",
    "#     noisy_image, clean_image = sample\n",
    "#     denoised_image = model(noisy_image)\n",
    "#     loss = loss_func(denoised_image, clean_image)\n",
    "#     denoised_image = denoised_image.squeeze(0).squeeze(0).squeeze(-1)\n",
    "#     clean_image = clean_image.squeeze(0).squeeze(0).squeeze(-1)\n",
    "\n",
    "#     psnr = PSNR(denoised_image, clean_image)\n",
    "#     ssim = SSIM(denoised_image, clean_image)\n",
    "\n",
    "#     return loss.item(), psnr, ssim\n",
    "\n",
    "\n",
    "def validate_epoch(model, data_loader, loss_func) -> float:\n",
    "    running_loss = 0.\n",
    "    running_psnr = 0.\n",
    "    running_ssim = 0.\n",
    "    num_batches = len(data_loader)\n",
    "    # for sample in tqdm(data_loader): # tqdm helps show a nice progress bar\n",
    "    for sample in data_loader:\n",
    "        # loss, psnr, ssim = validate_iteration(model, loss_func, sample)\n",
    "\n",
    "        noisy_image_5d, clean_image_5d = sample\n",
    "        denoised_image_5d = model(noisy_image_5d)\n",
    "        loss = loss_func(denoised_image_5d, clean_image_5d)\n",
    "\n",
    "        denoised_image_2d = denoised_image_5d.squeeze(0).squeeze(0).squeeze(-1)\n",
    "        clean_image_2d = clean_image_5d.squeeze(0).squeeze(0).squeeze(-1)\n",
    "        psnr = PSNR(denoised_image_2d, clean_image_2d)\n",
    "        ssim = SSIM(denoised_image_2d, clean_image_2d)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_psnr += psnr\n",
    "        running_ssim += ssim\n",
    "\n",
    "        # Free up memory\n",
    "        del loss \n",
    "        del denoised_image_5d # Delete output of model\n",
    "        del denoised_image_2d # Delete auxiliary variable\n",
    "        del clean_image_2d # Delete auxiliary variable\n",
    "        del noisy_image_5d # Noisy image is generated each time so can delete it\n",
    "        del clean_image_5d # TODO: Is this a copy that I can delete or a reference to the original?\n",
    "\n",
    "    avg_loss = running_loss / num_batches\n",
    "    avg_psnr = running_psnr / num_batches\n",
    "    avg_ssim = running_ssim / num_batches\n",
    "\n",
    "    del running_loss\n",
    "    del running_psnr\n",
    "    del running_ssim\n",
    "    del num_batches\n",
    "\n",
    "    return avg_loss, avg_psnr, avg_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image_tensor_2D, image_name, folder_name):\n",
    "    image_tensor_2D = torch.clamp(image_tensor_2D, 0, 1) # Clip the values to 0 and 1\n",
    "    image_numpy = image_tensor_2D.to(\"cpu\").detach().numpy()\n",
    "    image_numpy_256 = image_numpy * 255\n",
    "    image_numpy_256_uint8 = image_numpy_256.astype(np.uint8)\n",
    "    image_to_save = Image.fromarray(image_numpy_256_uint8)  # TODO: Is there a shorter way to do this, similar to .convert(\"L\")?\n",
    "    image_to_save.save(f\"{folder_name}/{image_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_testcase():\n",
    "    CHEST_XRAY_BASE_DATA_PATH = \"../data/chest_xray\"\n",
    "    image = Image.open(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL/IM-0115-0001.jpeg\")\n",
    "    image = crop_to_square_and_resize(image, 512)\n",
    "\n",
    "    folder_name = \"testcases_tmp\"\n",
    "\n",
    "    image_name = \"chest_xray_clean\"\n",
    "    image.save(f\"{folder_name}/{image_name}.png\") \n",
    "\n",
    "    # Add noise to the image\n",
    "    noisy_image_tensor_4D = add_noise(convert_to_tensor_4D(convert_to_numpy(image)), sigma=0.5)\n",
    "    noisy_image_tensor_2D = noisy_image_tensor_4D.squeeze(0).squeeze(-1)\n",
    "    noisy_image_name = \"chest_xray_noisy\"\n",
    "    save_image(noisy_image_tensor_2D, \"chest_xray_noisy\", folder_name)\n",
    "\n",
    "    # Read the saved images\n",
    "    clean_image = Image.open(f\"{folder_name}/{image_name}.png\")\n",
    "    noisy_image = Image.open(f\"{folder_name}/{noisy_image_name}.png\")\n",
    "    \n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(clean_image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Clean Image\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(noisy_image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Noisy Image\")\n",
    "    plt.show();\n",
    "\n",
    "# make_testcase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Use wandb to log the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Use wandb to log the training process\n",
    "# !wandb login\n",
    "def init_wandb(config):\n",
    "    project_name = config[\"project\"]\n",
    "    os.environ['WANDB_NOTEBOOK_NAME'] = project_name\n",
    "    os.environ['WANDB_MODE'] = config[\"wandb_mode\"] # https://docs.wandb.ai/quickstart\n",
    "    wandb.login()\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=project_name,\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config=get_config(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def temp_log_to_files():\n",
    "#     model_states_dir = \"tmp_2/model-chest_xray-2024_06_06_05_51_27\"\n",
    "#     config = get_config()\n",
    "#     with open(f\"{model_states_dir}/config.json\", \"w\") as f:\n",
    "#         json.dump(config, f, indent=4)\n",
    "#     with open(f\"{model_states_dir}/config.yaml\", \"w\") as f:\n",
    "#         yaml.dump(config, f)\n",
    "#     with open(f\"{model_states_dir}/config.txt\", \"w\") as f:\n",
    "#         f.write(str(config))\n",
    "\n",
    "# def test_temp_log_to_files():\n",
    "#     temp_log_to_files()\n",
    "\n",
    "# test_temp_log_to_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "def start_training(config, pretrained_model_path=None, is_state_dict=False, start_epoch=0):\n",
    "    \n",
    "    dataloader_train, dataloader_valid, dataloader_test = get_dataloaders(config)\n",
    "\n",
    "    del dataloader_test # Not used for now\n",
    "\n",
    "    if pretrained_model_path is None or is_state_dict:\n",
    "        # Define CNN block\n",
    "        unet = UNet3d(\n",
    "            in_channels=config[\"in_channels\"],\n",
    "            out_channels=config[\"out_channels\"],\n",
    "            init_filters=config[\"init_filters\"],\n",
    "            n_blocks=config[\"n_blocks\"],\n",
    "            activation=config[\"activation\"],\n",
    "            downsampling_kernel=config[\"downsampling_kernel\"],\n",
    "            downsampling_mode=config[\"downsampling_mode\"],\n",
    "            upsampling_kernel=config[\"upsampling_kernel\"],\n",
    "            upsampling_mode=config[\"upsampling_mode\"],\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Construct primal-dual operator with nn\n",
    "        pdhg = DynamicImageStaticPrimalDualNN(\n",
    "            cnn_block=unet, \n",
    "            T=config[\"T\"],\n",
    "            phase=\"training\",\n",
    "            up_bound=config[\"up_bound\"],\n",
    "        ).to(DEVICE)\n",
    "        if is_state_dict:\n",
    "            pdhg.load_state_dict(torch.load(f\"{model_states_dir}/{pretrained_model_path}.pt\"))\n",
    "    else:\n",
    "        pdhg = torch.load(f\"{model_states_dir}/{pretrained_model_path}.pt\")\n",
    "\n",
    "    pdhg.train(True)\n",
    "\n",
    "    # TODO: Sometimes, creating the optimizer gives this error:\n",
    "    #   AttributeError: partially initialized module 'torch._dynamo' has no attribute 'trace_rules' (most likely due to a circular import)\n",
    "    optimizer = torch.optim.Adam(pdhg.parameters(), lr=config[\"learning_rate\"])\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "\n",
    "    num_epochs = config[\"epochs\"]\n",
    "\n",
    "    save_epoch_local = config[\"save_epoch_local\"]\n",
    "    save_epoch_wandb = config[\"save_epoch_wandb\"]\n",
    "\n",
    "    time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    project = config[\"project\"]\n",
    "    model_name = f\"model-{project}-{time}\"\n",
    "\n",
    "    # Prepare to save the model\n",
    "    save_dir = config[\"save_dir\"]\n",
    "    model_states_dir = f\"{save_dir}/{model_name}\"\n",
    "\n",
    "    os.makedirs(model_states_dir, exist_ok=True)\n",
    "\n",
    "    def log_to_files():\n",
    "        with open(f\"{model_states_dir}/config.json\", \"w\") as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "        with open(f\"{model_states_dir}/config.yaml\", \"w\") as f:\n",
    "            yaml.dump(config, f)\n",
    "        with open(f\"{model_states_dir}/config.txt\", \"w\") as f:\n",
    "            f.write(str(config))\n",
    "        with open(f\"{model_states_dir}/unet.txt\", \"w\") as f:\n",
    "            f.write(str(unet))\n",
    "        with open(f\"{model_states_dir}/pdhg_net.txt\", \"w\") as f:\n",
    "            f.write(str(pdhg))\n",
    "\n",
    "        def log_data(dataloader, stage):\n",
    "            dataset = dataloader.dataset\n",
    "            with open(f\"{model_states_dir}/dataloader_{stage}.txt\", \"w\") as f:\n",
    "                f.write(f\"Batch size: {dataloader.batch_size}\\n\\n\")\n",
    "                f.write(f\"Number of batches: {len(dataloader)}\\n\\n\")\n",
    "                f.write(f\"Number of samples: {len(dataset)}\\n\\n\")\n",
    "                f.write(f\"Samples weights:\\n{str(dataset.samples_weights)}\\n\\n\")\n",
    "                f.write(f\"Sample 0 size:\\n{str(len(dataset[0]))}  {str(dataset[0][0].size())}\\n\\n\")\n",
    "                f.write(f\"Sample 0:\\n{str(dataset[0])}\\n\\n\")\n",
    "        log_data(dataloader_train, \"train\")\n",
    "        log_data(dataloader_valid, \"val\")\n",
    "        # log_data(dataloader_test, \"test\")\n",
    "\n",
    "    log_to_files()\n",
    "\n",
    "    # noisy_image_path = \"./testcases/chest_xray_noisy.png\"\n",
    "    # clean_image_path = \"./testcases/chest_xray_clean.png\"\n",
    "\n",
    "    # def get_image(image_path):\n",
    "    #     image = Image.open(image_path)\n",
    "    #     image = image.convert(\"L\")\n",
    "    #     image_data = np.asarray(image)\n",
    "    #     image_data = convert_to_tensor_4D(image_data)\n",
    "    #     image_data = image_data.unsqueeze(0).to(DEVICE)\n",
    "    #     return image_data\n",
    "\n",
    "    # noisy_image_data = get_image(noisy_image_path)\n",
    "    # clean_image_data = get_image(clean_image_path)\n",
    "\n",
    "    # dataset_train = MyDataset(noisy_image_path, clean_image_path)\n",
    "    # dataset_valid = MyDataset(noisy_image_path, clean_image_path)\n",
    "\n",
    "    # dataloader_train = torch.utils.data.DataLoader(\n",
    "    #     dataset_train, batch_size=1, \n",
    "    #     generator=torch.Generator(device=DEVICE),\n",
    "    #     shuffle=True)\n",
    "    # dataloader_valid = torch.utils.data.DataLoader(\n",
    "    #     dataset_valid, batch_size=1, \n",
    "    #     generator=torch.Generator(device=DEVICE),\n",
    "    #     shuffle=False)\n",
    "\n",
    "\n",
    "    init_wandb(config)\n",
    "\n",
    "    # for epoch in range(start_epoch, num_epochs):\n",
    "    for epoch in tqdm(range(start_epoch, num_epochs)):\n",
    "\n",
    "        # Model training\n",
    "        pdhg.train(True)\n",
    "        training_loss, training_psnr, training_ssim = train_epoch(pdhg, dataloader_train, optimizer, loss_function)\n",
    "        # training_loss, training_psnr, training_ssim = train_iteration(optimizer, pdhg, loss_function, sample=(noisy_image_data, clean_image_data))\n",
    "        # print(f\"Epoch {epoch+1} - TRAINING LOSS: {training_loss} - TRAINING PSNR: {training_psnr} - TRAINING SSIM: {training_ssim}\")\n",
    "\n",
    "        # Optional: Use wandb to log training progress\n",
    "        wandb.log({\"training_loss\": training_loss})\n",
    "        wandb.log({\"training PSNR\": training_psnr})\n",
    "        wandb.log({\"training SSIM\": training_ssim})\n",
    "\n",
    "        del training_loss\n",
    "        del training_psnr\n",
    "        del training_ssim\n",
    "\n",
    "        pdhg.train(False)\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Model validation\n",
    "            validation_loss, validation_psnr, validation_ssim = validate_epoch(pdhg, dataloader_valid, loss_function)\n",
    "            # validation_loss, validation_psnr, validation_ssim = validate_iteration(pdhg, loss_function, sample=(noisy_image_data, clean_image_data))\n",
    "            # print(f\"Epoch {epoch+1} - VALIDATION LOSS: {validation_loss} - VALIDATION PSNR: {validation_psnr} - VALIDATION SSIM: {validation_ssim}\")\n",
    "            time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "            # Optional: Use wandb to log training progress\n",
    "            wandb.log({\"validation_loss\": validation_loss})\n",
    "            wandb.log({\"validation PSNR\": validation_psnr})\n",
    "            wandb.log({\"validation SSIM\": validation_ssim})\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        if (epoch+1) % save_epoch_local == 0:\n",
    "            current_model_name = f\"model_epoch_{epoch+1}\"\n",
    "            torch.save(pdhg, f\"{model_states_dir}/{current_model_name}.pt\")\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} - VALIDATION LOSS: {validation_loss} - VALIDATION PSNR: {validation_psnr} - VALIDATION SSIM: {validation_ssim}\")\n",
    "\n",
    "        if (epoch+1) % save_epoch_wandb == 0:\n",
    "            wandb.log_model(f\"{model_states_dir}/{current_model_name}.pt\", name=f\"model_epoch_{epoch+1}\")\n",
    "            \n",
    "        del validation_loss\n",
    "        del validation_psnr\n",
    "        del validation_ssim\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # Save the entire model\n",
    "    torch.save(pdhg, f\"{model_states_dir}/final_model.pt\")\n",
    "    \n",
    "    wandb.log_model(f\"{model_states_dir}/final_model.pt\", name=f\"final_model\")\n",
    "    wandb.finish()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return pdhg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 520.18it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 417.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 200\n",
      "Number of validation samples: 8\n",
      "Number of test samples: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find chest_xray.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtrung-vuthanh24\u001b[0m (\u001b[33mwof\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/t/Documents/GIT/DISSERTATION/LearningRegularizationParameterMaps/src/wandb/run-20240607_102714-wedf9ud9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wof/chest_xray/runs/wedf9ud9' target=\"_blank\">true-voice-32</a></strong> to <a href='https://wandb.ai/wof/chest_xray' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wof/chest_xray' target=\"_blank\">https://wandb.ai/wof/chest_xray</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wof/chest_xray/runs/wedf9ud9' target=\"_blank\">https://wandb.ai/wof/chest_xray/runs/wedf9ud9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/10000 [23:06<384:02:28, 138.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - VALIDATION LOSS: 0.0005710100704163779 - VALIDATION PSNR: 32.57280731201172 - VALIDATION SSIM: 0.8625402835446092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/10000 [45:54<379:42:18, 136.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - VALIDATION LOSS: 0.0004446141356311273 - VALIDATION PSNR: 33.98154830932617 - VALIDATION SSIM: 0.8910293266603647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/10000 [1:08:23<372:37:07, 134.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - VALIDATION LOSS: 0.00044609045653487556 - VALIDATION PSNR: 34.16588592529297 - VALIDATION SSIM: 0.8891912482748477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 40/10000 [1:31:10<378:19:22, 136.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - VALIDATION LOSS: 0.00046893031867512036 - VALIDATION PSNR: 34.016319274902344 - VALIDATION SSIM: 0.8854235077881514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/10000 [1:53:53<379:36:18, 137.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 - VALIDATION LOSS: 0.00039057856793078827 - VALIDATION PSNR: 34.773155212402344 - VALIDATION SSIM: 0.9013886833650172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 60/10000 [2:16:52<377:46:16, 136.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 - VALIDATION LOSS: 0.00034093733302142937 - VALIDATION PSNR: 35.2879638671875 - VALIDATION SSIM: 0.9089701414319052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 70/10000 [2:39:21<369:26:43, 133.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 - VALIDATION LOSS: 0.000369558187230723 - VALIDATION PSNR: 34.543617248535156 - VALIDATION SSIM: 0.9055067809397875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 80/10000 [3:01:33<364:33:24, 132.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 - VALIDATION LOSS: 0.00041878189131239196 - VALIDATION PSNR: 34.784549713134766 - VALIDATION SSIM: 0.899257626666069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 90/10000 [3:23:37<364:08:50, 132.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 - VALIDATION LOSS: 0.00037674439045076724 - VALIDATION PSNR: 34.44959259033203 - VALIDATION SSIM: 0.9048025910991431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 100/10000 [3:45:40<365:14:38, 132.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 - VALIDATION LOSS: 0.00028592384387593484 - VALIDATION PSNR: 36.197486877441406 - VALIDATION SSIM: 0.9259324403460025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 110/10000 [4:07:40<361:54:41, 131.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110 - VALIDATION LOSS: 0.0003434357586229453 - VALIDATION PSNR: 34.83209991455078 - VALIDATION SSIM: 0.9128418510409295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 120/10000 [4:29:41<361:49:39, 131.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120 - VALIDATION LOSS: 0.00037911836443527136 - VALIDATION PSNR: 34.657203674316406 - VALIDATION SSIM: 0.9058457838294358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 130/10000 [4:51:41<363:27:31, 132.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130 - VALIDATION LOSS: 0.00039260132962226635 - VALIDATION PSNR: 35.01564025878906 - VALIDATION SSIM: 0.9056924155397117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 140/10000 [5:13:38<362:12:20, 132.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140 - VALIDATION LOSS: 0.0002803427496473887 - VALIDATION PSNR: 36.39105224609375 - VALIDATION SSIM: 0.9278214865857362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 150/10000 [5:35:24<355:27:24, 129.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 - VALIDATION LOSS: 0.0002712864079512656 - VALIDATION PSNR: 36.30091094970703 - VALIDATION SSIM: 0.9294841204915643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 160/10000 [5:57:11<357:15:31, 130.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160 - VALIDATION LOSS: 0.00034544040863693226 - VALIDATION PSNR: 34.82447052001953 - VALIDATION SSIM: 0.9119767731209398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 170/10000 [6:18:55<355:12:41, 130.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170 - VALIDATION LOSS: 0.0002863162662833929 - VALIDATION PSNR: 35.674896240234375 - VALIDATION SSIM: 0.925696957536459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 180/10000 [6:40:46<355:32:54, 130.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180 - VALIDATION LOSS: 0.00035685775947058573 - VALIDATION PSNR: 35.172569274902344 - VALIDATION SSIM: 0.9110527157171369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 190/10000 [7:02:44<363:29:42, 133.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190 - VALIDATION LOSS: 0.0003017404960701242 - VALIDATION PSNR: 35.33978271484375 - VALIDATION SSIM: 0.9221256313389539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 200/10000 [7:24:45<358:43:29, 131.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200 - VALIDATION LOSS: 0.00030490448989439756 - VALIDATION PSNR: 35.33066940307617 - VALIDATION SSIM: 0.9208765520272254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 210/10000 [7:46:33<356:49:16, 131.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210 - VALIDATION LOSS: 0.00031359372951555997 - VALIDATION PSNR: 35.36635971069336 - VALIDATION SSIM: 0.9207136869761348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 220/10000 [8:08:17<355:03:45, 130.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220 - VALIDATION LOSS: 0.0002375755320827011 - VALIDATION PSNR: 36.76220703125 - VALIDATION SSIM: 0.9363230171430409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 230/10000 [8:30:04<353:32:54, 130.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230 - VALIDATION LOSS: 0.00034934447467094287 - VALIDATION PSNR: 35.468143463134766 - VALIDATION SSIM: 0.9159953121314346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 240/10000 [8:51:56<356:33:28, 131.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240 - VALIDATION LOSS: 0.00034271990443812683 - VALIDATION PSNR: 34.949771881103516 - VALIDATION SSIM: 0.9135166183463335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 250/10000 [9:13:47<354:48:32, 131.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250 - VALIDATION LOSS: 0.0003629739567259094 - VALIDATION PSNR: 35.06393051147461 - VALIDATION SSIM: 0.9110916254837328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 260/10000 [9:35:39<355:34:40, 131.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 260 - VALIDATION LOSS: 0.00031218838921631686 - VALIDATION PSNR: 35.44330596923828 - VALIDATION SSIM: 0.9198073105296494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 270/10000 [9:57:41<357:32:06, 132.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 270 - VALIDATION LOSS: 0.0002506715045456076 - VALIDATION PSNR: 36.41447448730469 - VALIDATION SSIM: 0.9324335580759049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 280/10000 [10:19:36<354:06:51, 131.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280 - VALIDATION LOSS: 0.00034582159969431814 - VALIDATION PSNR: 34.82734680175781 - VALIDATION SSIM: 0.913219250877738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 290/10000 [10:41:26<352:05:47, 130.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290 - VALIDATION LOSS: 0.0003553400110831717 - VALIDATION PSNR: 35.33578872680664 - VALIDATION SSIM: 0.9138931101059168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 300/10000 [11:03:38<364:25:15, 135.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 - VALIDATION LOSS: 0.00042279198896721937 - VALIDATION PSNR: 34.53043746948242 - VALIDATION SSIM: 0.9011897466957868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 310/10000 [11:26:04<361:45:19, 134.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 - VALIDATION LOSS: 0.00027423193751019426 - VALIDATION PSNR: 36.25631332397461 - VALIDATION SSIM: 0.9289840636897385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 320/10000 [11:48:41<367:42:21, 136.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320 - VALIDATION LOSS: 0.0003045014618692221 - VALIDATION PSNR: 36.045265197753906 - VALIDATION SSIM: 0.922863181744486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 330/10000 [12:11:35<368:47:54, 137.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330 - VALIDATION LOSS: 0.0003765587680391036 - VALIDATION PSNR: 34.408599853515625 - VALIDATION SSIM: 0.9072337739935815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 340/10000 [12:34:46<376:04:58, 140.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340 - VALIDATION LOSS: 0.0002786550121527398 - VALIDATION PSNR: 35.95136260986328 - VALIDATION SSIM: 0.9278816000429392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 350/10000 [12:57:58<375:32:40, 140.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350 - VALIDATION LOSS: 0.0002663796185515821 - VALIDATION PSNR: 36.21206283569336 - VALIDATION SSIM: 0.9301189034148156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 360/10000 [13:21:37<381:04:32, 142.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360 - VALIDATION LOSS: 0.0003553970473149093 - VALIDATION PSNR: 34.993141174316406 - VALIDATION SSIM: 0.9113488619585932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 370/10000 [13:44:57<371:24:48, 138.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 370 - VALIDATION LOSS: 0.0002773917240119772 - VALIDATION PSNR: 35.781185150146484 - VALIDATION SSIM: 0.9270370944134295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 380/10000 [14:08:34<379:58:30, 142.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380 - VALIDATION LOSS: 0.00032124604967975756 - VALIDATION PSNR: 35.312442779541016 - VALIDATION SSIM: 0.9180198447954059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 390/10000 [14:31:22<362:34:31, 135.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 390 - VALIDATION LOSS: 0.0002667969820322469 - VALIDATION PSNR: 35.81911087036133 - VALIDATION SSIM: 0.9286644210569264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 400/10000 [14:53:43<358:07:17, 134.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400 - VALIDATION LOSS: 0.0003279361862951191 - VALIDATION PSNR: 35.22127914428711 - VALIDATION SSIM: 0.917436407330811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 410/10000 [15:15:59<356:59:36, 134.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 410 - VALIDATION LOSS: 0.00031655889688408934 - VALIDATION PSNR: 35.714744567871094 - VALIDATION SSIM: 0.9192865857840925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 420/10000 [15:38:10<353:19:13, 132.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 420 - VALIDATION LOSS: 0.0003732543536898447 - VALIDATION PSNR: 34.64527893066406 - VALIDATION SSIM: 0.9091283974208534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 430/10000 [16:00:53<367:51:14, 138.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430 - VALIDATION LOSS: 0.0002584841777206748 - VALIDATION PSNR: 36.286094665527344 - VALIDATION SSIM: 0.9310584384249152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 440/10000 [16:23:42<362:51:59, 136.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 440 - VALIDATION LOSS: 0.0003274968603363959 - VALIDATION PSNR: 35.38528823852539 - VALIDATION SSIM: 0.9185580161298811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 450/10000 [16:46:50<366:31:47, 138.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 450 - VALIDATION LOSS: 0.000366301628673682 - VALIDATION PSNR: 34.575279235839844 - VALIDATION SSIM: 0.9091054845716655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 460/10000 [17:09:18<358:35:40, 135.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460 - VALIDATION LOSS: 0.00036257514511817135 - VALIDATION PSNR: 34.73383712768555 - VALIDATION SSIM: 0.9105663987129032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 470/10000 [17:31:33<354:10:39, 133.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 - VALIDATION LOSS: 0.00036278705738368444 - VALIDATION PSNR: 34.85524368286133 - VALIDATION SSIM: 0.9110427331964374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 480/10000 [17:53:47<351:58:15, 133.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480 - VALIDATION LOSS: 0.00032931624446064234 - VALIDATION PSNR: 35.47811508178711 - VALIDATION SSIM: 0.9179235982112737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 490/10000 [18:16:07<353:09:28, 133.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 490 - VALIDATION LOSS: 0.0003094510830123909 - VALIDATION PSNR: 36.13254165649414 - VALIDATION SSIM: 0.9237297482959329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 500/10000 [18:38:26<354:14:16, 134.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500 - VALIDATION LOSS: 0.00037161956788622774 - VALIDATION PSNR: 34.949127197265625 - VALIDATION SSIM: 0.9106916185871065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 510/10000 [19:00:41<353:00:49, 133.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 510 - VALIDATION LOSS: 0.0003009050979017047 - VALIDATION PSNR: 35.50809097290039 - VALIDATION SSIM: 0.9225223165884316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 520/10000 [19:22:56<350:03:26, 132.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 520 - VALIDATION LOSS: 0.0002982141377287917 - VALIDATION PSNR: 35.90607833862305 - VALIDATION SSIM: 0.9241137993355989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 530/10000 [19:45:16<351:29:19, 133.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 530 - VALIDATION LOSS: 0.00031422345819009934 - VALIDATION PSNR: 35.54316329956055 - VALIDATION SSIM: 0.9209256641066969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 540/10000 [20:07:39<354:32:07, 134.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 540 - VALIDATION LOSS: 0.0003462353670329321 - VALIDATION PSNR: 35.035491943359375 - VALIDATION SSIM: 0.9141972299690545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 550/10000 [20:30:02<353:50:46, 134.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 550 - VALIDATION LOSS: 0.0003102407845290145 - VALIDATION PSNR: 35.38688659667969 - VALIDATION SSIM: 0.9207891973167657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 560/10000 [20:52:15<351:18:13, 133.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 560 - VALIDATION LOSS: 0.0003417861498746788 - VALIDATION PSNR: 35.0734977722168 - VALIDATION SSIM: 0.9154047495102585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 570/10000 [21:14:31<347:47:41, 132.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 570 - VALIDATION LOSS: 0.00034442612650309457 - VALIDATION PSNR: 35.419986724853516 - VALIDATION SSIM: 0.9155024042900056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 580/10000 [21:36:54<351:27:45, 134.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 580 - VALIDATION LOSS: 0.0003272772446507588 - VALIDATION PSNR: 35.06228256225586 - VALIDATION SSIM: 0.9165361375000178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 590/10000 [21:59:20<354:12:47, 135.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590 - VALIDATION LOSS: 0.0003139291466141003 - VALIDATION PSNR: 35.54153060913086 - VALIDATION SSIM: 0.9204425054844917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 600/10000 [22:21:43<351:54:25, 134.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600 - VALIDATION LOSS: 0.0002895740044550621 - VALIDATION PSNR: 36.157569885253906 - VALIDATION SSIM: 0.9277067356413603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 610/10000 [22:44:08<351:31:39, 134.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 610 - VALIDATION LOSS: 0.0003304223864688538 - VALIDATION PSNR: 34.94240951538086 - VALIDATION SSIM: 0.9161008900119663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 620/10000 [23:06:32<350:56:12, 134.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 620 - VALIDATION LOSS: 0.0002464691424393095 - VALIDATION PSNR: 36.264888763427734 - VALIDATION SSIM: 0.9338575872546732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 630/10000 [23:28:51<348:49:53, 134.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 630 - VALIDATION LOSS: 0.00029321247166080866 - VALIDATION PSNR: 35.5020866394043 - VALIDATION SSIM: 0.9235962719891073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 640/10000 [23:51:13<348:15:49, 133.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 640 - VALIDATION LOSS: 0.0002881837463064585 - VALIDATION PSNR: 36.34734344482422 - VALIDATION SSIM: 0.9274638181740641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 650/10000 [24:13:31<347:10:20, 133.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650 - VALIDATION LOSS: 0.00029010763500991743 - VALIDATION PSNR: 35.94929504394531 - VALIDATION SSIM: 0.9259191450217218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 660/10000 [24:35:52<348:26:55, 134.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 660 - VALIDATION LOSS: 0.0003242628499720013 - VALIDATION PSNR: 35.65809631347656 - VALIDATION SSIM: 0.920597232754156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 670/10000 [24:58:11<346:44:46, 133.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 670 - VALIDATION LOSS: 0.0003530389367369935 - VALIDATION PSNR: 35.36943817138672 - VALIDATION SSIM: 0.9151485012375414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 680/10000 [25:20:37<347:03:00, 134.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 680 - VALIDATION LOSS: 0.00032686765371181536 - VALIDATION PSNR: 35.47846603393555 - VALIDATION SSIM: 0.9174876233893335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 690/10000 [25:43:22<356:49:24, 137.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 690 - VALIDATION LOSS: 0.00035793083770840894 - VALIDATION PSNR: 35.01470184326172 - VALIDATION SSIM: 0.9126483997364043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 700/10000 [26:06:08<353:03:12, 136.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700 - VALIDATION LOSS: 0.00032299303165927995 - VALIDATION PSNR: 35.34532165527344 - VALIDATION SSIM: 0.9194182455847263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 710/10000 [26:28:33<347:22:13, 134.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 710 - VALIDATION LOSS: 0.00026204190544376615 - VALIDATION PSNR: 36.08319854736328 - VALIDATION SSIM: 0.929765397564292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 720/10000 [26:50:55<346:14:39, 134.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 720 - VALIDATION LOSS: 0.0003314681034680689 - VALIDATION PSNR: 35.4534912109375 - VALIDATION SSIM: 0.9180424858714193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 730/10000 [27:13:10<343:18:42, 133.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 730 - VALIDATION LOSS: 0.0003534709949235548 - VALIDATION PSNR: 35.18098831176758 - VALIDATION SSIM: 0.9142062904006466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 740/10000 [27:35:33<346:30:24, 134.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 740 - VALIDATION LOSS: 0.0002737346803769469 - VALIDATION PSNR: 36.17460250854492 - VALIDATION SSIM: 0.9293961929038762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 750/10000 [27:57:47<342:28:20, 133.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 750 - VALIDATION LOSS: 0.0003567539188225055 - VALIDATION PSNR: 34.63428497314453 - VALIDATION SSIM: 0.9113822292861342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 760/10000 [28:20:07<343:40:39, 133.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 760 - VALIDATION LOSS: 0.0003647286648629233 - VALIDATION PSNR: 34.886962890625 - VALIDATION SSIM: 0.9119024412530958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 770/10000 [28:42:21<343:49:41, 134.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 770 - VALIDATION LOSS: 0.0002916599187301472 - VALIDATION PSNR: 35.52537536621094 - VALIDATION SSIM: 0.9242206561322809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 777/10000 [29:02:23<344:42:13, 134.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      2\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 4\u001b[0m pdhg \u001b[38;5;241m=\u001b[39m \u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      7\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[49], line 118\u001b[0m, in \u001b[0;36mstart_training\u001b[0;34m(config, pretrained_model_path, is_state_dict, start_epoch)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(start_epoch, num_epochs)):\n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Model training\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     pdhg\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 118\u001b[0m     training_loss, training_psnr, training_ssim \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdhg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# training_loss, training_psnr, training_ssim = train_iteration(optimizer, pdhg, loss_function, sample=(noisy_image_data, clean_image_data))\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# print(f\"Epoch {epoch+1} - TRAINING LOSS: {training_loss} - TRAINING PSNR: {training_psnr} - TRAINING SSIM: {training_ssim}\")\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# Optional: Use wandb to log training progress\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: training_loss})\n",
      "Cell \u001b[0;32mIn[43], line 38\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer, loss_func)\u001b[0m\n\u001b[1;32m     35\u001b[0m denoised_image_5d \u001b[38;5;241m=\u001b[39m model(noisy_image_5d)\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(denoised_image_5d, clean_image_5d)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m!=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem():\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN returned by loss function...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/Users/t/Documents/GIT/DISSERTATION/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages/torch/_tensor.py:516\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m/mnt/c/Users/t/Documents/GIT/DISSERTATION/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages/torch/overrides.py:1619\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1619\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1621\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/mnt/c/Users/t/Documents/GIT/DISSERTATION/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/t/Documents/GIT/DISSERTATION/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/t/Documents/GIT/DISSERTATION/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/t/Documents/GIT/DISSERTATION/LearningRegularizationParameterMaps/venv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "pdhg = start_training(get_config())\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Inference\n",
    "\n",
    "Demo the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CODE TO INFER AND SHOW SOME RESULTS HERE\n",
    "\n",
    "\n",
    "\n",
    "# def simple_plot(input_image_tensor_5D, subplot_index, image_name, clean_image_tensor_5D, folder_name, num_rows=2, num_cols=3):\n",
    "#     plot_image_tensor_2D = input_image_tensor_5D.squeeze(0).squeeze(0).squeeze(-1)\n",
    "#     clean_image_tensor_2D = clean_image_tensor_5D.squeeze(0).squeeze(0).squeeze(-1)\n",
    "#     psnr_value = PSNR(clean_image_tensor_2D, plot_image_tensor_2D)\n",
    "#     ssim_value = SSIM(clean_image_tensor_2D, plot_image_tensor_2D)\n",
    "#     plt.subplot(num_rows, num_cols, subplot_index)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(plot_image_tensor_2D.to(\"cpu\").detach().numpy(), cmap='gray')\n",
    "#     plt.title(f\"{image_name} PSNR: {psnr_value:.2f} dB\\n{image_name} SSIM: {ssim_value:.2f}\", fontsize=10)\n",
    "#     # Write the image to a file\n",
    "#     save_image(plot_image_tensor_2D, f\"{image_name}\", folder_name)\n",
    "\n",
    "# def get_image_tensor_5D(image):\n",
    "#     image = image.convert(\"L\")\n",
    "#     image_numpy = np.asarray(image)\n",
    "#     image_tensor_4D = convert_to_tensor_4D(image_numpy)\n",
    "#     image_tensor_5D = image_tensor_4D.unsqueeze(0).to(DEVICE)\n",
    "#     return image_tensor_5D\n",
    "\n",
    "# def denoise(pdhg: DynamicImageStaticPrimalDualNN, noisy_image_tensor_5D):\n",
    "#     pdhg.eval()\n",
    "#     with torch.no_grad():\n",
    "#         best_lambda_map = pdhg.get_lambda_cnn(noisy_image_tensor_5D)\n",
    "#     x_denoised_lambda_map_best_tensor_5D = reconstruct_with_PDHG(noisy_image_tensor_5D, best_lambda_map, pdhg.T)\n",
    "#     # x_denoised_lambda_map_best_tensor_5D = torch.clamp(x_denoised_lambda_map_best_tensor_5D, 0, 1)\n",
    "#     with torch.no_grad():\n",
    "#         torch.cuda.empty_cache()\n",
    "#     return best_lambda_map, x_denoised_lambda_map_best_tensor_5D\n",
    "\n",
    "\n",
    "# def brute_force_lambda(noisy_image_tensor_5D, clean_image_tensor_5D, T, min_value=0.01, max_value=0.1, num_values=10):\n",
    "#     # TODO: Brute-force single lambda\n",
    "#     best_psnr = 0\n",
    "#     best_lambda = 0\n",
    "#     lambas = list(np.linspace(min_value, max_value, num_values))\n",
    "#     psnr_values = []\n",
    "#     for lambda_value in lambas:\n",
    "#         with torch.no_grad():\n",
    "#             x_denoised_single_lambda_tensor_5D = reconstruct_with_PDHG(noisy_image_tensor_5D, lambda_value, T)\n",
    "#         psnr_value = PSNR(clean_image_tensor_5D, x_denoised_single_lambda_tensor_5D)\n",
    "#         psnr_value = psnr_value.item()\n",
    "#         # Convert to float\n",
    "#         psnr_value = np.float64(psnr_value)\n",
    "#         if psnr_value > best_psnr:\n",
    "#             best_psnr = psnr_value\n",
    "#             best_lambda = lambda_value\n",
    "#         psnr_values.append(psnr_value)\n",
    "\n",
    "#     # Plot the PSNR values\n",
    "#     plt.plot(lambas, psnr_values)\n",
    "#     plt.xlabel(\"Lambda\")\n",
    "#     plt.ylabel(\"PSNR\")\n",
    "#     plt.title(\"PSNR vs Lambda\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     return best_lambda\n",
    "\n",
    "\n",
    "# def test_denoise(pdhg: DynamicImageStaticPrimalDualNN=None, model_name=\"\", best_lambda=None):\n",
    "#     \"\"\"\n",
    "#     Testing denoising with pre-trained parameters.\n",
    "#     \"\"\"\n",
    "#     clean_image = Image.open(f\"testcases/chest_xray_clean.png\")\n",
    "#     noisy_image = Image.open(f\"testcases/chest_xray_noisy.png\")\n",
    "#     clean_image_tensor_5D = get_image_tensor_5D(clean_image)\n",
    "#     noisy_image_tensor_5D = get_image_tensor_5D(noisy_image)\n",
    "\n",
    "#     if best_lambda is None:\n",
    "#         best_lambda = brute_force_lambda(noisy_image_tensor_5D, clean_image_tensor_5D, T=pdhg.T, min_value=0.01, max_value=1, num_values=100)\n",
    "\n",
    "#     print(f\"Best lambda: {best_lambda}\")\n",
    "\n",
    "#     k_w, k_h = 256, 256\n",
    "\n",
    "#     folder_name = f\"./tmp/images/model_{model_name}-kernel_{k_w}-best_lambda_{str(best_lambda).replace('.', '_')}-time_{datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "#     os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "#     plt.figure(figsize=(15, 6)) # Set the size of the plot\n",
    "\n",
    "#     simple_plot(clean_image_tensor_5D, 1, \"clean\", clean_image_tensor_5D, folder_name)\n",
    "#     simple_plot(noisy_image_tensor_5D, 2, \"noisy\", clean_image_tensor_5D, folder_name)\n",
    "\n",
    "#     x_denoised_single_lambda_tensor_5D = reconstruct_with_PDHG(noisy_image_tensor_5D, best_lambda, T=pdhg.T)\n",
    "    \n",
    "#     best_lambda_map, x_denoised_lambda_map_tensor_5D = denoise(pdhg, noisy_image_tensor_5D)\n",
    "\n",
    "#     # Clip to [0, 1]. The calculations may make it slightly below 0 and above 1\n",
    "#     x_denoised_single_lambda_tensor_5D = torch.clamp(x_denoised_single_lambda_tensor_5D, 0, 1)\n",
    "#     x_denoised_lambda_map_tensor_5D = torch.clamp(x_denoised_lambda_map_tensor_5D, 0, 1)\n",
    "\n",
    "#     simple_plot(x_denoised_single_lambda_tensor_5D, 3, f\"single_lambda_best_{str(best_lambda).replace('.', '_')}\", clean_image_tensor_5D, folder_name)\n",
    "#     simple_plot(x_denoised_lambda_map_tensor_5D, 4, \"lambda_map_best_using_function\", clean_image_tensor_5D, folder_name)\n",
    "\n",
    "#     lambda_map_1 = best_lambda_map[:, 0:1, :, :, :]\n",
    "#     lambda_map_2 = best_lambda_map[:, 1:2, :, :, :]\n",
    "#     lambda_map_3 = best_lambda_map[:, 2:3, :, :, :]\n",
    "\n",
    "#     lambda_map_1 = torch.clamp(lambda_map_1, 0, 1)\n",
    "#     lambda_map_2 = torch.clamp(lambda_map_2, 0, 1)\n",
    "#     lambda_map_3 = torch.clamp(lambda_map_3, 0, 1)\n",
    "\n",
    "#     simple_plot(lambda_map_1, 5, \"lambda_map_1\", clean_image_tensor_5D, folder_name)\n",
    "#     simple_plot(lambda_map_3, 6, \"lambda_map_3\", clean_image_tensor_5D, folder_name)\n",
    "\n",
    "#     plt.savefig(f\"{folder_name}/results.png\")\n",
    "\n",
    "#     plt.show();\n",
    "\n",
    "#     with open(f\"{folder_name}/log.txt\", \"w\") as f:\n",
    "#         f.write(f\"Best lambda: {best_lambda}\\n\")\n",
    "#         f.write(f\"PSNR (single lambda): {PSNR(clean_image_tensor_5D.squeeze(0).squeeze(0).squeeze(-1), x_denoised_single_lambda_tensor_5D.squeeze(0).squeeze(0).squeeze(-1))}\\n\")\n",
    "#         f.write(f\"PSNR (lambda map): {PSNR(clean_image_tensor_5D.squeeze(0).squeeze(0).squeeze(-1), x_denoised_lambda_map_tensor_5D.squeeze(0).squeeze(0).squeeze(-1))}\\n\")\n",
    "#         f.write(f\"Config: {get_config()}\\n\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "# model_dir = \"./tmp_2/model-2024_06_05_23_51_27\"\n",
    "# epoch = 4000\n",
    "# pdhg = torch.load(f\"{model_dir}/model_epoch_{epoch}.pt\")\n",
    "\n",
    "# test_denoise(\n",
    "#     pdhg=pdhg,\n",
    "#     model_name=f\"chest_xray_demo-epoch_{epoch}\",\n",
    "#     best_lambda=0.08\n",
    "# )\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def temp_test():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Create a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_video(model_name, start_epoch=20, end_epoch=10_000, step=20):\n",
    "#     clean_image_path = \"./test_cases/turtle_clean/turtle clean.png\"\n",
    "#     noisy_image_path = \"./test_cases/turtle_noisy/turtle noisy.png\"\n",
    "#     clean_image_tensor_5D = get_image_tensor_5D(clean_image_path)\n",
    "#     noisy_image_tensor_5D = get_image_tensor_5D(noisy_image_path)\n",
    "#     clean_image_tensor_2D = clean_image_tensor_5D.squeeze(0).squeeze(0).squeeze(-1)\n",
    "#     noisy_image_tensor_2D = noisy_image_tensor_5D.squeeze(0).squeeze(0).squeeze(-1)\n",
    "\n",
    "#     psnr_noisy = PSNR(noisy_image_tensor_2D, clean_image_tensor_2D)\n",
    "#     ssim_noisy = SSIM(noisy_image_tensor_2D, clean_image_tensor_2D)\n",
    "\n",
    "\n",
    "\n",
    "#     frames_folder = f\"./tmp/{model_name}\"\n",
    "#     model_folder=f\"./tmp_2/{model_name}\"\n",
    "#     os.makedirs(frames_folder, exist_ok=True)\n",
    "#     os.makedirs(f\"{frames_folder}/denoised\", exist_ok=True)\n",
    "#     os.makedirs(f\"{frames_folder}/lambda_map_1\", exist_ok=True)\n",
    "#     os.makedirs(f\"{frames_folder}/lambda_map_2\", exist_ok=True)\n",
    "#     os.makedirs(f\"{frames_folder}/lambda_map_3\", exist_ok=True)\n",
    "\n",
    "#     with open(f\"./tmp/{model_name}/metrics.csv\", \"w\") as f:\n",
    "#         f.write(f\"Image, PSNR, SSIM\\n\")\n",
    "#         f.write(f\"Noisy, {psnr_noisy:.2f}, {ssim_noisy:.2f}\\n\")\n",
    "\n",
    "#         for epoch in range(start_epoch, end_epoch + 1, step):\n",
    "#             model_name = f\"model_epoch_{epoch}\"\n",
    "#             pdhg = torch.load(f\"{model_folder}/{model_name}.pt\")\n",
    "#             best_lambda_map, x_denoised_lambda_map_best_tensor_5D = denoise(pdhg, noisy_image_tensor_5D)\n",
    "#             x_denoised_lambda_map_best_tensor_5D = torch.clamp(x_denoised_lambda_map_best_tensor_5D, 0, 1)\n",
    "\n",
    "#             x_denoised_lambda_map_best_tensor_2D = x_denoised_lambda_map_best_tensor_5D.squeeze(0).squeeze(0).squeeze(-1)\n",
    "#             psnr_denoised = PSNR(x_denoised_lambda_map_best_tensor_2D, clean_image_tensor_2D)\n",
    "#             ssim_denoised = SSIM(x_denoised_lambda_map_best_tensor_2D, clean_image_tensor_2D)\n",
    "#             f.write(f\"{epoch}, {psnr_denoised:.2f}, {ssim_denoised:.2f}\\n\")\n",
    "\n",
    "#             denoised_image_to_save = Image.fromarray((x_denoised_lambda_map_best_tensor_2D.to(\"cpu\").detach().numpy() * 255).astype(np.uint8))\n",
    "#             denoised_image_to_save.save(f\"{frames_folder}/denoised/{epoch}.png\")\n",
    "\n",
    "#             lambda_map_1 = best_lambda_map[:, 0:1, :, :, :]\n",
    "#             lambda_map_2 = best_lambda_map[:, 1:2, :, :, :]\n",
    "#             lambda_map_3 = best_lambda_map[:, 2:3, :, :, :]\n",
    "#             lambda_map_1 = torch.clamp(lambda_map_1, 0, 1)\n",
    "#             lambda_map_2 = torch.clamp(lambda_map_2, 0, 1)\n",
    "#             lambda_map_3 = torch.clamp(lambda_map_3, 0, 1)\n",
    "\n",
    "#             lambda_map_1_to_save = Image.fromarray((lambda_map_1.squeeze(0).squeeze(0).squeeze(-1).to(\"cpu\").detach().numpy() * 255).astype(np.uint8))\n",
    "#             lambda_map_1_to_save.save(f\"{frames_folder}/lambda_map_1/{epoch}.png\")\n",
    "\n",
    "#             lambda_map_2_to_save = Image.fromarray((lambda_map_2.squeeze(0).squeeze(0).squeeze(-1).to(\"cpu\").detach().numpy() * 255).astype(np.uint8))\n",
    "#             lambda_map_2_to_save.save(f\"{frames_folder}/lambda_map_2/{epoch}.png\")\n",
    "\n",
    "#             lambda_map_3_to_save = Image.fromarray((lambda_map_3.squeeze(0).squeeze(0).squeeze(-1).to(\"cpu\").detach().numpy() * 255).astype(np.uint8))\n",
    "#             lambda_map_3_to_save.save(f\"{frames_folder}/lambda_map_3/{epoch}.png\")\n",
    "        \n",
    "\n",
    "#     # # Create the video\n",
    "#     # frames = []\n",
    "#     # for epoch in range(start_epoch, end_epoch + 1, step):\n",
    "#     #     frames.append(cv2.imread(f\"{frames_folder}/frame_{epoch}.png\"))\n",
    "#     # height, width, layers = frames[0].shape\n",
    "#     # size = (width, height)\n",
    "#     # out = cv2.VideoWriter(f\"{frames_folder}/video.avi\", cv2.VideoWriter_fourcc(*'DIVX'), 1, size)\n",
    "#     # for i in range(len(frames)):\n",
    "#     #     out.write(frames[i])\n",
    "#     # out.release()\n",
    "\n",
    "# create_video(\"model_turtle_2024_06_04_04_19_21\", start_epoch=20, end_epoch=10_000, step=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_revisualise():\n",
    "\n",
    "#     def vis(image_folder, image_name):\n",
    "#         image_path = f\"{image_folder}/{image_name}.png\"\n",
    "#         image = Image.open(image_path)\n",
    "#         plt.imshow(image, cmap='gray')\n",
    "#         plt.show();\n",
    "\n",
    "#     image_folder = \"tmp/PRESENT/presentation-img_turtle-best_lambda_0_07-kernel_256-model_-trained_on_-time_2024_06_04_22_59_31-epoch_100_000\"\n",
    "#     image_names = [\n",
    "#         \"lambda_map_3\",\n",
    "#         \"single_lambda_best_0_06000000000000001\",\n",
    "#         \"clean\",\n",
    "#     ]\n",
    "\n",
    "#     for image_name in image_names:\n",
    "#         vis(image_folder, image_name)\n",
    "\n",
    "# test_revisualise()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
