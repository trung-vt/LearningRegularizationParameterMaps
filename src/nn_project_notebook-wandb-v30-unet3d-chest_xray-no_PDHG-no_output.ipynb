{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time_importing_torch = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# NOTE: Importing torch the first time will always take a long time!\n",
    "import time\n",
    "# NOTE: Importing torch the first time will always take a long time!\n",
    "if first_time_importing_torch:\n",
    "    print(f\"Importing torch ...\")\n",
    "    import_torch_start_time = time.time() \n",
    "import torch\n",
    "if first_time_importing_torch:\n",
    "    import_torch_end_time = time.time()\n",
    "    print(f\"Importing torch took {import_torch_end_time - import_torch_start_time} seconds\")\n",
    "    first_time_importing_torch = False\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "\n",
    "from skimage.metrics import structural_similarity\n",
    "# from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Optional\n",
    "from tqdm import tqdm # progress bar\n",
    "\n",
    "import wandb # Optional, for logging\n",
    "\n",
    "import json\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Path: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISABLING_TESTS = False\n",
    "# DISABLING_TESTS = True   # Disable tests for less output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(f\"Using {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(f\"Using {torch.backends.mps.get_device_name(0)} with MPS\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "torch.set_default_device(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIDD_DATA_PATH = \"../data/dyn_img_static/tmp/SIDD_Small_sRGB_Only/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHEST_XRAY_BASE_DATA_PATH = \"../data/chest_xray\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    CHEST_XRAY_BASE_DATA_PATH = \"../data/chest_xray\"\n",
    "    return {\n",
    "        \"project\": \"chest_xray\",\n",
    "        \"dataset\": CHEST_XRAY_BASE_DATA_PATH,\n",
    "\n",
    "        \"train_data_path\": f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\",\n",
    "\n",
    "        # \"val_data_path\": f\"{CHEST_XRAY_BASE_DATA_PATH}/val/NORMAL\",\n",
    "        # TODO: \n",
    "        \"val_data_path\": f\"{CHEST_XRAY_BASE_DATA_PATH}/test/NORMAL\", # More images in test folder. Val folder has only 8 images.\n",
    "\n",
    "        \"test_data_path\": f\"{CHEST_XRAY_BASE_DATA_PATH}/test/NORMAL\",\n",
    "\n",
    "        \"train_num_samples\": 200,\n",
    "        \"val_num_samples\": 20,\n",
    "        \"test_num_samples\": 1,\n",
    "\n",
    "        # \"patch\": 512,\n",
    "        # \"stride\": 512,\n",
    "        \"resize_square\": 256,\n",
    "        \"min_sigma\": 0.1,\n",
    "        \"max_sigma\": 0.5,\n",
    "        \"batch_size\": 1,\n",
    "        \"random_seed\": 42,\n",
    "\n",
    "        \"architecture\": \"UNET-PDHG\",\n",
    "        \"in_channels\": 1,\n",
    "        # \"out_channels\": 2,\n",
    "        \"out_channels\": 1,\n",
    "        \"init_filters\": 64,\n",
    "        \"n_blocks\": 3,\n",
    "        # \"activation\": \"LeakyReLU\",\n",
    "        \"activation\": \"ReLU\",\n",
    "        \"downsampling_kernel\": (2, 2, 1),\n",
    "        \"downsampling_mode\": \"max\",\n",
    "        \"upsampling_kernel\": (2, 2, 1),\n",
    "        \"upsampling_mode\": \"linear_interpolation\",\n",
    "\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"loss_function\": \"MSELoss\",\n",
    "\n",
    "        # \"up_bound\": 0.5,\n",
    "        \"up_bound\": 0,\n",
    "        \"T\": 0, # Higher T, NET does not have to try as hard? Less overfitting?\n",
    "\n",
    "        \"epochs\": 10_000,\n",
    "        \"device\": \"cuda:0\",\n",
    "\n",
    "        \"wandb_mode\": \"online\",\n",
    "        \"save_epoch_wandb\": 10_000,\n",
    "        \"save_epoch_local\": 10,\n",
    "        \"save_dir\": \"tmp_2\",\n",
    "    }\n",
    "\n",
    "print(get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Import the image and transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # REMEMBER TO COMMENT THIS OUT IF THE DATA HAS BEEN DOWNLOADED!\n",
    "# !wget https://competitions.codalab.org/my/datasets/download/a26784fe-cf33-48c2-b61f-94b299dbc0f2\n",
    "# !unzip \"a26784fe-cf33-48c2-b61f-94b299dbc0f2\" -d ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load SIDD images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_npy_file(sample_path: str, scale_factor: float) -> np.ndarray:\t\t\t\n",
    "    scale_factor_str = str(scale_factor).replace('.','_')\n",
    "    xf = np.load(os.path.join(sample_path, f\"xf_scale_factor{scale_factor_str}.npy\"))\n",
    "    xf = torch.tensor(xf, dtype=torch.float)\n",
    "    xf = xf.unsqueeze(0) / 255\n",
    "    return xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CHANGE THIS TO YOUR PATH\n",
    "# NOTE: Windows uses \\\\ instead of /\n",
    "def load_images_SIDD(ids: list, take_npy_files: bool) -> list:\n",
    "    data_path = SIDD_DATA_PATH\n",
    "    k = 0\n",
    "\n",
    "    images = []\n",
    "\n",
    "    for folder in os.listdir(data_path):\n",
    "        img_id = folder[:4]\t# The first 4 characters of folder name is the image id (0001, 0002, ..., 0200)\n",
    "        if img_id not in ids:\n",
    "            continue\n",
    "        k += 1\n",
    "        print(f'loading image id {img_id}, {k}/{len(ids)}')\n",
    "\n",
    "        files_path = os.path.join(data_path, folder)\n",
    "\n",
    "        # if take_npy_files:\n",
    "        #     xf = get_npy_file(files_path, scale_factor)\n",
    "        #     images.append(xf)\n",
    "        #     continue\n",
    "\n",
    "        # Use only the ground truth images\n",
    "        file = \"GT_SRGB_010.PNG\"  # GT = Ground Truth\n",
    "\n",
    "        image = Image.open(os.path.join(files_path, file))\n",
    "        assert image.mode == 'RGB', f\"Image mode is not RGB: {image.mode}\" # For now, expect RGB images\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_images_SIDD():\n",
    "    if DISABLING_TESTS: return\n",
    "    for img in load_images_SIDD([\"0065\"], False):\n",
    "        print(img.size)\n",
    "        plt.imshow(img)\n",
    "\n",
    "test_load_images_SIDD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Chest X-ray images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CHANGE THIS TO YOUR PATH\n",
    "# NOTE: Windows uses \\\\ instead of /\n",
    "def load_images_chest_xray(data_path: str, ids: list) -> list:\n",
    "    files = os.listdir(data_path)\n",
    "    jpeg_files = [f for f in files if f.endswith(\".jpeg\")]\n",
    "\n",
    "    images = []\n",
    "    for id in tqdm(ids):\n",
    "        if id >= len(jpeg_files): continue\n",
    "        # print(f\"Loading image {id} from {data_path}\")\n",
    "        image = Image.open(os.path.join(data_path, jpeg_files[id]))\n",
    "        images.append(image)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_images_chest_xray(stage=\"train\", label=\"NORMAL\"):\n",
    "    if DISABLING_TESTS: return\n",
    "    for img in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/{stage}/{label}\", [0]):\n",
    "        print(img.size)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "    plt.show();\n",
    "\n",
    "test_load_images_chest_xray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Turtle images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CHANGE THIS TO YOUR PATH\n",
    "# NOTE: Windows uses \\\\ instead of /\n",
    "def load_images_turtle(data_path: str, ids: list) -> list:\n",
    "    files = os.listdir(data_path)\n",
    "    image_files = [f for f in files if (f.endswith(\".jpeg\") or f.endswith(\".jpg\") or f.endswith(\".png\"))]\n",
    "\n",
    "    images = []\n",
    "    for id in tqdm(ids):\n",
    "        if id >= len(image_files): continue\n",
    "        # print(f\"Loading image {id} from {data_path}\")\n",
    "        image = Image.open(os.path.join(data_path, image_files[id]))\n",
    "        images.append(image)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_images_turtle():\n",
    "    if DISABLING_TESTS: return\n",
    "    turtle_data_path = \"../data/turtle/images\"\n",
    "    # for img in load_images_turtle(turtle_data_path, [12]):\n",
    "    # for img in load_images_turtle(turtle_data_path, [19]):\n",
    "    # for img in load_images_turtle(turtle_data_path, [28]):\n",
    "    # for img in load_images_turtle(turtle_data_path, [43]):\n",
    "    # for img in load_images_turtle(turtle_data_path, [50]):\n",
    "    # for img in load_images_turtle(turtle_data_path, [55]):\n",
    "    for img in load_images_turtle(turtle_data_path, [68]):  # baby turtle on sand\n",
    "    # for img in load_images_turtle(turtle_data_path, [74]):  # turtle in water\n",
    "    # for img in load_images_turtle(turtle_data_path, [75]):\n",
    "    # for img in load_images_turtle(turtle_data_path, [82]):\n",
    "        print(img.size)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "    plt.show();\n",
    "\n",
    "test_load_images_turtle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "#### Convert image to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayscale(image: Image) -> Image:\n",
    "    return image.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert_to_grayscale():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for img in load_images_SIDD([\"0065\"], False):\n",
    "    # for img in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "    # for img in load_images_turtle(\"../data/turtle/images\", [5]):\n",
    "    for img in load_images_turtle(\"../data/turtle/images\", [68]):\n",
    "        img = convert_to_grayscale(img)\n",
    "        plt.imshow(img, cmap='gray') # cmap='gray' for proper display in black and white. It does not convert the image to grayscale.\n",
    "\n",
    "test_convert_to_grayscale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_square(image: Image) -> Image:\n",
    "    width, height = image.size\n",
    "    new_size = min(width, height)\n",
    "    left = (width - new_size) / 2\n",
    "    top = (height - new_size) / 2\n",
    "    right = (width + new_size) / 2\n",
    "    bottom = (height + new_size) / 2\n",
    "    return image.crop((left, top, right, bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_crop_to_square():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for img in load_images_SIDD([\"0083\"], False):\n",
    "    # for img in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "    # for img in load_images_turtle(\"../data/turtle/images\", [5]):\n",
    "    for img in load_images_turtle(\"../data/turtle/images\", [68]):\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show();\n",
    "        img = crop_to_square(img)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "test_crop_to_square()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_square_and_resize(image: Image, side_len: int) -> Image:\n",
    "    image = crop_to_square(image)\n",
    "    return image.resize(size=(side_len, side_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_crop_to_square_and_resize():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for img in load_images_SIDD([\"0083\"], False):\n",
    "    # for img in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "    for img in load_images_turtle(\"../data/turtle/images\", [68]):\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show();\n",
    "        img = crop_to_square_and_resize(img, 120)\n",
    "        print(img.size)\n",
    "        plt.imshow(img, cmap='gray') # cmap='gray' for proper display in black and white. It does not convert the image to grayscale.\n",
    "        plt.show();\n",
    "\n",
    "test_crop_to_square_and_resize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numpy(image):\n",
    "    image_data = np.asarray(image)\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert_to_numpy():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for img in load_images_SIDD([\"0083\"], False):\n",
    "    # for img in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "    for img in load_images_turtle(\"../data/turtle/images\", [68]):\n",
    "        img = convert_to_grayscale(img)\n",
    "        print(f\"Before conversion: {type(img)}\")\n",
    "        image_data = convert_to_numpy(img)\n",
    "        print(f\"After conversion: {type(image_data)}\")\n",
    "        # plt.imshow still works with numpy array\n",
    "        plt.imshow(image_data, cmap='gray')\n",
    "\n",
    "test_convert_to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to tensor\n",
    "\n",
    "For efficient computation on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor_4D(image_numpy):\n",
    "    # xf = []\n",
    "    # xf.append(image_numpy)\n",
    "    # xf = np.stack(xf, axis=-1)\n",
    "    # xf = torch.tensor(xf, dtype=torch.float)\n",
    "    xf = torch.tensor(image_numpy, dtype=torch.float)\n",
    "    xf = xf.unsqueeze(0)\n",
    "    xf = xf.unsqueeze(-1)\n",
    "    xf = xf / 255 # Normalise from [0, 255] to [0, 1]\n",
    "    return xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_convert_to_tensor():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for image in load_images_SIDD([\"0083\"], False):\n",
    "    # for image in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "    for image in load_images_turtle(\"../data/turtle/images\", [68]):\n",
    "        image = convert_to_grayscale(image)\n",
    "        image_numpy = convert_to_numpy(image)\n",
    "        image_tensor_4D = convert_to_tensor_4D(image_numpy)\n",
    "        print(image_tensor_4D.size())\n",
    "        plt.imshow(image_tensor_4D.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "\n",
    "\n",
    "test_convert_to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add synthetic noise\n",
    "\n",
    "<!-- artificial Gaussian noise\n",
    "\n",
    "Noise can occur in reality.\n",
    "\n",
    "It is difficult to obtain a pair of clean and noisy images of one exact same scene.\n",
    "\n",
    "For training, it is common to add synthetic noise to an image that is considered clean and then try to reconstruct it.\n",
    "\n",
    "There are many types of noise and different ways to add noise. We can add salt-and-pepper noise. (?)We can add more noise in some parts and less in others. We can use a combination of noise-adding strategies to build more robust models.\n",
    "\n",
    "For our purpose, we will focus on Gaussian noise. This is sufficient for most cases. \n",
    "\n",
    "(?) We will add noise with the same probability for each pixel (not using the strategies of focusing on certain regions) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable_noise(sigma_min, sigma_max):\n",
    "    return sigma_min + torch.rand(1) * (sigma_max - sigma_min)\n",
    "\n",
    "def add_noise(xf: torch.tensor, sigma) -> torch.tensor:\n",
    "    std = torch.std(xf)\n",
    "    mu = torch.mean(xf)\n",
    "\n",
    "    x_centred = (xf  - mu) / std\n",
    "\n",
    "    x_centred += sigma * torch.randn(xf.shape, dtype = xf.dtype)\n",
    "\n",
    "    xnoise = std * x_centred + mu\n",
    "\n",
    "    del std, mu, x_centred\n",
    "\n",
    "    return xnoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_noise():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for rgb_image in load_images_SIDD([\"0083\"], False):\n",
    "    # for rgb_image in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "    for rgb_image in load_images_turtle(\"../data/turtle/images\", [68]):\n",
    "        grayscale_image = convert_to_grayscale(rgb_image)\n",
    "        grayscale_image = crop_to_square_and_resize(grayscale_image, 256)\n",
    "        print(f\"grayscale_image.size: {grayscale_image.size}\")\n",
    "        image_numpy = convert_to_numpy(grayscale_image)\n",
    "        image_tensor_4D = convert_to_tensor_4D(image_numpy)\n",
    "        constant_noise_img = add_noise(image_tensor_4D, sigma=0.5)\n",
    "        variable_noise_img = add_noise(image_tensor_4D, get_variable_noise(\n",
    "            sigma_min=0.1, sigma_max=0.5))\n",
    "        plt.imshow(grayscale_image, cmap='gray')\n",
    "        plt.show();\n",
    "        plt.imshow(constant_noise_img.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "        plt.imshow(variable_noise_img.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "test_add_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Calculate PSNR\n",
    "\n",
    "PSNR is a common metrics for noisy image.\n",
    "\n",
    "Compare before and after adding synthetic noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(original, compressed): \n",
    "    mse = torch.mean((original - compressed) ** 2) \n",
    "    if(mse == 0): # MSE is zero means no noise is present in the signal. \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    # max_pixel = 255.0\n",
    "    max_pixel = 1.0\n",
    "    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse)) \n",
    "\n",
    "    del mse\n",
    "\n",
    "    return psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_PSNR():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for rgb_image in load_images_SIDD([\"0083\"], False):\n",
    "    # for rgb_image in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "    for rgb_image in load_images_turtle(\"../data/turtle/images\", [68]):\n",
    "        grayscale_image = convert_to_grayscale(rgb_image)\n",
    "        grayscale_image = crop_to_square_and_resize(grayscale_image, 256)\n",
    "        image_numpy = convert_to_numpy(grayscale_image)\n",
    "        image_tensor_4D = convert_to_tensor_4D(image_numpy)\n",
    "\n",
    "        print(f\"PSNR of original image: {PSNR(image_tensor_4D, image_tensor_4D)} dB\")\n",
    "        plt.imshow(image_tensor_4D.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "        # noisy_image_tensor_4D = add_noise(image_tensor_4D, sigma=0.001) # 0.001 -> 75.6 dB. Very little noise to see what an excellent PSNR looks like\n",
    "        # noisy_image_tensor_4D = add_noise(image_tensor_4D, sigma=0.01) # 0.01 -> 55.6 dB. Very good quality\n",
    "        # noisy_image_tensor_4D = add_noise(image_tensor_4D, sigma=0.1) # 0.1 -> 35.6 dB. Still alright as a \"denoised\" reconstruction\n",
    "        noisy_image_tensor_4D = add_noise(image_tensor_4D, sigma=0.5) # 0.5 -> 21.6 dB. Clearly bad\n",
    "        # noisy_image_tensor_4D = add_noise(image_tensor_4D, sigma=1) # 1 -> 15.6 dB. Almost unusable\n",
    "        print(f\"PSNR of constant noise image: {PSNR(noisy_image_tensor_4D, image_tensor_4D):.2f} dB\")\n",
    "        plt.imshow(noisy_image_tensor_4D.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "\n",
    "test_PSNR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Calculate SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSIM(tensor_2D_a: torch.Tensor, tensor_2D_b: torch.Tensor, data_range: float=1) -> float:\n",
    "    return structural_similarity(\n",
    "        tensor_2D_a.to(\"cpu\").detach().numpy(), \n",
    "        tensor_2D_b.to(\"cpu\").detach().numpy(),\n",
    "        data_range=data_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_SSIM(sigma=0.5):\n",
    "    if DISABLING_TESTS: return\n",
    "    # for rgb_image in load_images_SIDD([\"0083\"], False):\n",
    "    for rgb_image in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "        grayscale_image = convert_to_grayscale(rgb_image)\n",
    "        grayscale_image = crop_to_square_and_resize(grayscale_image, 120)\n",
    "        image_numpy = convert_to_numpy(grayscale_image)\n",
    "        image_tensor_4D = convert_to_tensor_4D(image_numpy)\n",
    "\n",
    "        image_tensor_2D = image_tensor_4D.squeeze(0).squeeze(-1)\n",
    "        print(f\"image_tensor_2D: {image_tensor_2D.size()}\")\n",
    "        print(f\"SSIM of original image: {SSIM(image_tensor_2D, image_tensor_2D)}\")\n",
    "        plt.imshow(image_tensor_2D.cpu(), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "        noisy_image_tensor_2D = add_noise(image_tensor_2D, sigma=sigma)\n",
    "        print(f\"noisy_image_tensor_2D: {noisy_image_tensor_2D.size()}\")\n",
    "        print(f\"SSIM of noisy image (sigma={sigma}): {SSIM(noisy_image_tensor_2D, image_tensor_2D):.2f}\")\n",
    "        plt.imshow(noisy_image_tensor_2D.cpu(), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "test_SSIM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Reconstruct an image with PDHG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the gradient\n",
    "\n",
    "<!-- The gradient is a Laplacian ?\n",
    "\n",
    "There are $x$ gradient and $y$ gradient -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "class GradOperators(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def diff_kernel(ndim, mode):\n",
    "        if mode == \"doublecentral\":\n",
    "            kern = torch.tensor((-1, 0, 1))\n",
    "        elif mode == \"central\":\n",
    "            kern = torch.tensor((-1, 0, 1)) / 2\n",
    "        elif mode == \"forward\":\n",
    "            kern = torch.tensor((0, -1, 1))\n",
    "        elif mode == \"backward\":\n",
    "            kern = torch.tensor((-1, 1, 0))\n",
    "        else:\n",
    "            raise ValueError(f\"mode should be one of (central, forward, backward, doublecentral), not {mode}\")\n",
    "        kernel = torch.zeros(ndim, 1, *(ndim * (3,)))\n",
    "        for i in range(ndim):\n",
    "            idx = tuple([i, 0, *(i * (1,)), slice(None), *((ndim - i - 1) * (1,))])\n",
    "            kernel[idx] = kern\n",
    "        return kernel\n",
    "\n",
    "    def __init__(self, dim:int=2, mode:str=\"doublecentral\", padmode:str = \"circular\"):\n",
    "        \"\"\"\n",
    "        An Operator for finite Differences / Gradients\n",
    "        Implements the forward as apply_G and the adjoint as apply_GH.\n",
    "        \n",
    "        Args:\n",
    "            dim (int, optional): Dimension. Defaults to 2.\n",
    "            mode (str, optional): one of doublecentral, central, forward or backward. Defaults to \"doublecentral\".\n",
    "            padmode (str, optional): one of constant, replicate, circular or refelct. Defaults to \"circular\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"kernel\", self.diff_kernel(dim, mode), persistent=False)\n",
    "        self._dim = dim\n",
    "        self._conv = (torch.nn.functional.conv1d, torch.nn.functional.conv2d, torch.nn.functional.conv3d)[dim - 1]\n",
    "        self._convT = (torch.nn.functional.conv_transpose1d, torch.nn.functional.conv_transpose2d, torch.nn.functional.conv_transpose3d)[dim - 1]\n",
    "        self._pad = partial(torch.nn.functional.pad, pad=2 * dim * (1,), mode=padmode)\n",
    "        if mode == 'central':\n",
    "            self._norm = (self.dim) ** (1 / 2)\n",
    "        else:\n",
    "            self._norm = (self.dim * 4) ** (1 / 2)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim\n",
    "    \n",
    "    def apply_G(self, x):\n",
    "        \"\"\"\n",
    "        Forward\n",
    "        \"\"\"\n",
    "        if x.is_complex():\n",
    "            xr = torch.view_as_real(x).moveaxis(-1, 0)\n",
    "        else:\n",
    "            xr = x\n",
    "        xr = xr.reshape(-1, 1, *x.shape[-self.dim :])\n",
    "        xp = self._pad(xr)\n",
    "        y = self._conv(xp, weight=self.kernel, bias=None, padding=0)\n",
    "        if x.is_complex():\n",
    "            y = y.reshape(2, *x.shape[: -self.dim], self.dim, *x.shape[-self.dim :])\n",
    "            y = torch.view_as_complex(y.moveaxis(0, -1).contiguous())\n",
    "        else:\n",
    "            y = y.reshape(*x.shape[0 : -self.dim], self.dim, *x.shape[-self.dim :])\n",
    "\n",
    "        del x, xr, xp\n",
    "\n",
    "        return y\n",
    "\n",
    "    def apply_GH(self, x):\n",
    "        \"\"\"\n",
    "        Adjoint\n",
    "        \"\"\"\n",
    "        if x.is_complex():\n",
    "            xr = torch.view_as_real(x).moveaxis(-1, 0)\n",
    "        else:\n",
    "            xr = x\n",
    "        xr = xr.reshape(-1, self.dim, *x.shape[-self.dim :])\n",
    "        xp = self._pad(xr)\n",
    "        y = self._convT(xp, weight=self.kernel, bias=None, padding=2)\n",
    "        if x.is_complex():\n",
    "            y = y.reshape(2, *x.shape[: -self.dim - 1], *x.shape[-self.dim :])\n",
    "            y = torch.view_as_complex(y.moveaxis(0, -1).contiguous())\n",
    "        else:\n",
    "            y = y.reshape(*x.shape[: -self.dim - 1], *x.shape[-self.dim :])\n",
    "\n",
    "        del x, xr, xp\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def apply_GHG(self, x):\n",
    "        if x.is_complex():\n",
    "            xr = torch.view_as_real(x).moveaxis(-1, 0)\n",
    "        else:\n",
    "            xr = x\n",
    "        xr = xr.reshape(-1, 1, *x.shape[-self.dim :])\n",
    "        xp = self._pad(xr)\n",
    "        tmp = self._conv(xp, weight=self.kernel, bias=None, padding=0)\n",
    "        tmp = self._pad(tmp)\n",
    "        y = self._convT(tmp, weight=self.kernel, bias=None, padding=2)\n",
    "        if x.is_complex():\n",
    "            y = y.reshape(2, *x.shape)\n",
    "            y = torch.view_as_complex(y.moveaxis(0, -1).contiguous())\n",
    "        else:\n",
    "            y = y.reshape(*x.shape)\n",
    "\n",
    "        del x, xr, xp, tmp\n",
    "\n",
    "        return y\n",
    "\n",
    "    def forward(self, x, direction=1):\n",
    "        if direction>0:\n",
    "            return self.apply_G(x)\n",
    "        elif direction<0:\n",
    "            return self.apply_GH(x)\n",
    "        else:\n",
    "            return self.apply_GHG(x)\n",
    "\n",
    "    @property\n",
    "    def normGHG(self):\n",
    "        return self._norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function for PDHG: Clip act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "class ClipAct(nn.Module):\n",
    "    def forward(self, x, threshold):\n",
    "        return clipact(x, threshold)\n",
    "\n",
    "\n",
    "def clipact(x, threshold):\n",
    "    is_complex = x.is_complex()\n",
    "    if is_complex:\n",
    "        x = torch.view_as_real(x)\n",
    "        threshold = threshold.unsqueeze(-1)\n",
    "    x = torch.clamp(x, -threshold, threshold)\n",
    "    if is_complex:\n",
    "        x = torch.view_as_complex(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only PDHG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, running PDHG with T large (many iterations in PDGH) will make GPU memory full?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "def reconstruct_with_PDHG(\n",
    "        x_dynamic_image_tensor_5D, lambda_reg, T, \n",
    "        # lambda_reg_container=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reconstructs the image using the PDHG algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        dynamic_image_tensor_5D: The (noisy) (dynamic) image tensor.\n",
    "        Size of the tensor: (`patches`, `channels`, `Nx`, `Ny`, `Nt`) where\n",
    "        \n",
    "        - `patches`: number of patches\n",
    "        - `channels`: number of (colour) channels\n",
    "        - `Nx`: number of pixels in x\n",
    "        - `Ny`: number of pixels in y\n",
    "        - `Nt`: number of time steps (frames)\n",
    "\n",
    "        lambda_reg: The regularization parameter. Can be a scalar or a tensor of suitable size.\n",
    "        T: Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        The reconstructed image tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    dim = 3\n",
    "    patches, channels, Nx, Ny, Nt = x_dynamic_image_tensor_5D.shape\n",
    "    \n",
    "    assert channels == 1, \"Only grayscale images are supported.\"\n",
    "\n",
    "    device = x_dynamic_image_tensor_5D.device\n",
    "\n",
    "    # starting values\n",
    "    xbar = x_dynamic_image_tensor_5D.clone()\n",
    "    x0 = x_dynamic_image_tensor_5D.clone()\n",
    "    xnoisy = x_dynamic_image_tensor_5D.clone()\n",
    "\n",
    "    # dual variable\n",
    "    p = x_dynamic_image_tensor_5D.clone()\n",
    "    q = torch.zeros(patches, dim, Nx, Ny, Nt, dtype=x_dynamic_image_tensor_5D.dtype).to(device)\n",
    "\n",
    "    # operator norms\n",
    "    op_norm_AHA = torch.sqrt(torch.tensor(1.0))\n",
    "    op_norm_GHG = torch.sqrt(torch.tensor(12.0))\n",
    "    # operator norm of K = [A, \\nabla]\n",
    "    # https://iopscience.iop.org/article/10.1088/0031-9155/57/10/3065/pdf,\n",
    "    # see page 3083\n",
    "    L = torch.sqrt(op_norm_AHA**2 + op_norm_GHG**2)\n",
    "\n",
    "    tau = nn.Parameter(\n",
    "        torch.tensor(10.0), requires_grad=True\n",
    "    )  # starting value approximately  1/L\n",
    "    sigma = nn.Parameter(\n",
    "        torch.tensor(10.0), requires_grad=True\n",
    "    )  # starting value approximately  1/L\n",
    "\n",
    "    # theta should be in \\in [0,1]\n",
    "    theta = nn.Parameter(\n",
    "        torch.tensor(10.0), requires_grad=True\n",
    "    )  # starting value approximately  1\n",
    "\n",
    "    # sigma, tau, theta\n",
    "    sigma = (1 / L) * torch.sigmoid(sigma)  # \\in (0,1/L)\n",
    "    tau = (1 / L) * torch.sigmoid(tau)  # \\in (0,1/L)\n",
    "    theta = torch.sigmoid(theta)  # \\in (0,1)\n",
    "\n",
    "    GradOps = GradOperators(\n",
    "        dim=dim, \n",
    "        mode=\"forward\", padmode=\"circular\")\n",
    "    clip_act = ClipAct()\n",
    "    # Algorithm 2 - Unrolled PDHG algorithm (page 18)\n",
    "    # TODO: In the paper, L is one of the inputs but not used anywhere in the pseudo code???\n",
    "    for kT in range(T):\n",
    "        # update p\n",
    "        p =  (p + sigma * (xbar - xnoisy) ) / (1. + sigma)\n",
    "        # update q\n",
    "        q = clip_act(q + sigma * GradOps.apply_G(xbar), lambda_reg)\n",
    "\n",
    "        x1 = x0 - tau * p - tau * GradOps.apply_GH(q)\n",
    "\n",
    "        if kT != T - 1:\n",
    "            # update xbar\n",
    "            xbar = x1 + theta * (x1 - x0)\n",
    "            x0 = x1\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    del x_dynamic_image_tensor_5D\n",
    "    del xbar, x0, xnoisy\n",
    "    del p, q\n",
    "    del op_norm_AHA, op_norm_GHG, L\n",
    "    del tau, sigma, theta\n",
    "    del GradOps\n",
    "    del clip_act\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # if lambda_reg_container is not None:\n",
    "    #     assert isinstance(lambda_reg_container, list), f\"lambda_reg_container should be a list, not {type(lambda_reg_container)}\"\n",
    "    #     lambda_reg_container.append(lambda_reg) # For comparison\n",
    "\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reconstruct_with_PDHG():\n",
    "    if DISABLING_TESTS: return\n",
    "    # for rgb_image in load_images_SIDD([\"0083\"], False):\n",
    "    for rgb_image in load_images_chest_xray(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL\", [0]):\n",
    "        grayscale_image = convert_to_grayscale(rgb_image)\n",
    "        grayscale_image = crop_to_square_and_resize(grayscale_image, 512)\n",
    "        image_numpy = convert_to_numpy(grayscale_image)\n",
    "\n",
    "        image_tensor_4D = convert_to_tensor_4D(image_numpy)\n",
    "        print(f\"Image tensor size: {image_tensor_4D.size()}\")\n",
    "        assert len(image_tensor_4D.size()) == 4, \"The image should be 4D\"\n",
    "        plt.imshow(image_tensor_4D.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "        TEST_SIGMA = 0.5  # Relatively high noise\n",
    "        noisy_image_tensor_4D = add_noise(image_tensor_4D, sigma=TEST_SIGMA)\n",
    "        print(f\"PSNR of constant noise image: {PSNR(image_tensor_4D, noisy_image_tensor_4D):.2f} dB\")\n",
    "        print(f\"SSIM of constant noise image: {SSIM(image_tensor_4D.squeeze(0).squeeze(-1), noisy_image_tensor_4D.squeeze(0).squeeze(-1)):.2f}\")\n",
    "        plt.imshow(noisy_image_tensor_4D.squeeze(0).to(\"cpu\"), cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "        TEST_LAMBDA = 0.04\n",
    "        pdhg_input_tensor_5D = noisy_image_tensor_4D.unsqueeze(0)\n",
    "        print(f\"PDHG input size: {pdhg_input_tensor_5D.size()}\")\n",
    "        assert len(pdhg_input_tensor_5D.size()) == 5, \"The input for PDHG should be 5D\"\n",
    "        denoised_image_tensor_5D = reconstruct_with_PDHG(\n",
    "            pdhg_input_tensor_5D, \n",
    "            lambda_reg=TEST_LAMBDA, \n",
    "            T=128)\n",
    "        \n",
    "        denoised_image_tensor_5D = torch.clamp(denoised_image_tensor_5D, 0, 1) # Clip the values to 0 and 1\n",
    "        psnr_value_denoised = PSNR(image_tensor_4D, denoised_image_tensor_5D.squeeze(0))\n",
    "        print(f\"PSNR of reconstructed image: {psnr_value_denoised:.2f} dB\")\n",
    "        denoised_image_numpy_3D = denoised_image_tensor_5D.squeeze(0).squeeze(0).to(\"cpu\").detach().numpy()\n",
    "        plt.imshow(denoised_image_numpy_3D, cmap='gray')\n",
    "        plt.show();\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\"\"\n",
    "In this example, a lot of noise has been applied to the original image. The PDHG algorithm tries to reconstruct the image from the noisy image. It did remove some noise and improved the PSNR value. However, the quality has been degraded significantly. We will see whether we can improve this by learning a set of parameters map.\n",
    "\"\"\")\n",
    "    \n",
    "    # The lambda parameter is the regularization parameter. The higher the lambda, the more the regularization. The T parameter is the number of iterations. The higher the T, the more the iterations. The PSNR value is the Peak Signal to Noise Ratio. The higher the PSNR, the better the reconstruction.\n",
    "\n",
    "test_reconstruct_with_PDHG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Full Architecture\n",
    "\n",
    "<!-- UNET to PDHG\n",
    "\n",
    "The whole architecture can be seen as unsupervised: The data only contains (clean) images.\n",
    "\n",
    "The whole model: Input is an image. Output is also an image.\n",
    "\n",
    "The UNET actually only outputs the regularisation parameter map. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "class DynamicImageStaticPrimalDualNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        T=128,\n",
    "        cnn_block=None,\n",
    "        mode=\"lambda_cnn\",\n",
    "        up_bound=0,\n",
    "        phase=\"training\",\n",
    "    ):\n",
    "        # print(f\"Running: {DynamicImageStaticPrimalDualNN.__name__}\")\n",
    "        super(DynamicImageStaticPrimalDualNN, self).__init__()\n",
    "\n",
    "        # gradient operators and clipping function\n",
    "        dim = 3\n",
    "        self.GradOps = GradOperators(dim, mode=\"forward\", padmode=\"circular\")\n",
    "\n",
    "        # operator norms\n",
    "        self.op_norm_AHA = torch.sqrt(torch.tensor(1.0))\n",
    "        self.op_norm_GHG = torch.sqrt(torch.tensor(12.0))\n",
    "        # operator norm of K = [A, \\nabla]\n",
    "        # https://iopscience.iop.org/article/10.1088/0031-9155/57/10/3065/pdf,\n",
    "        # see page 3083\n",
    "        self.L = torch.sqrt(self.op_norm_AHA**2 + self.op_norm_GHG**2)\n",
    "\n",
    "        # function for projecting\n",
    "        self.ClipAct = ClipAct()\n",
    "\n",
    "        if mode == \"lambda_xyt\":\n",
    "            # one single lambda for x,y and t\n",
    "            self.lambda_reg = nn.Parameter(torch.tensor([-1.5]), requires_grad=True)\n",
    "\n",
    "        elif mode == \"lambda_xy_t\":\n",
    "            # one (shared) lambda for x,y and one lambda for t\n",
    "            self.lambda_reg = nn.Parameter(\n",
    "                torch.tensor([-4.5, -1.5]), requires_grad=True\n",
    "            )\n",
    "\n",
    "        elif mode == \"lambda_cnn\":\n",
    "            # the CNN-block to estimate the lambda regularization map\n",
    "            # must be a CNN yielding a two-channeld output, i.e.\n",
    "            # one map for lambda_cnn_xy and one map for lambda_cnn_t\n",
    "            self.cnn = cnn_block    # NOTE: This is actually the UNET!!! (At least in this project)\n",
    "            self.up_bound = torch.tensor(up_bound)\n",
    "\n",
    "        # number of terations\n",
    "        self.T = T\n",
    "        self.mode = mode\n",
    "\n",
    "        # constants depending on the operators\n",
    "        self.tau = nn.Parameter(\n",
    "            torch.tensor(10.0), requires_grad=True\n",
    "        )  # starting value approximately  1/L\n",
    "        self.sigma = nn.Parameter(\n",
    "            torch.tensor(10.0), requires_grad=True\n",
    "        )  # starting value approximately  1/L\n",
    "\n",
    "        # theta should be in \\in [0,1]\n",
    "        self.theta = nn.Parameter(\n",
    "            torch.tensor(10.0), requires_grad=True\n",
    "        )  # starting value approximately  1\n",
    "\n",
    "        # distinguish between training and test phase;\n",
    "        # during training, the input is padded using \"reflect\" padding, because\n",
    "        # patches are used by reducing the number of temporal points;\n",
    "        # while testing, \"reflect\" padding is used in x,y- direction, while\n",
    "        # circular padding is used in t-direction\n",
    "        self.phase = phase\n",
    "\n",
    "    def get_lambda_cnn(self, x):\n",
    "        # padding\n",
    "        # arbitrarily chosen, maybe better to choose it depending on the\n",
    "        # receptive field of the CNN or so;\n",
    "        # seems to be important in order not to create \"holes\" in the\n",
    "        # lambda_maps in t-direction\n",
    "        npad_xy = 4\n",
    "        # npad_t = 8\n",
    "        npad_t = 0 # TODO: Time dimension should not be necessary for single image input.\n",
    "        # I changed the npad_t to 0 so that I can run on single image input without change the 3D type config. It seems that the number of frames must be greater than npad_t?\n",
    "\n",
    "        pad = (npad_t, npad_t, npad_xy, npad_xy, npad_xy, npad_xy)\n",
    "\n",
    "        if self.phase == \"training\":\n",
    "            x = F.pad(x, pad, mode=\"reflect\")\n",
    "\n",
    "        elif self.phase == \"testing\":\n",
    "            pad_refl = (0, 0, npad_xy, npad_xy, npad_xy, npad_xy)\n",
    "            pad_circ = (npad_t, npad_t, 0, 0, 0, 0)\n",
    "\n",
    "            x = F.pad(x, pad_refl, mode=\"reflect\")\n",
    "            x = F.pad(x, pad_circ, mode=\"circular\")\n",
    "\n",
    "        # estimate parameter map\n",
    "        lambda_cnn = self.cnn(x) # NOTE: The cnn is actually the UNET block!!! (At least in this project)\n",
    "\n",
    "        # crop\n",
    "        neg_pad = tuple([-pad[k] for k in range(len(pad))])\n",
    "        lambda_cnn = F.pad(lambda_cnn, neg_pad)\n",
    "\n",
    "        # double spatial map and stack\n",
    "        lambda_cnn = torch.cat((lambda_cnn[:, 0, ...].unsqueeze(1), lambda_cnn), dim=1)\n",
    "\n",
    "        # constrain map to be striclty positive; further, bound it from below\n",
    "        if self.up_bound > 0:\n",
    "            # constrain map to be striclty positive; further, bound it from below\n",
    "            lambda_cnn = self.up_bound * self.op_norm_AHA * torch.sigmoid(lambda_cnn)\n",
    "        else:\n",
    "            lambda_cnn = 0.1 * self.op_norm_AHA * F.softplus(lambda_cnn)\n",
    "\n",
    "        del pad\n",
    "        del x\n",
    "        del neg_pad\n",
    "\n",
    "        return lambda_cnn\n",
    "\n",
    "    def forward(\n",
    "            self, x, lambda_map=None, \n",
    "            # lambda_reg_container=None,\n",
    "    ):\n",
    "        if lambda_map is None:\n",
    "            # estimate lambda reg from the image\n",
    "            lambda_reg = self.get_lambda_cnn(x)\n",
    "        else:\n",
    "            lambda_reg = lambda_map\n",
    "\n",
    "        # if lambda_reg_container is not None:\n",
    "        #     assert type(lambda_reg_container) == list, f\"lambda_reg_container should be a list, not {type(lambda_reg_container)}\"\n",
    "        #     lambda_reg_container.append(lambda_reg) # For comparison\n",
    "\n",
    "        x.to(DEVICE)\n",
    "        x1 = reconstruct_with_PDHG(x, lambda_reg, self.T)\n",
    "\n",
    "        del lambda_reg\n",
    "        del x\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Data loading class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "class DynamicImageStaticDenoisingDataset(Dataset):\n",
    "\t\n",
    "\tdef __init__(\n",
    "\t\tself, \n",
    "\t\tdata_path: str, \n",
    "\t\tids: list,\n",
    "\t\t# scale_factor = 0.5, \n",
    "\t\t# patches_size = None,\n",
    "\t\t# strides= None,\n",
    "\t\tresize_square = 120,\n",
    "\t\tsigma = (0.1, 0.5),  \n",
    "\t\tdevice: str = \"cuda\"\n",
    "\t):\n",
    "\t\tself.device = device\n",
    "\t\t# self.scale_factor = scale_factor\n",
    "\t\tself.resize_square = resize_square\n",
    "\n",
    "\t\txray_images = load_images_chest_xray(data_path, ids)\n",
    "\n",
    "\t\txf_list = []\n",
    "\t\tfor image in xray_images:\n",
    "\t\t\timage = crop_to_square_and_resize(image, self.resize_square)\n",
    "\t\t\timage = image.convert('L') #convert to grey_scale\n",
    "\t\t\timage_data = np.asarray(image)\n",
    "\t\t\txf = torch.tensor(image_data, dtype=torch.float)\n",
    "\t\t\t# Assume image is in [0, 255] range\n",
    "\t\t\txf = xf / 255\n",
    "\t\t\tassert len(xf.size()) == 2, f\"Expected 2D tensor, got {xf.size()}\"\n",
    "\t\t\txf = xf.unsqueeze(0) # Add channel dimension\n",
    "\t\t\txf = xf.unsqueeze(-1) # Add time dimension. TODO: For legacy dynamic image code only. Will remove later.\n",
    "\t\t\txf_list.append(xf)\n",
    "\t\txf = torch.stack(xf_list, dim=0) # will have shape (mb, 1, Nx, Ny, Nt), where mb denotes the number of patches\n",
    "\t\tassert len(xf.size()) == 5, f\"Expected 5D tensor, got {xf.size()}\"\n",
    "\t\tassert xf.size(1) == 1, f\"Expected 1 channel, got {xf.size(1)}\"\n",
    "\t\tassert xf.size(2) == self.resize_square, f\"Expected width (Nx) of {self.resize_square}, got {xf.size(-3)}\"\n",
    "\t\tassert xf.size(3) == self.resize_square, f\"Expected height (Ny) of {self.resize_square}, got {xf.size(-2)}\"\n",
    "\t\tassert xf.size(4) == 1, f\"Expected 1 time step, got {xf.size(-1)}\"\n",
    "\n",
    "\t\t#create temporal TV vector to detect which patches contain the most motion\n",
    "\t\txf_patches_tv = (xf[...,1:] - xf[...,:-1]).pow(2).sum(dim=[1,2,3,4]) #contains the TV for all patches\n",
    "\t\t\n",
    "\t\t#normalize to 1 to have a probability vector\n",
    "\t\txf_patches_tv /= torch.sum(xf_patches_tv)\n",
    "\t\t\n",
    "\t\t#sort TV in descending order --> xfp_tv_ids[0] is the index of the patch with the most motion\n",
    "\t\tself.samples_weights = xf_patches_tv\n",
    "\n",
    "\t\t# # TODO: Investigate\n",
    "\t\t# # Change the values in samples_weights to be a range of integers from 0 to len(samples_weights)\n",
    "\t\t# # Unless I do this, when I run on a set of identical images, it will give me an error:\n",
    "\t\t# # RuntimeError: invalid multinomial distribution (encountering probability entry < 0)\n",
    "\t\t# self.samples_weights = torch.arange(len(self.samples_weights))\n",
    "\t\t\n",
    "\t\tself.xf = xf\n",
    "\t\tself.len = xf.shape[0]\n",
    "\t\t\n",
    "\t\tself.sigma_min = sigma[0]\n",
    "\t\tself.sigma_max = sigma[1]\n",
    "\t\t\n",
    "\t\t\t\n",
    "\tdef __getitem__(self, index):\n",
    "\n",
    "\t\tsigma = self.sigma_min + torch.rand(1) * ( self.sigma_max - self.sigma_min )\n",
    "\n",
    "\t\tx_noise = add_noise(self.xf[index], sigma)\n",
    "\n",
    "\t\tdel sigma\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\tx_noise.to(device=self.device),\n",
    "   \t\t\tself.xf[index].to(device=self.device)\n",
    "        )\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### UNET\n",
    "\n",
    "The specific UNET architecture we use has the following parts:\n",
    "\n",
    "...\n",
    "\n",
    "We use Leaky RELU instead of RELU or Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_parts.py as a reference\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels: int, out_channels: int, n_dimensions=3, activation=\"LeakyReLU\"):\n",
    "        super(DoubleConv, self).__init__()\n",
    "\n",
    "        def get_conv(in_channels, out_channels):\n",
    "            # 1-dimensional convolution is not supported\n",
    "            if n_dimensions == 3:\n",
    "                return nn.Conv3d(in_channels, out_channels, kernel_size=(3, 3, 1), padding=(1, 1, 0))\n",
    "            elif n_dimensions == 2:\n",
    "                return nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported number of dimensions: {n_dimensions}\")\n",
    "\n",
    "        def get_activation():\n",
    "            if activation == \"LeakyReLU\":\n",
    "                return nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "            elif activation == \"ReLU\":\n",
    "                return nn.ReLU(inplace=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            get_conv(in_channels, out_channels), get_activation(),\n",
    "            get_conv(out_channels, out_channels), get_activation())\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv_block(x)\n",
    "        \n",
    "\n",
    "class EncodeBlock3d(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels: int, n_dimensions=3,\n",
    "            activation=\"LeakyReLU\",\n",
    "            downsampling_kernel=(2, 2, 1), downsampling_mode=\"max\"):\n",
    "        super(EncodeBlock3d, self).__init__()\n",
    "\n",
    "        len = downsampling_kernel[0] # Assume kernel has shape (len, len, 1)\n",
    "        assert downsampling_kernel == (len, len, 1), f\"Expected a flat square kernel like {(len, len, 1)}, got {downsampling_kernel}\"\n",
    "        stride = (2, 2, 1) # Stride 2x2 to halve each side \n",
    "        padding = ((len-1)//2, (len-1)//2, 0) # Padding (len-1) // 2 to exactly halve each side \n",
    "        if downsampling_mode == \"max\":\n",
    "            self.pool = nn.MaxPool3d(\n",
    "                kernel_size=downsampling_kernel, stride=stride, padding=padding)\n",
    "        elif downsampling_mode == \"avg\":\n",
    "            self.pool = nn.AvgPool3d(\n",
    "                kernel_size=downsampling_kernel, stride=stride, padding=padding)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling method: {downsampling_mode}\")\n",
    "\n",
    "        self.double_conv = DoubleConv(in_channels, in_channels * 2, n_dimensions, activation=activation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.pool(x)\n",
    "        x = self.double_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class DecodeBlock3d(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels: int, n_dimensions=3, \n",
    "            activation=\"LeakyReLU\",\n",
    "            upsampling_kernel=(2, 2, 1), upsampling_mode=\"linear_interpolation\"):\n",
    "        super(DecodeBlock3d, self).__init__()\n",
    "\n",
    "        if upsampling_mode == \"linear_interpolation\":\n",
    "            self.upsampling = nn.Sequential(\n",
    "                nn.Upsample(\n",
    "                    scale_factor=(2, 2, 1), # Assume the shape is (Nx, Ny, 1) where Nx is the image width and Ny is the image height.\n",
    "                    mode='trilinear', align_corners=True), # What difference does it make in the end if align_corners is True or False? Preserving symmetry?\n",
    "                # 1x1x1 convolution to reduce the number of channels while keeping the size the same\n",
    "                nn.Conv3d(\n",
    "                    in_channels, in_channels // 2, \n",
    "                    kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0))\n",
    "            )\n",
    "        elif upsampling_mode == \"transposed_convolution\":  \n",
    "            len = upsampling_kernel[0] # Assume kernel has shape (len, len, 1)\n",
    "            assert upsampling_kernel == (len, len, 1), f\"Expected a flat square kernel like {(len, len, 1)}, got {upsampling_kernel}\"\n",
    "            stride = (2, 2, 1) # Stride 2x2 to double each side \n",
    "            padding = ((len-1)//2, (len-1)//2, 0) # Padding (len-1) // 2 to exactly double each side    \n",
    "            self.upsampling = nn.ConvTranspose3d(\n",
    "                in_channels, in_channels // 2, \n",
    "                kernel_size=upsampling_kernel, stride=stride, padding=padding, \n",
    "                output_padding=padding # TODO: Should this be the same as padding?\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported upsampling method: {upsampling_mode}\")\n",
    "        \n",
    "        self.double_conv = DoubleConv(in_channels, in_channels // 2, n_dimensions, activation=activation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_encoder_output: torch.Tensor):\n",
    "        x = self.upsampling(x)\n",
    "        x = torch.cat([x_encoder_output, x], dim=1)   # skip-connection. No cropping since we ensure that the size is the same.\n",
    "        x = self.double_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class UNet3d(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=1, out_channels=2, init_filters=32, n_blocks=3,\n",
    "            activation=\"LeakyReLU\",\n",
    "            downsampling_kernel=(2, 2, 1), downsampling_mode=\"max\",\n",
    "            upsampling_kernel=(2, 2, 1), upsampling_mode=\"linear_interpolation\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Assume that input is 5D tensor of shape (batch_size, channels, Nx, Ny, Nt)\n",
    "        where Nx is the image width and Ny is the image height.\n",
    "        Assume that batch_size = 1, channels = 1, Nx = Ny (square image), Nt = 1 (static image).\n",
    "        NOTE: The convention used in pytorch documentation is (batch_size, channels, Nt, Ny, Nx).\n",
    "        \"channels\" is equivalent to the number of filters or features.\n",
    "\n",
    "        Our paper (figure 2):\n",
    "            - in_channels = 1\n",
    "            - out_channels = 2\n",
    "            - init_filters = 32\n",
    "            - n_blocks = 3\n",
    "            - pooling: max pooling 2x2\n",
    "            - pool padding = 1\n",
    "                - 1 padding will keep the size of the \"image\" the same after each convolution. The skip-connection will NOT crop the encoder's output.\n",
    "            - upsampling kernel: 2x2 ?\n",
    "            - up_mode: linear interpolation\n",
    "\n",
    "        U-Net paper (2015, Olaf Ronneberger https://arxiv.org/abs/1505.04597):\n",
    "            - in_channels = 1\n",
    "            - out_channels = 2\n",
    "            - init_filters = 64\n",
    "            - n_blocks = 4\n",
    "            - pooling: max pooling 2x2\n",
    "            - pool padding = 0\n",
    "                - 0 padding will reduce the size of the \"image\" by 2 in each dimension after each convolution. The skip-connection will have to crop the encoder's output to match the decoder's input.\n",
    "            - upsampling kernel: 2x2\n",
    "            - up_mode: ? (linear interpolation or transposed convolution)\n",
    "        \"\"\"\n",
    "        super(UNet3d, self).__init__()\n",
    "        \n",
    "        self.c0x0 = DoubleConv( # TODO: Find a better name\n",
    "            in_channels=in_channels, \n",
    "            out_channels=init_filters,\n",
    "            activation=activation\n",
    "        )\n",
    "        self.encoder = nn.ModuleList([\n",
    "            EncodeBlock3d(\n",
    "                in_channels=init_filters * 2**i,\n",
    "                activation=activation,\n",
    "                downsampling_kernel=downsampling_kernel,\n",
    "                downsampling_mode=downsampling_mode\n",
    "            ) for i in range(n_blocks)\n",
    "        ])\n",
    "        self.decoder = nn.ModuleList([\n",
    "            DecodeBlock3d(\n",
    "                in_channels=init_filters * 2**(n_blocks-i),\n",
    "                activation=activation, \n",
    "                upsampling_kernel=upsampling_kernel,\n",
    "                upsampling_mode=upsampling_mode\n",
    "            ) for i in range(n_blocks)\n",
    "        ])\n",
    "        # 1x1x1 convo\n",
    "        self.c1x1 = nn.Conv3d( # TODO: Find a better name\n",
    "            in_channels=init_filters,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Assume that x is 5D tensor of shape (batch_size, channels, Nx, Ny, Nt)\n",
    "        # where Nx is the image width and Ny is the image height.\n",
    "        # Aslo assume that batch_size = 1, channels = 1, Nx = Ny (square image), Nt = 1 (static image).\n",
    "        # NOTE: The convention used in pytorch documentation is (batch_size, channels, Nt, Ny, Nx).\n",
    "        assert len(x.size()) == 5, f\"Expected 5D tensor, got {x.size()}\"\n",
    "        batch_size, channels, Nx, Ny, Nt = x.size()\n",
    "        assert channels == 1, f\"Expected 1 channel, got {channels}\" # TODO: Allow multiple channels (colour images)\n",
    "        assert Nx == Ny, f\"Expected square image, got ({Nx}, {Ny})\" # TODO: Allow non-square images\n",
    "        assert Nt == 1, f\"Expected 1 time step, got {Nt}\" # TODO: Allow multiple time steps (dynamic images, video)\n",
    "        assert batch_size == 1, f\"Expected batch size 1, got {batch_size}\" # TODO: Might train with larger batch size\n",
    "\n",
    "        n_blocks = len(self.encoder)\n",
    "        assert Nx >= 2**n_blocks, f\"Expected width (Nx) of at least {2**n_blocks}, got {Nx}\"\n",
    "        assert Ny >= 2**n_blocks, f\"Expected height (Ny) of at least {2**n_blocks}, got {Ny}\"\n",
    "\n",
    "        x = self.c0x0(x)\n",
    "\n",
    "        encoder_outputs = []\n",
    "        for i, enc_block in enumerate(self.encoder):\n",
    "            encoder_outputs.append(x)\n",
    "            x = enc_block(x)\n",
    "        for i, dec_block in enumerate(self.decoder):\n",
    "            x = dec_block(x, encoder_outputs[-i-1]) # skip-connection inside\n",
    "            \n",
    "        x = self.c1x1(x)\n",
    "\n",
    "        for enc_output in encoder_outputs:\n",
    "            del enc_output\n",
    "        del encoder_outputs\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_and_clear_cuda(expected, actual):\n",
    "    try:\n",
    "        assert expected == actual\n",
    "    except AssertionError:\n",
    "        print(f\"!!! ERROR !!! Expected: {expected}, got {actual}\")\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "def test_unet_3d():  \n",
    "    if DISABLING_TESTS: return  \n",
    "    input_tensor = torch.randn(1, 1, 512, 512, 1)  # batch size of 1, 1 channel, 512x512x1 volume\n",
    "    \n",
    "    config = get_config()\n",
    "\n",
    "    # Example usage\n",
    "    model = UNet3d(\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        init_filters=32,\n",
    "        n_blocks=config[\"n_blocks\"],\n",
    "        activation=\"ReLU\",\n",
    "        downsampling_kernel=(2, 2, 1),\n",
    "        downsampling_mode=config[\"downsampling_mode\"],\n",
    "        upsampling_kernel=(2, 2, 1),\n",
    "        upsampling_mode=config[\"upsampling_mode\"],\n",
    "    )\n",
    "    output = model(input_tensor)\n",
    "    print(f\"UNet output shape: {output.shape}\")\n",
    "    # assert_and_clear_cuda((1, 2, 512, 512, 1), output.shape)\n",
    "    assert_and_clear_cuda((1, 1, 512, 512, 1), output.shape)\n",
    "\n",
    "\n",
    "    conv_3d = nn.Conv3d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "    conv_3d_output = conv_3d(input_tensor)\n",
    "    print(f\"Conv3d output shape: {conv_3d_output.shape}\")\n",
    "    assert_and_clear_cuda((1, 64, 512, 512, 1), conv_3d_output.shape)\n",
    "\n",
    "\n",
    "    double_conv_3d = DoubleConv(64, 128)\n",
    "    double_conv_output = double_conv_3d(conv_3d_output)\n",
    "    print(f\"{DoubleConv.__name__} output shape: {double_conv_output.shape}\")\n",
    "    assert_and_clear_cuda((1, 128, 512, 512, 1), double_conv_output.shape)\n",
    "\n",
    "\n",
    "    max_3d = nn.MaxPool3d((3, 3, 1), stride=(2, 2, 1), padding=(1, 1, 0))\n",
    "    max_3d_output_1 = max_3d(input_tensor)\n",
    "    print(f\"MaxPool3d output 1 shape: {max_3d_output_1.shape}\")\n",
    "    assert_and_clear_cuda((1, 1, 256, 256, 1), max_3d_output_1.shape)\n",
    "\n",
    "    max_3d_input = torch.randn(1, 128, 512, 512, 1)\n",
    "    max_3d_output_2 = max_3d(max_3d_input)\n",
    "    print(f\"MaxPool3d output 2 shape: {max_3d_output_2.shape}\")\n",
    "    assert_and_clear_cuda((1, 128, 256, 256, 1), max_3d_output_2.shape)\n",
    "\n",
    "    conv_transpose_3d = nn.ConvTranspose3d(\n",
    "        1024, 512, \n",
    "        kernel_size=(3, 3, 1), \n",
    "        stride=(2, 2, 1), \n",
    "        padding=(1, 1, 0), \n",
    "        output_padding=(1, 1, 0)\n",
    "    )\n",
    "    conv_transpose_3d_input = torch.randn(1, 1024, 32, 32, 1)\n",
    "    conv_transpose_3d_output = conv_transpose_3d(conv_transpose_3d_input)\n",
    "    print(f\"ConvTranspose3d output shape: {conv_transpose_3d_output.shape}\")\n",
    "    assert_and_clear_cuda((1, 512, 64, 64, 1), conv_transpose_3d_output.shape)\n",
    "\n",
    "\n",
    "    up_sample = nn.Upsample(\n",
    "        scale_factor=(2, 2, 1), \n",
    "        mode='trilinear', align_corners=True) # What difference does it make if align_corners is True or False?\n",
    "    up_sample_output = up_sample(input_tensor)\n",
    "    print(f\"Upsample output shape: {up_sample_output.shape}\")\n",
    "    assert_and_clear_cuda((1, 1, 1024, 1024, 1), up_sample_output.shape)\n",
    "                    \n",
    "\n",
    "    # # print(f\"\\n{model}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # # Delete the model and the output tensor\n",
    "    # del model\n",
    "    # del output\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "test_unet_3d()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from ...\n",
    "\n",
    "def get_datasets(config):\n",
    "    min_sigma = config[\"min_sigma\"]\n",
    "    max_sigma = config[\"max_sigma\"]\n",
    "    resize_square = config[\"resize_square\"]\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    train_num_samples = config[\"train_num_samples\"]\n",
    "    train_ids = list(range(0, train_num_samples))\n",
    "    dataset_train = DynamicImageStaticDenoisingDataset(\n",
    "        data_path=config[\"train_data_path\"],\n",
    "        ids=train_ids,\n",
    "        sigma=(min_sigma, max_sigma),\n",
    "        resize_square=resize_square,\n",
    "        # strides=[120, 120, 1],\n",
    "        # patches_size=[120, 120, 1],\n",
    "        # strides=[256, 256, 1], # stride < patch will allow overlapping patches, maybe good to blend the patches?\n",
    "        # strides=[512, 512, 1],\n",
    "        # patches_size=[512, 512, 1],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    val_num_samples = config[\"val_num_samples\"]\n",
    "    val_ids = list(range(0, val_num_samples))\n",
    "    dataset_valid = DynamicImageStaticDenoisingDataset(\n",
    "        data_path=config[\"val_data_path\"],\n",
    "        ids=val_ids,\n",
    "        sigma=(min_sigma, max_sigma),\n",
    "        resize_square=resize_square,\n",
    "        # strides=[120, 120, 1],\n",
    "        # patches_size=[120, 120, 1],\n",
    "        # strides=[256, 256, 1], # stride < patch will allow overlapping patches, maybe good to blend the patches?\n",
    "        # strides=[512, 512, 1],\n",
    "        # patches_size=[512, 512, 1],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    test_num_samples = config[\"test_num_samples\"]\n",
    "    test_ids = list(range(0, test_num_samples))\n",
    "    dataset_test = DynamicImageStaticDenoisingDataset(\n",
    "        data_path=config[\"test_data_path\"],\n",
    "        ids=test_ids,\n",
    "        sigma=(min_sigma, max_sigma),\n",
    "        resize_square=resize_square,\n",
    "        # strides=[120, 120, 1],\n",
    "        # patches_size=[120, 120, 1],\n",
    "        # strides=[256, 256, 1], # stride < patch will allow overlapping patches, maybe good to blend the patches?\n",
    "        # strides=[512, 512, 1],\n",
    "        # patches_size=[512, 512, 1],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(f\"Number of training samples: {len(dataset_train)}\")\n",
    "    print(f\"Number of validation samples: {len(dataset_valid)}\")\n",
    "    print(f\"Number of test samples: {len(dataset_test)}\")\n",
    "\n",
    "    return dataset_train, dataset_valid, dataset_test\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloaders(config):\n",
    "\n",
    "    dataset_train, dataset_valid, dataset_test = get_datasets(config)\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    device = config[\"device\"]\n",
    "    random_seed = config[\"random_seed\"]\n",
    "\n",
    "    # Create training dataloader\n",
    "    # train_sampler = WeightedRandomSampler(dataset_train.samples_weights, len(dataset_train.samples_weights))\n",
    "    dataloader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=batch_size, \n",
    "        # sampler=train_sampler,\n",
    "        generator=torch.Generator(device=device).manual_seed(random_seed),\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Create validation dataloader \n",
    "    # val_sampler = WeightedRandomSampler(dataset_valid.samples_weights, len(dataset_valid.samples_weights))\n",
    "    dataloader_valid = torch.utils.data.DataLoader(\n",
    "        dataset_valid, batch_size=batch_size, \n",
    "        # sampler=val_sampler,\n",
    "        generator=torch.Generator(device=device).manual_seed(random_seed),\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # Create test dataloader\n",
    "    test_sampler = WeightedRandomSampler(dataset_test.samples_weights, len(dataset_test.samples_weights))\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=batch_size, \n",
    "        # sampler=test_sampler,\n",
    "        generator=torch.Generator(device=device).manual_seed(random_seed),\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        dataloader_train, \n",
    "        dataloader_valid, \n",
    "        dataloader_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataloader():\n",
    "    if DISABLING_TESTS: return\n",
    "    dataloader_train, dataloader_valid, dataloader_test = get_dataloaders(get_config())\n",
    "    for i, (x, y) in enumerate(dataloader_train):\n",
    "        print(f\"Batch {i+1}\")\n",
    "        print(f\"x size: {x.size()}\")\n",
    "        print(f\"y size: {y.size()}\")\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(x.squeeze(0).squeeze(0).squeeze(-1).to(\"cpu\"), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(y.squeeze(0).squeeze(0).squeeze(-1).to(\"cpu\"), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show();\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "    del dataloader_train\n",
    "    del dataloader_valid\n",
    "    del dataloader_test\n",
    "\n",
    "test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "# def train_iteration(optimizer, model, loss_func, sample):\n",
    "#     optimizer.zero_grad(set_to_none=True)  # Zero your gradients for every batch!\n",
    "#     noisy_image, clean_image = sample\n",
    "#     denoised_image = model(noisy_image)\n",
    "#     loss = loss_func(denoised_image, clean_image)\n",
    "#     loss.backward()\n",
    "    \n",
    "#     if loss.item() != loss.item():\n",
    "#         raise ValueError(\"NaN returned by loss function...\")\n",
    "\n",
    "#     optimizer.step()\n",
    "\n",
    "#     denoised_image = denoised_image.squeeze(0).squeeze(0).squeeze(-1)\n",
    "#     clean_image = clean_image.squeeze(0).squeeze(0).squeeze(-1)\n",
    "\n",
    "#     psnr = PSNR(denoised_image, clean_image)\n",
    "#     ssim = SSIM(denoised_image, clean_image)\n",
    "\n",
    "#     return loss.item(), psnr, ssim\n",
    "\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, loss_func) -> float:\n",
    "    running_loss = 0.\n",
    "    running_psnr = 0.\n",
    "    running_ssim = 0.\n",
    "    num_batches = len(data_loader)\n",
    "    # for sample in tqdm(data_loader): # tqdm helps show a nice progress bar\n",
    "    for sample in data_loader:\n",
    "        # loss, psnr, ssim = train_iteration(optimizer, model, loss_func, sample)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)  # Zero your gradients for every batch! TODO: Why?\n",
    "        noisy_image_5d, clean_image_5d = sample\n",
    "        denoised_image_5d = model(noisy_image_5d)\n",
    "        loss = loss_func(denoised_image_5d, clean_image_5d)\n",
    "\n",
    "        loss.backward()\n",
    "        if loss.item() != loss.item():\n",
    "            raise ValueError(\"NaN returned by loss function...\")\n",
    "        optimizer.step()\n",
    "\n",
    "        denoised_image_2d = denoised_image_5d.squeeze(0).squeeze(0).squeeze(-1)\n",
    "        clean_image_2d = clean_image_5d.squeeze(0).squeeze(0).squeeze(-1)\n",
    "        psnr = PSNR(denoised_image_2d, clean_image_2d)\n",
    "        ssim = SSIM(denoised_image_2d, clean_image_2d)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_psnr += psnr\n",
    "        running_ssim += ssim\n",
    "\n",
    "        # Free up memory\n",
    "        del loss \n",
    "        del denoised_image_5d # Delete output of model\n",
    "        del denoised_image_2d # Delete auxiliary variable\n",
    "        del clean_image_2d # Delete auxiliary variable\n",
    "        del noisy_image_5d # Noisy image is generated each time so can delete it\n",
    "        del clean_image_5d # TODO: Is this a copy that I can delete or a reference to the original?\n",
    "\n",
    "    avg_loss = running_loss / num_batches\n",
    "    avg_psnr = running_psnr / num_batches\n",
    "    avg_ssim = running_ssim / num_batches\n",
    "\n",
    "    del running_loss\n",
    "    del running_psnr\n",
    "    del running_ssim\n",
    "    del num_batches\n",
    "\n",
    "    return avg_loss, avg_psnr, avg_ssim\n",
    "\n",
    "\n",
    "# def validate_iteration(model, loss_func, sample):\n",
    "#     noisy_image, clean_image = sample\n",
    "#     denoised_image = model(noisy_image)\n",
    "#     loss = loss_func(denoised_image, clean_image)\n",
    "#     denoised_image = denoised_image.squeeze(0).squeeze(0).squeeze(-1)\n",
    "#     clean_image = clean_image.squeeze(0).squeeze(0).squeeze(-1)\n",
    "\n",
    "#     psnr = PSNR(denoised_image, clean_image)\n",
    "#     ssim = SSIM(denoised_image, clean_image)\n",
    "\n",
    "#     return loss.item(), psnr, ssim\n",
    "\n",
    "\n",
    "def validate_epoch(model, data_loader, loss_func) -> float:\n",
    "    running_loss = 0.\n",
    "    running_psnr = 0.\n",
    "    running_ssim = 0.\n",
    "    num_batches = len(data_loader)\n",
    "    # for sample in tqdm(data_loader): # tqdm helps show a nice progress bar\n",
    "    for sample in data_loader:\n",
    "        # loss, psnr, ssim = validate_iteration(model, loss_func, sample)\n",
    "\n",
    "        noisy_image_5d, clean_image_5d = sample\n",
    "        denoised_image_5d = model(noisy_image_5d)\n",
    "        loss = loss_func(denoised_image_5d, clean_image_5d)\n",
    "\n",
    "        denoised_image_2d = denoised_image_5d.squeeze(0).squeeze(0).squeeze(-1)\n",
    "        clean_image_2d = clean_image_5d.squeeze(0).squeeze(0).squeeze(-1)\n",
    "        psnr = PSNR(denoised_image_2d, clean_image_2d)\n",
    "        ssim = SSIM(denoised_image_2d, clean_image_2d)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_psnr += psnr\n",
    "        running_ssim += ssim\n",
    "\n",
    "        # Free up memory\n",
    "        del loss \n",
    "        del denoised_image_5d # Delete output of model\n",
    "        del denoised_image_2d # Delete auxiliary variable\n",
    "        del clean_image_2d # Delete auxiliary variable\n",
    "        del noisy_image_5d # Noisy image is generated each time so can delete it\n",
    "        del clean_image_5d # TODO: Is this a copy that I can delete or a reference to the original?\n",
    "\n",
    "    avg_loss = running_loss / num_batches\n",
    "    avg_psnr = running_psnr / num_batches\n",
    "    avg_ssim = running_ssim / num_batches\n",
    "\n",
    "    del running_loss\n",
    "    del running_psnr\n",
    "    del running_ssim\n",
    "    del num_batches\n",
    "\n",
    "    return avg_loss, avg_psnr, avg_ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image_tensor_2D, image_name, folder_name):\n",
    "    image_tensor_2D = torch.clamp(image_tensor_2D, 0, 1) # Clip the values to 0 and 1\n",
    "    image_numpy = image_tensor_2D.to(\"cpu\").detach().numpy()\n",
    "    image_numpy_256 = image_numpy * 255\n",
    "    image_numpy_256_uint8 = image_numpy_256.astype(np.uint8)\n",
    "    image_to_save = Image.fromarray(image_numpy_256_uint8)  # TODO: Is there a shorter way to do this, similar to .convert(\"L\")?\n",
    "    image_to_save.save(f\"{folder_name}/{image_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_testcase():\n",
    "    CHEST_XRAY_BASE_DATA_PATH = \"../data/chest_xray\"\n",
    "    image = Image.open(f\"{CHEST_XRAY_BASE_DATA_PATH}/train/NORMAL/IM-0115-0001.jpeg\")\n",
    "    image = crop_to_square_and_resize(image, 512)\n",
    "\n",
    "    folder_name = \"testcases_tmp\"\n",
    "\n",
    "    image_name = \"chest_xray_clean\"\n",
    "    image.save(f\"{folder_name}/{image_name}.png\") \n",
    "\n",
    "    # Add noise to the image\n",
    "    noisy_image_tensor_4D = add_noise(convert_to_tensor_4D(convert_to_numpy(image)), sigma=0.5)\n",
    "    noisy_image_tensor_2D = noisy_image_tensor_4D.squeeze(0).squeeze(-1)\n",
    "    noisy_image_name = \"chest_xray_noisy\"\n",
    "    save_image(noisy_image_tensor_2D, \"chest_xray_noisy\", folder_name)\n",
    "\n",
    "    # Read the saved images\n",
    "    clean_image = Image.open(f\"{folder_name}/{image_name}.png\")\n",
    "    noisy_image = Image.open(f\"{folder_name}/{noisy_image_name}.png\")\n",
    "    \n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(clean_image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Clean Image\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(noisy_image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Noisy Image\")\n",
    "    plt.show();\n",
    "\n",
    "# make_testcase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Use wandb to log the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Use wandb to log the training process\n",
    "# !wandb login\n",
    "def init_wandb(config):\n",
    "    project_name = config[\"project\"]\n",
    "    os.environ['WANDB_NOTEBOOK_NAME'] = project_name\n",
    "    os.environ['WANDB_MODE'] = config[\"wandb_mode\"] # https://docs.wandb.ai/quickstart\n",
    "    wandb.login()\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=project_name,\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config=get_config(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def temp_log_to_files():\n",
    "#     model_states_dir = \"tmp_2/model-chest_xray-2024_06_06_05_51_27\"\n",
    "#     config = get_config()\n",
    "#     with open(f\"{model_states_dir}/config.json\", \"w\") as f:\n",
    "#         json.dump(config, f, indent=4)\n",
    "#     with open(f\"{model_states_dir}/config.yaml\", \"w\") as f:\n",
    "#         yaml.dump(config, f)\n",
    "#     with open(f\"{model_states_dir}/config.txt\", \"w\") as f:\n",
    "#         f.write(str(config))\n",
    "\n",
    "# def test_temp_log_to_files():\n",
    "#     temp_log_to_files()\n",
    "\n",
    "# test_temp_log_to_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://www.github.com/koflera/LearningRegularizationParameterMaps\n",
    "\n",
    "def start_training(config, pretrained_model_path=None, is_state_dict=False, start_epoch=0):\n",
    "    \n",
    "    dataloader_train, dataloader_valid, dataloader_test = get_dataloaders(config)\n",
    "\n",
    "    del dataloader_test # Not used for now\n",
    "\n",
    "    if pretrained_model_path is None or is_state_dict:\n",
    "        # Define CNN block\n",
    "        unet = UNet3d(\n",
    "            in_channels=config[\"in_channels\"],\n",
    "            out_channels=config[\"out_channels\"],\n",
    "            init_filters=config[\"init_filters\"],\n",
    "            n_blocks=config[\"n_blocks\"],\n",
    "            activation=config[\"activation\"],\n",
    "            downsampling_kernel=config[\"downsampling_kernel\"],\n",
    "            downsampling_mode=config[\"downsampling_mode\"],\n",
    "            upsampling_kernel=config[\"upsampling_kernel\"],\n",
    "            upsampling_mode=config[\"upsampling_mode\"],\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Construct primal-dual operator with nn\n",
    "        pdhg = DynamicImageStaticPrimalDualNN(\n",
    "            cnn_block=unet, \n",
    "            T=config[\"T\"],\n",
    "            phase=\"training\",\n",
    "            up_bound=config[\"up_bound\"],\n",
    "        ).to(DEVICE)\n",
    "        if is_state_dict:\n",
    "            pdhg.load_state_dict(torch.load(f\"{model_states_dir}/{pretrained_model_path}.pt\"))\n",
    "    else:\n",
    "        pdhg = torch.load(f\"{model_states_dir}/{pretrained_model_path}.pt\").to(DEVICE)\n",
    "\n",
    "    # IMPORTANT!!! Train with only the UNET here\n",
    "    pdhg = unet # HOTFIX: Avoid rewriting the code with unet variable name\n",
    "    pdhg.train(True)\n",
    "\n",
    "    # TODO: Sometimes, creating the optimizer gives this error:\n",
    "    #   AttributeError: partially initialized module 'torch._dynamo' has no attribute 'trace_rules' (most likely due to a circular import)\n",
    "    optimizer = torch.optim.Adam(pdhg.parameters(), lr=config[\"learning_rate\"])\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "\n",
    "    num_epochs = config[\"epochs\"]\n",
    "\n",
    "    save_epoch_local = config[\"save_epoch_local\"]\n",
    "    save_epoch_wandb = config[\"save_epoch_wandb\"]\n",
    "\n",
    "    time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    project = config[\"project\"]\n",
    "    model_name = f\"model-{project}-{time}\"\n",
    "\n",
    "    # Prepare to save the model\n",
    "    save_dir = config[\"save_dir\"]\n",
    "    model_states_dir = f\"{save_dir}/{model_name}\"\n",
    "\n",
    "    os.makedirs(model_states_dir, exist_ok=True)\n",
    "\n",
    "    def log_to_files():\n",
    "        with open(f\"{model_states_dir}/config.json\", \"w\") as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "        with open(f\"{model_states_dir}/config.yaml\", \"w\") as f:\n",
    "            yaml.dump(config, f)\n",
    "        with open(f\"{model_states_dir}/config.txt\", \"w\") as f:\n",
    "            f.write(str(config))\n",
    "        with open(f\"{model_states_dir}/unet.txt\", \"w\") as f:\n",
    "            f.write(str(unet))\n",
    "        with open(f\"{model_states_dir}/pdhg_net.txt\", \"w\") as f:\n",
    "            f.write(str(pdhg))\n",
    "\n",
    "        def log_data(dataloader, stage):\n",
    "            dataset = dataloader.dataset\n",
    "            with open(f\"{model_states_dir}/dataloader_{stage}.txt\", \"w\") as f:\n",
    "                f.write(f\"Batch size: {dataloader.batch_size}\\n\\n\")\n",
    "                f.write(f\"Number of batches: {len(dataloader)}\\n\\n\")\n",
    "                f.write(f\"Number of samples: {len(dataset)}\\n\\n\")\n",
    "                f.write(f\"Samples weights:\\n{str(dataset.samples_weights)}\\n\\n\")\n",
    "                f.write(f\"Sample 0 size:\\n{str(len(dataset[0]))}  {str(dataset[0][0].size())}\\n\\n\")\n",
    "                f.write(f\"Sample 0:\\n{str(dataset[0])}\\n\\n\")\n",
    "        log_data(dataloader_train, \"train\")\n",
    "        log_data(dataloader_valid, \"val\")\n",
    "        # log_data(dataloader_test, \"test\")\n",
    "\n",
    "    log_to_files()\n",
    "\n",
    "    # noisy_image_path = \"./testcases/chest_xray_noisy.png\"\n",
    "    # clean_image_path = \"./testcases/chest_xray_clean.png\"\n",
    "\n",
    "    # def get_image(image_path):\n",
    "    #     image = Image.open(image_path)\n",
    "    #     image = image.convert(\"L\")\n",
    "    #     image_data = np.asarray(image)\n",
    "    #     image_data = convert_to_tensor_4D(image_data)\n",
    "    #     image_data = image_data.unsqueeze(0).to(DEVICE)\n",
    "    #     return image_data\n",
    "\n",
    "    # noisy_image_data = get_image(noisy_image_path)\n",
    "    # clean_image_data = get_image(clean_image_path)\n",
    "\n",
    "    # dataset_train = MyDataset(noisy_image_path, clean_image_path)\n",
    "    # dataset_valid = MyDataset(noisy_image_path, clean_image_path)\n",
    "\n",
    "    # dataloader_train = torch.utils.data.DataLoader(\n",
    "    #     dataset_train, batch_size=1, \n",
    "    #     generator=torch.Generator(device=DEVICE),\n",
    "    #     shuffle=True)\n",
    "    # dataloader_valid = torch.utils.data.DataLoader(\n",
    "    #     dataset_valid, batch_size=1, \n",
    "    #     generator=torch.Generator(device=DEVICE),\n",
    "    #     shuffle=False)\n",
    "\n",
    "\n",
    "    init_wandb(config)\n",
    "\n",
    "    # for epoch in range(start_epoch, num_epochs):\n",
    "    for epoch in tqdm(range(start_epoch, num_epochs)):\n",
    "\n",
    "        # Model training\n",
    "        pdhg.train(True)\n",
    "        training_loss, training_psnr, training_ssim = train_epoch(pdhg, dataloader_train, optimizer, loss_function)\n",
    "        # training_loss, training_psnr, training_ssim = train_iteration(optimizer, pdhg, loss_function, sample=(noisy_image_data, clean_image_data))\n",
    "        # print(f\"Epoch {epoch+1} - TRAINING LOSS: {training_loss} - TRAINING PSNR: {training_psnr} - TRAINING SSIM: {training_ssim}\")\n",
    "\n",
    "        # Optional: Use wandb to log training progress\n",
    "        wandb.log({\"training_loss\": training_loss})\n",
    "        wandb.log({\"training PSNR\": training_psnr})\n",
    "        wandb.log({\"training SSIM\": training_ssim})\n",
    "\n",
    "        del training_loss\n",
    "        del training_psnr\n",
    "        del training_ssim\n",
    "\n",
    "        pdhg.train(False)\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Model validation\n",
    "            validation_loss, validation_psnr, validation_ssim = validate_epoch(pdhg, dataloader_valid, loss_function)\n",
    "            # validation_loss, validation_psnr, validation_ssim = validate_iteration(pdhg, loss_function, sample=(noisy_image_data, clean_image_data))\n",
    "            # print(f\"Epoch {epoch+1} - VALIDATION LOSS: {validation_loss} - VALIDATION PSNR: {validation_psnr} - VALIDATION SSIM: {validation_ssim}\")\n",
    "            time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "            # Optional: Use wandb to log training progress\n",
    "            wandb.log({\"validation_loss\": validation_loss})\n",
    "            wandb.log({\"validation PSNR\": validation_psnr})\n",
    "            wandb.log({\"validation SSIM\": validation_ssim})\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        if (epoch+1) % save_epoch_local == 0:\n",
    "            current_model_name = f\"model_epoch_{epoch+1}\"\n",
    "            torch.save(pdhg, f\"{model_states_dir}/{current_model_name}.pt\")\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} - VALIDATION LOSS: {validation_loss} - VALIDATION PSNR: {validation_psnr} - VALIDATION SSIM: {validation_ssim}\")\n",
    "\n",
    "        if (epoch+1) % save_epoch_wandb == 0:\n",
    "            wandb.log_model(f\"{model_states_dir}/{current_model_name}.pt\", name=f\"model_epoch_{epoch+1}\")\n",
    "            \n",
    "        del validation_loss\n",
    "        del validation_psnr\n",
    "        del validation_ssim\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # Save the entire model\n",
    "    torch.save(pdhg, f\"{model_states_dir}/final_model.pt\")\n",
    "    \n",
    "    wandb.log_model(f\"{model_states_dir}/final_model.pt\", name=f\"final_model\")\n",
    "    wandb.finish()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return pdhg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "pdhg = start_training(get_config())\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Inference\n",
    "\n",
    "Demo the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CODE TO INFER AND SHOW SOME RESULTS HERE\n",
    "\n",
    "\n",
    "\n",
    "# def simple_plot(input_image_tensor_5D, subplot_index, image_name, clean_image_tensor_5D, folder_name, num_rows=2, num_cols=3):\n",
    "#     plot_image_tensor_2D = input_image_tensor_5D.squeeze(0).squeeze(0).squeeze(-1)\n",
    "#     clean_image_tensor_2D = clean_image_tensor_5D.squeeze(0).squeeze(0).squeeze(-1)\n",
    "#     psnr_value = PSNR(clean_image_tensor_2D, plot_image_tensor_2D)\n",
    "#     ssim_value = SSIM(clean_image_tensor_2D, plot_image_tensor_2D)\n",
    "#     plt.subplot(num_rows, num_cols, subplot_index)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(plot_image_tensor_2D.to(\"cpu\").detach().numpy(), cmap='gray')\n",
    "#     plt.title(f\"{image_name} PSNR: {psnr_value:.2f} dB\\n{image_name} SSIM: {ssim_value:.2f}\", fontsize=10)\n",
    "#     # Write the image to a file\n",
    "#     save_image(plot_image_tensor_2D, f\"{image_name}\", folder_name)\n",
    "\n",
    "# def get_image_tensor_5D(image):\n",
    "#     image = image.convert(\"L\")\n",
    "#     image_numpy = np.asarray(image)\n",
    "#     image_tensor_4D = convert_to_tensor_4D(image_numpy)\n",
    "#     image_tensor_5D = image_tensor_4D.unsqueeze(0).to(DEVICE)\n",
    "#     return image_tensor_5D\n",
    "\n",
    "# def denoise(pdhg: DynamicImageStaticPrimalDualNN, noisy_image_tensor_5D):\n",
    "#     pdhg.eval()\n",
    "#     with torch.no_grad():\n",
    "#         best_lambda_map = pdhg.get_lambda_cnn(noisy_image_tensor_5D)\n",
    "#     x_denoised_lambda_map_best_tensor_5D = reconstruct_with_PDHG(noisy_image_tensor_5D, best_lambda_map, pdhg.T)\n",
    "#     # x_denoised_lambda_map_best_tensor_5D = torch.clamp(x_denoised_lambda_map_best_tensor_5D, 0, 1)\n",
    "#     with torch.no_grad():\n",
    "#         torch.cuda.empty_cache()\n",
    "#     return best_lambda_map, x_denoised_lambda_map_best_tensor_5D\n",
    "\n",
    "\n",
    "# def brute_force_lambda(noisy_image_tensor_5D, clean_image_tensor_5D, T, min_value=0.01, max_value=0.1, num_values=10):\n",
    "#     # TODO: Brute-force single lambda\n",
    "#     best_psnr = 0\n",
    "#     best_lambda = 0\n",
    "#     lambas = list(np.linspace(min_value, max_value, num_values))\n",
    "#     psnr_values = []\n",
    "#     for lambda_value in lambas:\n",
    "#         with torch.no_grad():\n",
    "#             x_denoised_single_lambda_tensor_5D = reconstruct_with_PDHG(noisy_image_tensor_5D, lambda_value, T)\n",
    "#         psnr_value = PSNR(clean_image_tensor_5D, x_denoised_single_lambda_tensor_5D)\n",
    "#         psnr_value = psnr_value.item()\n",
    "#         # Convert to float\n",
    "#         psnr_value = np.float64(psnr_value)\n",
    "#         if psnr_value > best_psnr:\n",
    "#             best_psnr = psnr_value\n",
    "#             best_lambda = lambda_value\n",
    "#         psnr_values.append(psnr_value)\n",
    "\n",
    "#     # Plot the PSNR values\n",
    "#     plt.plot(lambas, psnr_values)\n",
    "#     plt.xlabel(\"Lambda\")\n",
    "#     plt.ylabel(\"PSNR\")\n",
    "#     plt.title(\"PSNR vs Lambda\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     return best_lambda\n",
    "\n",
    "\n",
    "# def test_denoise(pdhg: DynamicImageStaticPrimalDualNN=None, model_name=\"\", best_lambda=None):\n",
    "#     \"\"\"\n",
    "#     Testing denoising with pre-trained parameters.\n",
    "#     \"\"\"\n",
    "#     clean_image = Image.open(f\"testcases/chest_xray_clean.png\")\n",
    "#     noisy_image = Image.open(f\"testcases/chest_xray_noisy.png\")\n",
    "#     clean_image_tensor_5D = get_image_tensor_5D(clean_image)\n",
    "#     noisy_image_tensor_5D = get_image_tensor_5D(noisy_image)\n",
    "\n",
    "#     if best_lambda is None:\n",
    "#         best_lambda = brute_force_lambda(noisy_image_tensor_5D, clean_image_tensor_5D, T=pdhg.T, min_value=0.01, max_value=1, num_values=100)\n",
    "\n",
    "#     print(f\"Best lambda: {best_lambda}\")\n",
    "\n",
    "#     k_w, k_h = 256, 256\n",
    "\n",
    "#     folder_name = f\"./tmp/images/model_{model_name}-kernel_{k_w}-best_lambda_{str(best_lambda).replace('.', '_')}-time_{datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "#     os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "#     plt.figure(figsize=(15, 6)) # Set the size of the plot\n",
    "\n",
    "#     simple_plot(clean_image_tensor_5D, 1, \"clean\", clean_image_tensor_5D, folder_name)\n",
    "#     simple_plot(noisy_image_tensor_5D, 2, \"noisy\", clean_image_tensor_5D, folder_name)\n",
    "\n",
    "#     x_denoised_single_lambda_tensor_5D = reconstruct_with_PDHG(noisy_image_tensor_5D, best_lambda, T=pdhg.T)\n",
    "    \n",
    "#     best_lambda_map, x_denoised_lambda_map_tensor_5D = denoise(pdhg, noisy_image_tensor_5D)\n",
    "\n",
    "#     # Clip to [0, 1]. The calculations may make it slightly below 0 and above 1\n",
    "#     x_denoised_single_lambda_tensor_5D = torch.clamp(x_denoised_single_lambda_tensor_5D, 0, 1)\n",
    "#     x_denoised_lambda_map_tensor_5D = torch.clamp(x_denoised_lambda_map_tensor_5D, 0, 1)\n",
    "\n",
    "#     simple_plot(x_denoised_single_lambda_tensor_5D, 3, f\"single_lambda_best_{str(best_lambda).replace('.', '_')}\", clean_image_tensor_5D, folder_name)\n",
    "#     simple_plot(x_denoised_lambda_map_tensor_5D, 4, \"lambda_map_best_using_function\", clean_image_tensor_5D, folder_name)\n",
    "\n",
    "#     lambda_map_1 = best_lambda_map[:, 0:1, :, :, :]\n",
    "#     lambda_map_2 = best_lambda_map[:, 1:2, :, :, :]\n",
    "#     lambda_map_3 = best_lambda_map[:, 2:3, :, :, :]\n",
    "\n",
    "#     lambda_map_1 = torch.clamp(lambda_map_1, 0, 1)\n",
    "#     lambda_map_2 = torch.clamp(lambda_map_2, 0, 1)\n",
    "#     lambda_map_3 = torch.clamp(lambda_map_3, 0, 1)\n",
    "\n",
    "#     simple_plot(lambda_map_1, 5, \"lambda_map_1\", clean_image_tensor_5D, folder_name)\n",
    "#     simple_plot(lambda_map_3, 6, \"lambda_map_3\", clean_image_tensor_5D, folder_name)\n",
    "\n",
    "#     plt.savefig(f\"{folder_name}/results.png\")\n",
    "\n",
    "#     plt.show();\n",
    "\n",
    "#     with open(f\"{folder_name}/log.txt\", \"w\") as f:\n",
    "#         f.write(f\"Best lambda: {best_lambda}\\n\")\n",
    "#         f.write(f\"PSNR (single lambda): {PSNR(clean_image_tensor_5D.squeeze(0).squeeze(0).squeeze(-1), x_denoised_single_lambda_tensor_5D.squeeze(0).squeeze(0).squeeze(-1))}\\n\")\n",
    "#         f.write(f\"PSNR (lambda map): {PSNR(clean_image_tensor_5D.squeeze(0).squeeze(0).squeeze(-1), x_denoised_lambda_map_tensor_5D.squeeze(0).squeeze(0).squeeze(-1))}\\n\")\n",
    "#         f.write(f\"Config: {get_config()}\\n\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "# model_dir = \"./tmp_2/model-2024_06_05_23_51_27\"\n",
    "# epoch = 4000\n",
    "# pdhg = torch.load(f\"{model_dir}/model_epoch_{epoch}.pt\")\n",
    "\n",
    "# test_denoise(\n",
    "#     pdhg=pdhg,\n",
    "#     model_name=f\"chest_xray_demo-epoch_{epoch}\",\n",
    "#     best_lambda=0.08\n",
    "# )\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def temp_test():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Create a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_video(model_name, start_epoch=20, end_epoch=10_000, step=20):\n",
    "#     clean_image_path = \"./test_cases/turtle_clean/turtle clean.png\"\n",
    "#     noisy_image_path = \"./test_cases/turtle_noisy/turtle noisy.png\"\n",
    "#     clean_image_tensor_5D = get_image_tensor_5D(clean_image_path)\n",
    "#     noisy_image_tensor_5D = get_image_tensor_5D(noisy_image_path)\n",
    "#     clean_image_tensor_2D = clean_image_tensor_5D.squeeze(0).squeeze(0).squeeze(-1)\n",
    "#     noisy_image_tensor_2D = noisy_image_tensor_5D.squeeze(0).squeeze(0).squeeze(-1)\n",
    "\n",
    "#     psnr_noisy = PSNR(noisy_image_tensor_2D, clean_image_tensor_2D)\n",
    "#     ssim_noisy = SSIM(noisy_image_tensor_2D, clean_image_tensor_2D)\n",
    "\n",
    "\n",
    "\n",
    "#     frames_folder = f\"./tmp/{model_name}\"\n",
    "#     model_folder=f\"./tmp_2/{model_name}\"\n",
    "#     os.makedirs(frames_folder, exist_ok=True)\n",
    "#     os.makedirs(f\"{frames_folder}/denoised\", exist_ok=True)\n",
    "#     os.makedirs(f\"{frames_folder}/lambda_map_1\", exist_ok=True)\n",
    "#     os.makedirs(f\"{frames_folder}/lambda_map_2\", exist_ok=True)\n",
    "#     os.makedirs(f\"{frames_folder}/lambda_map_3\", exist_ok=True)\n",
    "\n",
    "#     with open(f\"./tmp/{model_name}/metrics.csv\", \"w\") as f:\n",
    "#         f.write(f\"Image, PSNR, SSIM\\n\")\n",
    "#         f.write(f\"Noisy, {psnr_noisy:.2f}, {ssim_noisy:.2f}\\n\")\n",
    "\n",
    "#         for epoch in range(start_epoch, end_epoch + 1, step):\n",
    "#             model_name = f\"model_epoch_{epoch}\"\n",
    "#             pdhg = torch.load(f\"{model_folder}/{model_name}.pt\")\n",
    "#             best_lambda_map, x_denoised_lambda_map_best_tensor_5D = denoise(pdhg, noisy_image_tensor_5D)\n",
    "#             x_denoised_lambda_map_best_tensor_5D = torch.clamp(x_denoised_lambda_map_best_tensor_5D, 0, 1)\n",
    "\n",
    "#             x_denoised_lambda_map_best_tensor_2D = x_denoised_lambda_map_best_tensor_5D.squeeze(0).squeeze(0).squeeze(-1)\n",
    "#             psnr_denoised = PSNR(x_denoised_lambda_map_best_tensor_2D, clean_image_tensor_2D)\n",
    "#             ssim_denoised = SSIM(x_denoised_lambda_map_best_tensor_2D, clean_image_tensor_2D)\n",
    "#             f.write(f\"{epoch}, {psnr_denoised:.2f}, {ssim_denoised:.2f}\\n\")\n",
    "\n",
    "#             denoised_image_to_save = Image.fromarray((x_denoised_lambda_map_best_tensor_2D.to(\"cpu\").detach().numpy() * 255).astype(np.uint8))\n",
    "#             denoised_image_to_save.save(f\"{frames_folder}/denoised/{epoch}.png\")\n",
    "\n",
    "#             lambda_map_1 = best_lambda_map[:, 0:1, :, :, :]\n",
    "#             lambda_map_2 = best_lambda_map[:, 1:2, :, :, :]\n",
    "#             lambda_map_3 = best_lambda_map[:, 2:3, :, :, :]\n",
    "#             lambda_map_1 = torch.clamp(lambda_map_1, 0, 1)\n",
    "#             lambda_map_2 = torch.clamp(lambda_map_2, 0, 1)\n",
    "#             lambda_map_3 = torch.clamp(lambda_map_3, 0, 1)\n",
    "\n",
    "#             lambda_map_1_to_save = Image.fromarray((lambda_map_1.squeeze(0).squeeze(0).squeeze(-1).to(\"cpu\").detach().numpy() * 255).astype(np.uint8))\n",
    "#             lambda_map_1_to_save.save(f\"{frames_folder}/lambda_map_1/{epoch}.png\")\n",
    "\n",
    "#             lambda_map_2_to_save = Image.fromarray((lambda_map_2.squeeze(0).squeeze(0).squeeze(-1).to(\"cpu\").detach().numpy() * 255).astype(np.uint8))\n",
    "#             lambda_map_2_to_save.save(f\"{frames_folder}/lambda_map_2/{epoch}.png\")\n",
    "\n",
    "#             lambda_map_3_to_save = Image.fromarray((lambda_map_3.squeeze(0).squeeze(0).squeeze(-1).to(\"cpu\").detach().numpy() * 255).astype(np.uint8))\n",
    "#             lambda_map_3_to_save.save(f\"{frames_folder}/lambda_map_3/{epoch}.png\")\n",
    "        \n",
    "\n",
    "#     # # Create the video\n",
    "#     # frames = []\n",
    "#     # for epoch in range(start_epoch, end_epoch + 1, step):\n",
    "#     #     frames.append(cv2.imread(f\"{frames_folder}/frame_{epoch}.png\"))\n",
    "#     # height, width, layers = frames[0].shape\n",
    "#     # size = (width, height)\n",
    "#     # out = cv2.VideoWriter(f\"{frames_folder}/video.avi\", cv2.VideoWriter_fourcc(*'DIVX'), 1, size)\n",
    "#     # for i in range(len(frames)):\n",
    "#     #     out.write(frames[i])\n",
    "#     # out.release()\n",
    "\n",
    "# create_video(\"model_turtle_2024_06_04_04_19_21\", start_epoch=20, end_epoch=10_000, step=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_revisualise():\n",
    "\n",
    "#     def vis(image_folder, image_name):\n",
    "#         image_path = f\"{image_folder}/{image_name}.png\"\n",
    "#         image = Image.open(image_path)\n",
    "#         plt.imshow(image, cmap='gray')\n",
    "#         plt.show();\n",
    "\n",
    "#     image_folder = \"tmp/PRESENT/presentation-img_turtle-best_lambda_0_07-kernel_256-model_-trained_on_-time_2024_06_04_22_59_31-epoch_100_000\"\n",
    "#     image_names = [\n",
    "#         \"lambda_map_3\",\n",
    "#         \"single_lambda_best_0_06000000000000001\",\n",
    "#         \"clean\",\n",
    "#     ]\n",
    "\n",
    "#     for image_name in image_names:\n",
    "#         vis(image_folder, image_name)\n",
    "\n",
    "# test_revisualise()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
